%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{CamemBERT}\label{chap:camembert}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
    \begin{minipage}{0.66\textwidth}
        \begin{small}
            In which we present a part of the work of \citet{martin-etal-2020-camembert} who pre-trained the first transformer based language model for Contemporary French using the French subcorpus of OSCAR 2019 \citep{ortiz-suarez-etal-2019-asynchronous,ortiz-suarez-etal-2020-monolingual}. The model that we call \camembert is then evaluated in dependency parsing, part-of-speech tagging, named entity recognition and natural language inference. We also study the question of how corpus size and diversity affects the performance of an architecture like \roberta \citep{liu-etal-2019-roberta} in downstream tasks.\footnotemark
        \end{small}
    \end{minipage}
    \vspace{0.5cm}
\end{center}

\footnotetext{Contributions: I prepared OSCAR 2019 for the pre-training of \camembert and actually had to re-write the whole pipeline in order to produce the first unshuffled version of OSCAR. I did all the experiments where \camembert is used in embedding form. I also wrote the code to synchronize and extract fixed token embeddings from \camembert which was necessary at the time since this option did not exist in Hugging Face Transformer library at the time. Moreover, the whole section \ref{sec:origin_and_size} and one of the main scientific contributions of the article was originally devised by me as one of the experiments that we wanted to conduct for the OSCAR project and was supposed to be part of \citep{ortiz-suarez-etal-2020-monolingual} presented in chapter \ref{chap:monolingual}. However, due to time and space constraints we preferred to do these experiments as part of the \camembert project. Finally, I actively participated in writing \citep{martin-etal-2020-camembert}.}

Having extensively worked into creating and curating textual resources in previous chapters and parts of this thesis, we wanted to use these resources in order to train a monolingual contextual language model for Contemporary French.

When we started the experiments that will be discussed in this chapter, the availability of large monolingual transformer based models was limited to English-only models \citep{devlin-etal-2019-bert,radford-etal-2019-language,liu-etal-2019-roberta,yang-etal-2019-xlnet,raffel-etal-2020-exploring} and most of the work in other languages was being done through multilingual models like \mbert \citep{devlin-etal-2019-bert}. And even though multilingual models gave remarkable results at the time, they were often larger, and their results, as we will observe for French, could lag behind their monolingual counterparts for high-resource languages.

In order to reproduce and validate results that had so far only been obtained for English, we took advantage of the first version of OSCAR\footnote{Now OSCAR 2019.} \citep{ortiz-suarez-etal-2019-asynchronous} which had just been released at that time. We used the French subcorpus of OSCAR 2019 to train a monolingual language model for French, dubbed \camembert. We also trained alternative versions of \camembert on different smaller corpora with different levels of homogeneity in genre and style in order to assess the impact of these parameters on downstream task performance.
\camembert used the \roberta architecture \citep{liu-etal-2019-roberta}.

We then evaluated our model on four different downstream tasks for French: part-of-speech (POS) tagging, dependency parsing, named entity recognition (NER) and natural language inference (NLI). \camembert improved on the state of the art in all four tasks compared to previous monolingual and multilingual approaches including \mbert, XLM and XLM-R, which confirmed the effectiveness of pre-trained contextual language models for French.

\section{\camembert: A Contemporary French Language Model}\label{sec:Camembert}
In this section, we describe the pre-training data, architecture, training objective and optimization setup we use for \camembert.

\subsection{Training data}
Pre-trained language models benefits from being trained on large datasets \citep{devlin-etal-2019-bert,liu-etal-2019-roberta,raffel-etal-2020-exploring}. We therefore use the French subcorpus of OSCAR 2019 \citep{ortiz-suarez-etal-2019-asynchronous,ortiz-suarez-etal-2020-monolingual}. No other filtering is done. We use the deduplicated non-shuffled version of the French subcorpus, which amounts to 138GB of raw text and to around 32.7B tokens after subword tokenization.

\subsection{Pre-processing}
We segment the input text data into subword units using SentencePiece \citep{kudo-richardson-2018-sentencepiece}. SentencePiece is an extension of Byte-Pair encoding (BPE) \citep{sennrich-etal-2016-neural} and WordPiece \citep{kudo-2018-subword} that does not require pre-tokenization (at the word or token level), thus removing the need for language-specific tokenisers. We use a vocabulary size of 32k subword tokens. These subwords are learned on $10^7$ sentences sampled randomly from the pre-training dataset.
We do not use subword regularization (i.e.~sampling from multiple possible segmentations) for the sake of simplicity.


\subsection{Language Modeling}

\paragraph{Transformer}
Similar to \roberta and \bert, \camembert is a multi-layer bidirectional Transformer \citep{vaswani-etal-2017-attention}. \camembert uses the original architectures of \bertbase (12 layers, 768 hidden dimensions, 12 attention heads, 110M parameters) and \bertlarge (24 layers, 1024 hidden dimensions, 16 attention heads, 335M parameters). \camembert is very similar to \roberta, the main difference being the use of whole-word masking and the usage of SentencePiece tokenization \citep{kudo-richardson-2018-sentencepiece} instead of WordPiece \citep{schuster-nakajima-2012-japanese}.

\paragraph{Pretraining Objective}
We train our model on the Masked Language Modeling (MLM) task.
Given an input text sequence composed of $N$ tokens $x_1, ..., x_N$, we select 15\% of tokens for possible replacement. Among those selected tokens, 80\% are replaced with the special \texttt{<MASK>} token, 10\% are left unchanged and 10\% are replaced by a random token. The model is then trained to predict the initial masked tokens using cross-entropy loss.

Following the \roberta approach, we dynamically mask tokens instead of fixing them statically for the whole dataset during preprocessing. This improves variability and makes the model more robust when training for multiple epochs.

Since we use SentencePiece to tokenize our corpus, the input tokens to the model are a mix of whole words and subwords. An upgraded version of \bert\footnote{\url{https://github.com/google-research/bert/blob/master/README.md}} and \citet{joshi-etal-2020-spanbert} have shown that masking whole words instead of individual subwords leads to improved performance. Whole-word Masking (WWM) makes the training task more difficult because the model has to predict a whole word rather than predicting only part of the word given the rest. We train our models using WWM by using white spaces in the initial non-tokenized text as word delimiters.

WWM is implemented by first randomly sampling 15\% of the words in the sequence and then considering all subword tokens in each of this 15\% for candidate replacement. This amounts to a proportion of selected tokens that is close to the original 15\%. These tokens are then either replaced by \texttt{<MASK>} tokens (80\%), left unchanged (10\%) or replaced by a random token.

Subsequent work has shown that the next sentence prediction (NSP) task originally used in \bert does not improve downstream task performance \citep{conneau-lample-2019-cross,liu-etal-2019-roberta}, thus we also remove it.

\paragraph{Optimization}
Following \citep{liu-etal-2019-roberta}, we optimize the model using Adam \citep{kingma-ba-2015-adam} ($\beta_1 = 0.9$, $\beta_2 = 0.98$) for 100k steps with large batch sizes of 8192 sequences, each sequence containing at most 512 tokens. We enforce each sequence to only contain complete paragraphs (which correspond to lines in the pre-training dataset).

\paragraph{Pre-training}
We use the \roberta implementation in the fairseq library \citep{ott-etal-2019-fairseq}. Our learning rate is warmed up for 10k steps up to a peak value of $0.0007$ instead of the original $0.0001$ given our large batch size, and then fades to zero with polynomial decay. Unless otherwise specified, our models use the BASE architecture, and are pre-trained for 100k backpropagation steps on 256 Nvidia V100 GPUs (32 GB each) for a day. We do not train our models for longer due to practical considerations, even though the performance still seemed to continue increasing afterwards.

\subsection{Using \camembert for downstream tasks}
We use the pretrained \camembert in two ways. In the first one, which we refer to as \textit{fine-tuning}, we fine-tune the model on a specific task in an end-to-end manner. In the second one, referred to as \textit{feature-based embeddings} or simply \textit{embeddings}, we extract frozen contextual embedding vectors from \camembert.
These two complementary approaches shed light on the quality of the pretrained hidden representations captured by \camembert.


\paragraph{Fine-tuning}
For each task, we append the relevant predictive layer on top of \camembert's  architecture. Following the work done on the \bert paper \citep{devlin-etal-2019-bert}, for sequence tagging and sequence labeling we append a linear layer that respectively takes as input the last hidden representation of the \texttt{<s>} special token and the last hidden representation of the first subword token of each word. For dependency parsing, we plug a bi-affine graph predictor head as inspired by \citet{dozat-manning-2017-deep}. We fine-tune on XNLI by adding a classification head composed of one hidden layer with a non-linearity and one linear projection layer, with input dropout for both.

We fine-tune \camembert independently for each task and each dataset, optimizing the model using the Adam optimizer \citep{kingma-ba-2015-adam} with a fixed learning rate. Likewise, we run a grid search on a combination of learning rates and batch sizes. Furthermore, we select the best model on the validation set out of the 30 first epochs. For NLI we use the default hyper-parameters provided by the authors of RoBERTa on the MNLI task.\footnote{More details at \url{https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md}.} Although this might have pushed the performances even further, we do not apply any regularization techniques such as weight decay, learning rate warm-up or discriminative fine-tuning, except for NLI. We show that fine-tuning \camembert in a straightforward manner leads to state-of-the-art results on all tasks and outperforms the existing multilingual \bert-based models in all cases. The POS tagging, dependency parsing, and NER experiments are run using Hugging Face's Transformer library extended to support \camembert and dependency parsing \citep{wolf-etal-2019-huggingface}. The NLI experiments use the fairseq library following the \roberta implementation.

\paragraph{Embeddings}

Following \citet{strakova-etal-2019-neural} and \citet{straka-strakova-2019-evaluating} for \mbert and the English BERT, we make use of \camembert in a feature-based embeddings setting. In order to obtain a representation for a given token, we first compute the average of each sub-wordâ€™s representations in the last four layers of the Transformer, and then average the resulting sub-word vectors.

We evaluate \camembert in the embeddings setting for POS tagging, dependency parsing and NER; using the open-source implementations of \citet{straka-strakova-2019-evaluating} and \citet{strakova-etal-2019-neural}.\footnote{UDPipe Future is available at \url{https://github.com/CoNLL-UD-2018/UDPipe-Future}, and the code for nested NER is available at \url{https://github.com/ufal/acl2019_nested_ner}.}


\paragraph{Dowstream Tasks}

For POS tagging and dependency parsing, we run our experiments using the Universal Dependencies (UD)\footnote{\url{https://universaldependencies.org}.} framework and its corresponding UD POS tag set \citep{petrov-etal-2012-universal} and UD treebank collection \citep{nivre-etal-2018-universal}, which was used for the CoNLL 2018 shared task \citep{seker-etal-2018-universal}. We perform our evaluations on the four freely available French UD treebanks in UD~v2.2: GSD \citep{mcdonald-etal-2013-universal}, Sequoia\footnote{\url{https://deep-sequoia.inria.fr}.} \citep{candito-seddah-2012-le,candito-etal-2014-deep}, Spoken \citep{lacheret-etal-2014-rhapsodie,bawden-etal-2014-correcting},\footnote{Speech transcript uncased that includes annotated disfluencies without punctuation.} and ParTUT \cite{sanguinetti-Bosco-2015-parttut}.

For NER, we use the French Treebank (FTB) \citep{abeille-etal-2003-building} in its 2008 version introduced by \citet{candito-crabbe-2009-improving} and with NER annotations by \citet{sagot-etal-2012-annotation}. More precisely, we used the corrected and synchronized version \citep{ortiz-suarez-etal-2020-establishing} presented in subsection \ref{subsec:alignment}.

Finally, we evaluate our model on NLI, using the French part of the XNLI dataset \cite{conneau-etal-2018-xnli}. The XNLI dataset is the extension of the Multi-Genre NLI (MultiNLI) corpus \cite{williams-etal-2018-broad} to 15 languages by translating the validation and test sets manually into each of those languages. The English training set is machine translated for all languages other than English.

\section{Evaluation of \camembert}

In this section, we measure the performance of our models by evaluating them on the four aforementioned tasks: POS tagging, dependency parsing, NER and NLI.

\paragraph{POS Tagging and Dependency Parsing}
For POS tagging and dependency parsing, we compare \camembert with other models in the two settings: \textit{fine-tuning} and as \textit{feature-based embeddings}. We report the results in Table~\ref{tab:pos_and_dp_results}.

\begin{table}[ht]
    \small\centering
    \resizebox{\linewidth}{!}{
        \begin{tabu}{ l  c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c  @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c }
            \toprule
                                                                               & \multicolumn{2}{c @{\hspace{0.5cm}}}{\textsc{GSD}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Sequoia}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Spoken}} & \multicolumn{2}{c @{\hspace{0.35cm}}}{\textsc{ParTUT}}                                                                                 \\
            \cmidrule(l{2pt}r{0.4cm}){2-3}\cmidrule(l{-0.2cm}r{0.4cm}){4-5}\cmidrule(l{-0.2cm}r{0.4cm}){6-7}\cmidrule(l{-0.2cm}r{2pt}){8-9}
            \multirow{-2}{*}[1pt]{\textsc{Model}}                              & \textsc{UPOS}                                      & \textsc{LAS}                                           & \textsc{UPOS}                                         & \textsc{LAS}                                           & \textsc{UPOS}     & \textsc{LAS}      & \textsc{UPOS}     & \textsc{LAS}      \\
            \midrule
            \mbert  (fine-tuned)                                               & 97.48                                              & 89.73                                                  & 98.41                                                 & 91.24                                                  & 96.02             & 78.63             & 97.35             & 91.37             \\
            \xlmmlmtlm (fine-tuned)                                            & 98.13                                              & 90.03                                                  & 98.51                                                 & 91.62                                                  & 96.18             & 80.89             & 97.39             & 89.43             \\ % 10138744 new XLM %& 97.71             & -                 & 98.51          & 91.62             & 95.29             & 74.17             & 96.84             & 89.22             \\ % 10138744 new XLM  % partut sequoia parsing 10138787 
            UDify \citep{kondratyuk-straka-2019-75}                            & 97.83                                              & \underline{91.45}                                      & 97.89                                                 & 90.05                                                  & 96.23             & 80.01             & 96.12             & 88.06             \\
            %\xlmEnFr & 97.51 & 93.49 & 90.72 & 98.30 &  93.62 & 91.565.42 & 96.53 & 93.03 & 90.64 \\ 
            UDPipe Future \citep{straka-2018-udpipe}                           & 97.63                                              & 88.06                                                  & 98.79                                                 & 90.73                                                  & 95.91             & 77.53             & 96.93             & 89.63             \\
            \: + mBERT + Flair  (emb.) \citep{straka-strakova-2019-evaluating} & \underline{97.98}                                  & 90.31                                                  & \textbf{99.32}                                        & 93.81                                                  & \textbf{97.23}    & \underline{81.40} & \underline{97.64} & \underline{92.47} \\
            \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
            \camembert (fine-tuned)                                            & \textbf{98.18}                                     & \textbf{92.57}                                         & \underline{99.29}                                     & \textbf{94.20}                                         & 96.99             & 81.37             & \textbf{97.65}    & \textbf{93.43}    \\ % 10125734 : POS best seed PARSING gsd 10126431 PARSING other : 10126429
            UDPipe Future \mbox{+ \camembert} (embeddings)                     & 97.96                                              & 90.57                                                  & 99.25                                                 & \underline{93.89}                                      & \underline{97.09} & \textbf{81.81}    & 97.50             & 92.32             \\
            \bottomrule
        \end{tabu}
    }
    \caption{\textbf{POS} and \textbf{dependency parsing} scores on 4 French treebanks, reported on test sets assuming gold tokenization and segmentation (best model selected on validation out of 4). Best scores in bold, second best underlined.}%\lm{uniformize the notations of XLM models}}%\comment{Report best seed and not average?}
    \label{tab:pos_and_dp_results}
\end{table}

\camembert reaches state-of-the-art scores on all treebanks and metrics in both scenarios. The two approaches achieve similar scores, with a slight advantage for the fine-tuned version of \camembert, thus questioning the need for complex task-specific architectures such as UDPipe Future.

Despite a much simpler optimization process and no task specific architecture, fine-tuning \camembert outperforms UDify on all treebanks and sometimes by a large margin (e.g. +4.15\% LAS on Sequoia and +5.37 LAS on ParTUT). \camembert also reaches better performance  than other multilingual pre-trained models such as \mbert and \xlmmlmtlm on all treebanks.

\camembert achieves overall slightly better results than the previous state-of-the-art and task-specific architecture UDPipe Future+mBERT+Flair, except for POS tagging on Sequoia and POS tagging on Spoken, where \camembert lags by 0.03\% and 0.14\% UPOS respectively.
UDPipe Future+mBERT+Flair uses the contextualized string embeddings Flair \citep{akbik-etal-2018-contextual}, which are in fact pre-trained contextualized character-level word embeddings specifically designed to handle misspelled words as well as subword structures such as prefixes and suffixes. This design choice might explain the difference in score for POS tagging with CamemBERT, especially for the Spoken treebank where words are not capitalized, a factor that might pose a problem for CamemBERT which was trained on capitalized data, but that might be properly handle by Flair on the UDPipe Future+mBERT+Flair model.

\paragraph{Named-Entity Recognition}
For NER, we similarly evaluate \camembert in the fine-tuning setting and as input embeddings to the task specific architecture LSTM+CRF. We report these scores in Table~\ref{table:ner_ablation}.

\begin{table}[ht]
    \centering \small
    \begin{tabu}{lc}
        \toprule
        Model                                     & F1                \\
        \midrule
        SEM (CRF) \citep{dupont-2017-exploration} & 85.02             \\
        LSTM-CRF \citep{dupont-2017-exploration}  & 85.57             \\
        \mbert (fine-tuned)                       & 87.35             \\
        \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
        \camembert (fine-tuned)                   & \underline{89.08} \\% 10129644  %91.30 dev 10129153 (2 seeds only)
        LSTM+CRF+\camembert (embeddings)          & \textbf{89.55}    \\
        %            \midrule
        %            \multicolumn{2}{c}{\em Supplement: subword masking model}\\
        %            LSTM+CRF+\camembertoscarswm (embeddings)  & \textbf{90.25} \\
        \bottomrule
    \end{tabu}
    \caption{\textbf{NER} scores on the FTB (best model selected on validation out of 4). Best scores in bold, second best underlined.
        \label{table:ner_ablation}}
\end{table}

In both scenarios, \camembert achieves higher F1 scores than the traditional CRF-based architectures (both non-neural and neural), and than the fine-tuned multilingual BERT models.\footnote{\xlmmlmtlm is a lower-case model. Case is crucial for NER, therefore we do not report its low performance (84.37\%)}

Using \camembert as embeddings to the traditional LSTM+CRF architecture gives slightly higher scores than by fine-tuning the model (89.08 vs.~89.55).
This demonstrates that even though \camembert can be used successfully without any task-specific architecture, it can still produce high quality contextualized embeddings that might be useful in scenarios where powerful downstream architectures exist.

\paragraph{Natural Language Inference}
On the XNLI benchmark, we compare \camembert to previous state-of-the-art multilingual models in the fine-tuning setting. In addition to the standard \camembert model with a BASE architecture, we train another model with the LARGE architecture, referred to as \camembertccnetlarge, for a fair comparison with XLM-R\textsubscript{LARGE}. This model was trained with the \ccnet corpus, described in Sec.~\ref{sec:origin_and_size}, for 100k steps.\footnote{We train our LARGE model with the \ccnet corpus for practical reasons, mainly due to the fact that it was more readily available on the Facebook infrastructure we used to train \camembert. Given that BASE models reach similar performance when using \oscar or \ccnet as pretraining corpus (Appendix Table~\ref{tab:ablation}), we expect an \oscar LARGE model to reach comparable scores.} We expect that training the model for longer would yield even better performance.

\begin{table}[ht]
    \centering\small
    \begin{tabu}{lcc}
        \toprule
        Model                                                             & Acc.             & \#Params \\
        \midrule
        %BiLSTM-max \citep{conneau-etal-2018-xnli} & 68.3 & - \\
        \mbert \citep{devlin-etal-2019-bert}                              & 76.9             & 175M     \\
        \xlmmlmtlm \citep{conneau-lample-2019-cross}                      & \underline{80.2} & 250M     \\
        XLM-R\textsubscript{BASE} \citep{conneau-etal-2020-unsupervised}  & 80.1             & 270M     \\
        \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
        \camembert (fine-tuned)                                           & \textbf{82.5}    & 110M     \\
        \midrule
        \multicolumn{3}{c}{\em Supplement: LARGE models}                                                \\
        XLM-R\textsubscript{LARGE} \citep{conneau-etal-2020-unsupervised} & \underline{85.2} & 550M     \\
        \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
        \camembertccnetlarge (fine-tuned)                                 & \textbf{85.7}    & 335M     \\
        \bottomrule
    \end{tabu}
    \caption{\textbf{NLI} accuracy on the French XNLI test set (best model selected on validation out of 10). Best scores in bold, second best underlined.\label{table:xnli}}
\end{table}

\camembert reaches higher accuracy than its BASE counterparts reaching +5.6\% over \mbert, +2.3 over \xlmmlmtlm, and +2.4 over XLM-R\textsubscript{BASE}. \camembert also uses as few as half as many parameters (110M vs. 270M for XLM-R\textsubscript{BASE}).

\camembertccnetlarge achieves a state-of-the-art accuracy of 85.7\% on the XNLI benchmark, as opposed to 85.2, for the recent XLM-R\textsubscript{LARGE}.

\camembert uses fewer parameters than multilingual models, mostly because of its smaller vocabulary size (e.g. 32k vs. 250k for XLM-R). Two elements might explain the better performance of \camembert over XLM-R. Even though XLM-R was trained on an impressive amount of data (2.5TB), only 57GB of this data is in French, whereas we used 138GB of French data. Additionally, XLM-R also handles 100 languages, and the authors show that when reducing the number of languages to 7, they can reach 82.5\% accuracy for French XNLI with their BASE architecture.

\paragraph{Summary of \camembert's results}
\camembert improves the state of the art for the 4 downstream tasks considered, thereby confirming the usefulness of a monolingual Transformer-based models for contemporary French. We obtain these results when using \camembert as a fine-tuned model or when used as contextual embeddings with task-specific architectures. This questions the need for more complex downstream architectures, similar to what was shown for English \citep{devlin-etal-2019-bert}. Additionally, this suggests that \camembert is also able to produce high-quality representations out-of-the-box without further tuning.

\section{Impact of corpus origin and size}
\label{sec:origin_and_size}

In this section we investigate the influence of the homogeneity and size of the pre-training corpus on downstream task performance. With this aim, we train alternative version of \camembert by varying the pre-training datasets. For this experiment, we fix the number of pre-training steps to 100k, and allow the number of epochs to vary accordingly (more epochs for smaller dataset sizes). All models use the BASE architecture.

In order to investigate the need for homogeneous clean data versus more diverse and possibly noisier data, we use alternative sources of pre-training data in addition to \oscar 2019:
\begin{itemize}
    \item \textbf{Wikipedia}, which is homogeneous in terms of genre and style. We use the official 2019 French Wikipedia dumps.\footnote{ \url{https://dumps.wikimedia.org/backup-index.html}.} We remove HTML tags and tables using Giuseppe Attardi's  \emph{WikiExtractor}.\footnote{ \url{https://github.com/attardi/wikiextractor}.}
    \item \textbf{\ccnet} \citep{wenzek-etal-2020-ccnet}, a dataset extracted from Common Crawl with a different filtering process than for \oscar. It was built using a language model trained on Wikipedia, in order to filter out bad quality texts such as code or tables.\footnote{We use the \textsc{head} split, which corresponds to the top 33\% of documents in terms of filtering perplexity.} As this filtering step biases the noisy data from Common Crawl to more Wikipedia-like text, we expect \ccnet to act as a middle ground between the unfiltered ``noisy'' \oscar 2019 dataset, and the ``clean'' Wikipedia dataset. As a result of the different filtering processes, \ccnet contains longer documents on average compared to \oscar 2019 with smaller---and often noisier---documents weeded out.
\end{itemize}
Table~\ref{table:corpora_statistics} summarizes statistics of these different corpora.

\begin{table}[ht]
    \centering\small
    \begin{tabular}{lcccccc}
        \toprule
        Corpus     & Size   & \#tokens & \#docs & \multicolumn{3}{c}{Tokens/doc}                 \\
                   &        &          &        & \multicolumn{3}{c}{Percentiles:}               \\
                   &        &          &        & 5\%                              & 50\% & 95\% \\
        \midrule
        Wikipedia  & 4 GB   & 990M     & 1.4M   & 102                              & 363  & 2530 \\
        CCNet      & 135 GB & 31.9B    & 33.1M  & 128                              & 414  & 2869 \\
        OSCAR 2019 & 138 GB & 32.7B    & 59.4M  & 28                               & 201  & 1946 \\
        \bottomrule
    \end{tabular}
    \caption{Statistics on the pre-training datasets used.}
    \label{table:corpora_statistics}
\end{table}

In order to make a fair comparison between these three sources of pre-training data, we randomly sample 4 GB (the size of Wikipedia) of text (at the document level) from \oscar and \ccnet, thereby creating samples of both Common-Crawl-based corpora of the same size as the French Wikipedia. These smaller 4GB samples also provides us a way to investigate the impact of pre-training data size. Downstream task performance for our alternative versions of \camembert are provided in Table~\ref{tab:ablation_data_size}.
The upper section reports scores in the fine-tuning setting while the lower section reports scores for the embeddings.

\subsection{Common Crawl vs.~Wikipedia?}
\label{subsec:homogeneityimpact}

Table~\ref{tab:ablation_data_size} clearly shows that models trained on the 4 GB versions of \oscar 2019 and \ccnet (Common Crawl) perform consistently better than the one trained on the French Wikipedia. This is true both in the fine-tuning and embeddings setting. Unsurprisingly, the gap is larger on tasks involving texts whose genre and style are more divergent from those of Wikipedia, such as tagging and parsing on the Spoken treebank. The performance gap is also very large on the XNLI task, probably as a consequence of the larger diversity of Common-Crawl-based corpora in terms of genres and topics. XNLI is indeed based on multiNLI which covers a range of genres of spoken and written text.

\begin{table}[ht]
    \small\centering
    \resizebox{\textwidth}{!}{
        \tabulinesep =_1pt^1pt
        \begin{tabu}{ l l @{\hspace{0.7cm}}  c  c  @{\hspace{0.7cm}} c  c  @{\hspace{0.7cm}} c  c @{\hspace{0.7cm}} c  c @{\hspace{0.7cm}} c c @{\hspace{0.7cm}} c @{\hspace{0.7cm}} c @{\hspace{0.7cm}}}
            \toprule
                                                    &                                      & \multicolumn{2}{c @{\hspace{0.5cm}}}{\textsc{GSD}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Sequoia}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Spoken}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{ParTUT}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{\textbf{Average}}} & NER               & NLI                                                                                                                            \\
            \cmidrule(l{2pt}r{0.4cm}){3-4}\cmidrule(l{-0.2cm}r{0.4cm}){5-6}\cmidrule(l{-0.2cm}r{0.4cm}){7-8}\cmidrule(l{-0.2cm}r{0.4cm}){9-10}\cmidrule(l{-0.2cm}r{0.4cm}){11-12}\cmidrule(l{-0.2cm}r{0.4cm}){13-13}\cmidrule(l{-0.2cm}r{0.4cm}){14-14}
            \multirow{-2}{*}[2pt]{\textsc{Dataset}} & \multirow{-2}{*}[2pt]{\textsc{Size}} & \textsc{UPOS}                                      & \textsc{LAS}                                           & \textsc{UPOS}                                         & \textsc{LAS}                                          & \textsc{UPOS}                                                   & \textsc{LAS}      & \textsc{UPOS}              & \textsc{LAS}      & \textsc{UPOS}     & \textsc{LAS}      & \textsc{F1}       & \textsc{Acc.}     \\
            \midrule

            \multicolumn{10}{l}{\hspace*{6mm}\em Fine-tuning}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\[0.5mm]
            %\oscar                                  & 0.1GB                                  & 98.12 & 92.28 & 98.52 &  90.09 & 96.45 &  76.01 & 95.08 & 87.49 & - & - & 83.25 & 72.98\\ 
            %100MB & 98.12 & 92.28 & 98.52 &  90.09 & 96.45 &  76.01 & 95.08 & 87.49  & 83.25 & 72.98\\
            Wiki                                    & 4GB                                  & 98.28                                              & 93.04                                                  & 98.74                                                 & 92.71                                                 & 96.61                                                           & 79.61             & 96.20                      & 89.67             & 97.45             & 88.75             & 89.86             & 78.32             \\ %  10137841 10137842 parsing, ppos 10138173  tagging
            \ccnet                                  & 4GB                                  & 98.34                                              & 93.43                                                  & 98.95                                                 & 93.67                                                 & 96.92                                                           & \textbf{82.09}    & 96.50                      & \textbf{90.98}    & 97.67             & \textbf{90.04}    & 90.46             & \textbf{82.06}    \\
            \oscar                                  & 4GB                                  & \underline{98.35}                                  & \underline{93.55}                                      & \underline{98.97}                                     & \underline{93.70}                                     & \underline{96.94}                                               & \underline{81.97} & \underline{96.58}          & 90.28             & \underline{97.71} & 89.87             & \underline{90.65} & \underline{81.88} \\
            \tabucline[\hbox{$\scriptstyle \cdot$}]{-}
            %\ccnet & 135GB & \underline{98.36} &  90.57 & 98.97 & 94.04 & 96.98 &  82.07 & 96.39  & \textbf{91.18} & 90.13 & \textbf{82.22} \\%  -  &  -  &
            \oscar                                  & 138GB                                & \textbf{98.39}                                     & \textbf{93.80}                                         & \textbf{98.99}                                        & \textbf{94.00}                                        & \textbf{97.17}                                                  & 81.18             & \textbf{96.63}             & \underline{90.56} & \textbf{97.79}    & \underline{89.88} & \textbf{91.55}    & 81.55             \\
            \midrule
            \multicolumn{11}{l}{\hspace*{6mm}\em Embeddings (with UDPipe Future (tagging, parsing) or LSTM+CRF (NER))}                                                                                                                                                                                                                                                                                                                                                                                                                          \\[0.5mm]
            %\oscar                                  & 0.1GB                                  & 98.04 & 91.95 & 98.73 & 92.60 & 96.96 & 81.02 & - & - & - & - & 89.78 & - \\ 
            Wiki                                    & 4GB                                  & 98.09                                              & 92.31                                                  & 98.74                                                 & 93.55                                                 & 96.24                                                           & 78.91             & 95.78                      & 89.79             & 97.21             & 88.64             & 91.23             & -                 \\
            \ccnet                                  & 4GB                                  & \textbf{98.22}                                     & \textbf{92.93}                                         & \underline{99.12}                                     & \underline{94.65}                                     & 97.17                                                           & \textbf{82.61}    & \underline{\textbf{96.74}} & \underline{89.95} & \underline{97.81} & \underline{90.04} & \textbf{92.30}    & -                 \\
            \oscar                                  & 4GB                                  & \underline{98.21}                                  & \underline{92.77}                                      & \underline{99.12}                                     & \textbf{94.92}                                        & \underline{97.20}                                               & \underline{82.47} & \underline{\textbf{96.74}} & \textbf{90.05}    & \textbf{97.82}    & \textbf{90.05}    & \underline{91.90} & -                 \\
            \tabucline[\hbox{$\scriptstyle \cdot$}]{-}
            %\ccnet & 135GB & 98.27   & 92.94  &   99.05  &  94.51   & 97.04  & 82.09  & \underline{96.68}  & 89.89 & 91.88  & - \\  -  &  -  &
            \oscar                                  & 138GB                                & 98.18                                              & \underline{92.77}                                      & \textbf{99.14}                                        & 94.24                                                 & \textbf{97.26}                                                  & 82.44             & 96.52                      & 89.89             & 97.77             & 89.84             & 91.83             & -                 \\

            \bottomrule
        \end{tabu}
    }
    \caption{Results on the four tasks using language models pre-trained on data sets of varying homogeneity and size, reported on validation sets (average of 4 runs for POS tagging, parsing and NER, average of 10 runs for NLI).}
    \label{tab:ablation_data_size}
\end{table}

The downstream task performances of the models trained on the 4 GB version of \ccnet and \oscar are much more similar.\footnote{We provide the results of a model trained on the whole \ccnet corpus in the Appendix. The conclusions are similar when comparing models trained on the full corpora: downstream results are similar when using \oscar or \ccnet.}


\subsection{How much data do you need?}
\label{subsec:sizeimpact}

An unexpected outcome of our experiments is that the model trained ``only'' on the 4 GB sample of \oscar 2019 performs remarkably similarly to the standard \camembert trained on the whole 138 GB \oscar 2019. The only task with a large performance gap is NER, where  ``138 GB'' models are better by 0.9 F1 points. This could be due to the higher number of named entities present in the larger corpora, which is beneficial for this task. On the contrary, other tasks don't seem to gain from the additional data.

In other words, when trained on corpora such as \oscar and \ccnet, which are heterogeneous in terms of genre and style, 4 GB of uncompressed text is large enough as pre-training corpus to reach state-of-the-art results with the BASE architecture, better than those obtained with \mbert (pre-trained on 60 GB of text).\footnote{The OSCAR-4 GB model gets slightly better XNLI accuracy than the full OSCAR-138 GB model (81.88 vs. 81.55). This might be due to the random seed used for pre-training, as each model is pre-trained only once.} This calls into question the need to use a very large corpus such as \oscar or \ccnet when training a monolingual Transformer-based language model such as BERT or \roberta. Not only does this mean that the computational (and therefore environmental) cost of training a state-of-the-art language model can be reduced, but it also means that \camembert-like models can be trained for all languages for which a Common-Crawl-based corpus of 4 GB or more can be created. \oscar is available in more than 150 languages, and provides such a corpus for around 38 languages. Moreover, it is possible that slightly smaller corpora (e.g.~down to 1 GB) could also prove sufficient to train high-performing language models. We obtained our results with BASE architectures. Further research is needed to confirm the validity of our findings on larger architectures and other more complex natural language understanding tasks. However, even with a BASE architecture and 4 GB of training data, the validation loss is still decreasing beyond 100k steps (and 400 epochs). This suggests that we are still under-fitting the 4 GB pre-training dataset, training longer might increase downstream performance.

\section{Discussion}

Between the pre-publication of this work\footnote{\url{https://arxiv.org/abs/1911.03894v1} (First ArXiv version).} and the publication of its peer-reviewed version \citep{martin-etal-2020-camembert}, many monolingual language models appeared, e.g. \citep{le-etal-2020-flaubert-unsupervised,virtanen-etal-2019-multilingual,delobelle-etal-2020-robbert}, and for as much as 30 languages \citep{nozza-etal-2020-what}. In almost all tested configurations they displayed better results than multilingual language models such as \mbert \citep{pires-etal-2019-multilingual}. Interestingly, \citet{le-etal-2020-flaubert-unsupervised} showed that using FlauBERT, another RoBERTa-based language model for Contemporary French, which was trained on less but more edited data, in conjunction to \camembert in an ensemble system could improve the performance of a parsing model and establish a new state-of-the-art in constituency parsing for Contemporary French, highlighting thus the complementarity of both models.\footnote{We refer the reader to \citep{le-etal-2020-flaubert-unsupervised} for a comprehensive benchmark and details therein.}

As it was the case for English when \bert was first released, the availability of similar scale language models for Contemporary French enabled interesting applications, such as large scale anonymization of legal texts, where \camembert-based models established a new state-of-the-art on this task \citep{benesty-2019-ner}, or the first large question answering experiments on a French Squad data set that was released after the publication of \camembert \citep{dhoffschmidt-etal-2020-fquad} where the authors matched human performance using \camembertlarge. Being the first pre-trained Trasnformer-based language model that used the OSCAR corpus and given its impact on the community, \camembert paved the way for many works on monolingual language models that followed. Furthermore, the availability of all its training data favors reproducibility and is a step towards better understanding such models and the impact that the pre-training data has on them. In that spirit, we make the models used in our experiments available via our website\footnote{\url{https://camembert-model.fr}} and via the \texttt{huggingface} and \texttt{fairseq} APIs, in addition to the base \camembert model.

\section{Conclusion}
In this chapter we investigated the feasibility of training a Transformer-based language model for languages other than Contemporary English. Using Contemporary French as an example, we trained \camembert, a language model based on \roberta. We evaluated \camembert on four downstream tasks (part-of-speech tagging, dependency parsing, named entity recognition
and natural language inference) in which our best model reached or improved the state of the art in all tasks considered, even when compared to strong multilingual models such as \mbert, XLM and XLM-R, while also having fewer parameters.

Our experiments confirm the previous findings presented in chapter \ref{chap:monolingual} that using web crawled data with high variability is preferable to using Wikipedia-based data. In addition, we showed that our models could reach surprisingly high performances with as low as 4 GB of pre-training data, questioning thus the need for large scale pre-training corpora. This shows that state-of-the-art Transformer-based language models can be trained on languages with far fewer resources than previously believed, and whenever a few gigabytes of data are available. This paves the way for the rise of monolingual contextual pre-trained language models for mid- and low-resourced languages. The question of knowing whether pre-training on small domain specific content will be a better option than transfer learning techniques such as fine-tuning remains open, and we will partially study it in the context of historical data in upcoming chapters.
