% Encoding: UTF-8

@article{arivazhagan-etal-2019-massively,
  author        = {{Arivazhagan}, Naveen and {Bapna}, Ankur and {Firat}, Orhan and {Lepikhin}, Dmitry and {Johnson}, Melvin and {Krikun}, Maxim and {Chen}, Mia Xu and {Cao}, Yuan and {Foster}, George and {Cherry}, Colin and {Macherey}, Wolfgang and {Chen}, Zhifeng and {Wu}, Yonghui},
  title         = {{Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  year          = 2019,
  month         = jul,
  eid           = {arXiv:1907.05019},
  pages         = {arXiv:1907.05019},
  archiveprefix = {arXiv},
  eprint        = {1907.05019},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190705019A},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{biderman-etal-2020-pitfalls,
  author        = {{Biderman}, Stella and {Scheirer}, Walter J.},
  title         = {{Pitfalls in Machine Learning Research: Reexamining the Development Cycle}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Machine Learning, Statistics - Methodology, Statistics - Machine Learning},
  year          = 2020,
  month         = nov,
  eid           = {arXiv:2011.02832},
  pages         = {arXiv:2011.02832},
  archiveprefix = {arXiv},
  eprint        = {2011.02832},
  primaryclass  = {cs.LG},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv201102832B},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{blumofe-etal-1999-scheduling,
  author     = {Blumofe, Robert D. and Leiserson, Charles E.},
  title      = {Scheduling Multithreaded Computations by Work Stealing},
  year       = {1999},
  issue_date = {Sept. 1999},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {46},
  number     = {5},
  issn       = {0004-5411},
  url        = {https://doi.org/10.1145/324133.324234},
  doi        = {10.1145/324133.324234},
  abstract   = {This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured)
multithreaded computations on parallel computers. A popular and practical method of
scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which
processors needing work steal computational threads from other processors. In this
paper, we give the first provably good work-stealing scheduler for multithreaded computations
with dependencies.Specifically, our analysis shows that the expected time to execute
a fully strict computation on P processors using our work-stealing scheduler is T1/P
+ O(T ∞ , where T1 is the minimum serial execution time of the multithreaded computation
and (T ∞ is the minimum execution time with an infinite number of processors. Moreover,
the space required by the execution is at most S1P, where S1 is the minimum serial
space requirement. We also show that the expected total communication of the algorithm
is at most O(PT ∞( 1 + nd)Smax), where Smax is the size of the largest activation
record of any thread and nd is the maximum number of times that any thread synchronizes
with its parent. This communication bound justifies the folk wisdom that work-stealing
schedulers are more communication efficient than their work-sharing counterparts.
All three of these bounds are existentially optimal to within a constant factor.},
  journal    = {J. ACM},
  month      = sep,
  pages      = {720–748},
  numpages   = {29},
  keywords   = {multithreading, work stealing, critical-path length, randomized algorithm, multiprocessor, thread scheduling}
}

@inproceedings{brown-etal-2020-language,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@misc{callan-etal-2009-clueweb09,
  title  = {Clueweb09 data set},
  author = {Callan, Jamie and Hoy, Mark and Yoo, Changkuk and Zhao, Le},
  year   = {2009}
}

@article{caswell-etal-2021-quality,
  author        = {{Caswell}, Isaac and {Kreutzer}, Julia and {Wang}, Lisa and {Wahab}, Ahsan and {van Esch}, Daan and {Ulzii-Orshikh}, Nasanbayar and {Tapo}, Allahsera and {Subramani}, Nishant and {Sokolov}, Artem and {Sikasote}, Claytone and {Setyawan}, Monang and {Sarin}, Supheakmungkol and {Samb}, Sokhar and {Sagot}, Beno{\^\i}t and {Rivera}, Clara and {Rios}, Annette and {Papadimitriou}, Isabel and {Osei}, Salomey and {Ortiz Su{\'a}rez}, Pedro Javier and {Orife}, Iroro and {Ogueji}, Kelechi and {Niyongabo}, Rubungo Andre and {Nguyen}, Toan Q. and {M{\"u}ller}, Mathias and {M{\"u}ller}, Andr{\'e} and {Hassan Muhammad}, Shamsuddeen and {Muhammad}, Nanda and {Mnyakeni}, Ayanda and {Mirzakhalov}, Jamshidbek and {Matangira}, Tapiwanashe and {Leong}, Colin and {Lawson}, Nze and {Kudugunta}, Sneha and {Jernite}, Yacine and {Jenny}, Mathias and {Firat}, Orhan and {Dossou}, Bonaventure F.~P. and {Dlamini}, Sakhile and {de Silva}, Nisansa and {{\c{C}}abuk Ball{\i}}, Sakine and {Biderman}, Stella and {Battisti}, Alessia and {Baruwa}, Ahmed and {Bapna}, Ankur and {Baljekar}, Pallavi and {Abebe Azime}, Israel and {Awokoya}, Ayodele and {Ataman}, Duygu and {Ahia}, Orevaoghene and {Ahia}, Oghenefego and {Agrawal}, Sweta and {Adeyemi}, Mofetoluwa},
  title         = {{Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  year          = 2021,
  month         = mar,
  eid           = {arXiv:2103.12028},
  pages         = {arXiv:2103.12028},
  archiveprefix = {arXiv},
  eprint        = {2103.12028},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2021arXiv210312028C},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{clark-etal-2020-electra,
  title     = {ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author    = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=r1xMH1BtvB}
}

@article{fan-etal-2020-beyond,
  author        = {{Fan}, Angela and {Bhosale}, Shruti and {Schwenk}, Holger and {Ma}, Zhiyi and {El-Kishky}, Ahmed and {Goyal}, Siddharth and {Baines}, Mandeep and {Celebi}, Onur and {Wenzek}, Guillaume and {Chaudhary}, Vishrav and {Goyal}, Naman and {Birch}, Tom and {Liptchinsky}, Vitaliy and {Edunov}, Sergey and {Grave}, Edouard and {Auli}, Michael and {Joulin}, Armand},
  title         = {{Beyond English-Centric Multilingual Machine Translation}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  year          = 2020,
  month         = oct,
  eid           = {arXiv:2010.11125},
  pages         = {arXiv:2010.11125},
  archiveprefix = {arXiv},
  eprint        = {2010.11125},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv201011125F},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{gao-etal-2020-the,
  author        = {{Gao}, Leo and {Biderman}, Stella and {Black}, Sid and {Golding}, Laurence and {Hoppe}, Travis and {Foster}, Charles and {Phang}, Jason and {He}, Horace and {Thite}, Anish and {Nabeshima}, Noa and {Presser}, Shawn and {Leahy}, Connor},
  title         = {{The Pile: An 800GB Dataset of Diverse Text for Language Modeling}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  year          = 2020,
  month         = dec,
  eid           = {arXiv:2101.00027},
  pages         = {arXiv:2101.00027},
  archiveprefix = {arXiv},
  eprint        = {2101.00027},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2021arXiv210100027G},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{joulin-etal-2016-fasttext,
  author        = {{Joulin}, Armand and {Grave}, Edouard and {Bojanowski}, Piotr and {Douze}, Matthijs and {J{\'e}gou}, H{\'e}rve and {Mikolov}, Tomas},
  title         = {{FastText.zip: Compressing text classification models}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  year          = 2016,
  month         = dec,
  eid           = {arXiv:1612.03651},
  pages         = {arXiv:1612.03651},
  archiveprefix = {arXiv},
  eprint        = {1612.03651},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv161203651J},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{mikolov-etal-2013-distributed,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  url       = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
  volume    = {26},
  year      = {2013}
}

@inproceedings{ortiz-suarez-etal-2019-asynchronous,
  author    = {Pedro Javier {Ortiz Su{\'a}rez} and Beno{\^i}t Sagot and Laurent Romary},
  booktitle = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019},
  title     = {Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures},
  editor    = {Piotr Bański and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc Kupietz and Harald L{\"u}ngen and Caroline Iliadi},
  publisher = {Leibniz-Institut f{\"u}r Deutsche Sprache},
  address   = {Mannheim},
  doi       = {10.14618/ids-pub-9021},
  url       = {http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215},
  pages     = {9 -- 16},
  year      = {2019},
  abstract  = {Common Crawl is a considerably large, heterogeneous multilingual corpus comprised of crawled documents from the internet, surpassing 20TB of data and distributed as a set of more than 50 thousand plain text files where each contains many documents written in a wide variety of languages. Even though each document has a metadata block associated to it, this data lacks any information about the language in which each document is written, making it extremely difficult to use Common Crawl for monolingual applications. We propose a general, highly parallel, multithreaded pipeline to clean and classify Common Crawl by language; we specifically design it so that it runs efficiently on medium to low resource infrastructures where I/O speeds are the main constraint. We develop the pipeline so that it can be easily reapplied to any kind of heterogeneous corpus and so that it can be parameterised to a wide range of infrastructures. We also distribute a 6.3TB version of Common Crawl, filtered, classified by language, shuffled at line level in order to avoid copyright issues, and ready to be used for NLP applications.},
  language  = {en}
}

@article{parker-etal-2011-english,
  title   = {English gigaword fifth edition, linguistic data consortium},
  author  = {Parker, Robert and Graff, David and Kong, Junbo and Chen, Ke and Maeda, Kazuaki},
  journal = {Technical report, Technical Report. Linguistic Data Consortium},
  address = {Philadelphia, Tech. Rep.},
  year    = {2011}
}

@article{radford-etal-2018-improving,
  title   = {Improving language understanding by generative pre-training},
  author  = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal = {OpenAI Blog},
  year    = {2018}
}

@article{radford-etal-2019-language,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal = {OpenAI Blog},
  volume  = {1},
  pages   = {8},
  year    = {2019}
}

@article{raffel-etal-2020-exploring,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{vaswani-etal-2017-attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@article{xiong-etal-2021-nystromformer,
  title        = {Nyströmformer: A Nyström-based Algorithm for Approximating Self-Attention},
  volume       = {35},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17664},
  abstractnote = {Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences - a topic being actively studied in the community. To address this limitation, we propose Nyströmformer - a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nyström method to approximate standard self-attention with O(n) complexity. The scalability of Nyströmformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nyströmformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nyströmformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github.com/mlpen/Nystromformer.},
  number       = {16},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  year         = {2021},
  month        = {May},
  pages        = {14138-14148}
}

@inproceedings{yang-etal-2019-xlnet,
  author    = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  url       = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@inproceedings{zaheer-etal-2020-big,
  author    = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {17283--17297},
  publisher = {Curran Associates, Inc.},
  title     = {Big Bird: Transformers for Longer Sequences},
  url       = {https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{zhu-etal-2015-aligning,
  author    = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {19-27},
  abstract  = {Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.},
  keywords  = {},
  doi       = {10.1109/ICCV.2015.11},
  issn      = {2380-7504},
  month     = {Dec}
}

@Comment{jabref-meta: databaseType:bibtex;}
