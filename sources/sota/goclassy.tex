%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Goclassy Related Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}

Common Crawl has already been successfully used to train language models, even multilingual ones. The most notable example in probably fastText which was first trained for English using Common Crawl \citep{mikolov-etal-2018-advances} and then for other 157 different languages \citep{grave-etal-2018-learning}. In fact \citet{grave-etal-2018-learning} proposed a pipeline to filter, clean and classify Common Crawl, which we shall call the ``fastText pre-processing pipeline.'' They used the fastText linear classifier \citep{joulin-etal-2016-fasttext, joulin-etal-2017-bag} to classify each line of Common Crawl by language, and downloaded the initial corpus and schedule the I/O using some simple Bash scripts. Their solution, however, proved to be a synchronous blocking pipeline that works well on infrastructures having the necessary hardware to assure high I/O speeds even when storing tens of terabytes of data at a time. But that downscales poorly to medium-low resource infrastructures that rely on more traditional cost-effective electromechanical mediums in order to store this amount of data.

Concerning contextual models, \citet{baevski-etal-2019-cloze} trained a BERT-like bi-directional Transformer for English using Common Crawl. They followed the ``fastText pre-processing pipeline'' but they removed all copies of Wikipedia inside Common Crawl. They also trained their model using News Crawl \citep{bojar-etal-2018-findings} and using Wikipedia~+~BooksCorpus, they compared three models and showed that Common Crawl gives the best performance out of the three corpora.

The XLNet model was trained for English by joining the BookCorpus, English Wikipedia, Giga5 \citep{parker-etal-2011-english}, ClueWeb 2012-B \citep{callan-etal-2009-clueweb09} and Common Crawl. Particularly for Common Crawl, \citet{yang-etal-2019-xlnet} say they use ``heuristics to aggressively filter out short or low-quality articles'' from Common Crawl, however they don't give any detail about these ``heuristics'' nor about the pipeline they use to classify and extract the English part of Common Crawl.

It is important to note that none of these projects distributed their classified, filtered and cleaned versions of Common Crawl, making it difficult in general to faithfully reproduce their results.

\section{Common Crawl}

Common Crawl is a non-profit foundation which produces and maintains an open repository of web crawled data that is both accessible and analysable.\footnote{\url{http://commoncrawl.org/about/}} Common Crawl's complete web archive consists of petabytes of data collected over 8 years of web crawling. The repository contains raw web page HTML data (WARC files), metdata extracts (WAT files) and plain text extracts (WET files). The organisation's crawlers has always respected \texttt{nofollow}\footnote{\url{http://microformats.org/wiki/rel-nofollow}} and \texttt{robots.txt}\footnote{\url{https://www.robotstxt.org/}} policies.

Each monthly Common Crawl snapshot is in itself a massive multilingual corpus, where every single file contains data coming from multiple web pages written in a large variety of languages and covering all possible types of topics. Thus, in order to effectively use this corpus for the previously mentioned Natural Language Processing and Machine Learning applications, one has first to extract, filter, clean and classify the data in the snapshot by language.

For our purposes we use the WET files which contain the extracted plain texts from the websites mostly converted to UTF-8, as well as headers containing the metatada of each crawled document. Each WET file comes compressed in gzip format\footnote{\url{https://www.gnu.org/software/gzip/}} and is stored on Amazon Web Services. We use the November 2018 snapshot which surpasses 20TB of uncompressed data and contains more than 50 thousand plain text files where each file consists of the plain text from multiple websites along its metadata header. From now on, when we mention the ``Common Crawl'' corpus, we refer to this particular November 2018 snapshot.

\section{fastText's Pipeline}

In order to download, extract, filter, clean and classify Common Crawl we base ourselves on the ``fastText pre-processing pipeline'' used by \citet{grave-etal-2018-learning}. Their pipeline first launches multiple process, preferably as many as available cores. Each of these processes first downloads one Common Crawl WET file which then proceeds to decompress after the download is over. After decompressing, an instance of the fastText linear classifier \citep{joulin-etal-2016-fasttext, joulin-etal-2017-bag} is launched, the classifier processes each WET file line by line, generating a language tag for each line. The tags are then stored in a tag file which holds a one-to-one correspondence between lines of the WET file and its corresponding language tag. The WET file and the tag files are read sequentially and each on the WET file line holding the condition of being longer that 100 bytes is appended to a language file containing only plain text (tags are discarded). Finally the tag file and the WET files are deleted.

Only when one of these processes finishes another can be launched. This means that one can at most process and download as many files as cores the machine has. That is, if for example a machine has 24 cores, only 24 WET files can be downloaded and processed simultaneously, moreover, the 25\textsuperscript{th} file won't be downloaded until one of the previous 24 files is completely processed.

When all the WET files are classified, one would normally get around 160 language files, each file holding just plain text written in its corresponding language. These files still need to be filtered in order to get rid of all files containing invalid UTF-8 characters, so again a number of processes are launched, this time depending on the amount of memory of the machine. Each process reads a language file, first filters for invalid UTF-8 characters and then performs deduplication. A simple non-collision resistant hashing algorithm is used to deduplicate the files.

The fastText linear classifier works by representing sentences for classification as Bags of Words (BoW) and training a linear classifier. A weight matrix $A$ is used as a look-up table over the words and the word representations are then averaged into a text representation which is fed to the linear classifier. The architecture is in general similar to the CBoW model of \citet{mikolov-etal-2013-distributed} but the middle word is replaced by a label. They uses a softmax function $f$ to compute the probability distribution over the classes. For a set of $N$ documents, the model is trained to minimise the negative log-likelihood over the classes:
\[
    -\frac{1}{N}\sum_{n=1}^{N} y_n\log\left(f(BAx_n)\right),
\]
where $x_n$ is the normalised bag of features of the $n$-th document, $y_n$ is the $n$-th label, and $A,B$ are the weight matrices. The pre-trained fastText model for language recognition \citep{grave-etal-2018-learning} is capable of recognising around 176 different languages and was trained using 400 million tokens from Wikipedia as well as sentences from the Tatoeba website\footnote{\url{https://tatoeba.org/}}.
