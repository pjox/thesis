\chapter{FrELMo}


\section{A named entity annotation layer for the UD version of the French TreeBank}

In this section, we describe the process whereby we re-aligned the named entity FTB annotations by \newcite{sagot-etal-2012-annotation} with the UD version of the FTB. This makes it possible to share these annotations in the form of a set of additional columns that can easily be pasted to the UD FTB file. This new version of the named entity FTB layer is much more readily usable than the original XML version, and will serve as a basis for our experiments in the next sections.
Yet information about the named entity annotation guidelines, process and results can only be found in \newcite{sagot-etal-2012-annotation}, which is written in French. We therefore begin with a brief summary of this publication before describing the alignment process.


\subsection{Alignment to the UD version of the FTB}
\label{subsec:alignment}
% Alingment
%For creating an NER model, we first had to align the provided annotated XML with the already tokenised version of FTB.
%TODO: add this somewhere? This allows to use ML tools while minimizing variation due to automatically segmenting / annotating the data.
%Both resources were created semi-automatically and we found some mismatches when trying to align the two.
%Used a script that performed basic alignment of textual data (lowercase, no space), and manually corrected in case of mismatch.
%Found X errors in XML corpus, and Y in the already tokenised version.

The named entity (NE) annotation layer for the FTB was developed using an XML editor on the raw text of the FTB. Annotations are provided as inline XML elements within the sentence-segmented but non tokenized text. For creating our NER models, we first had to align these XML annotations with the already tokenized UD version of FTB.

Sentences were provided in the same order for both corpora, so we did not have to align them.
For each sentence, we created a mapping $M$ between the raw text of the NE-annotated FTB (i.e.~after having removed all XML annotations) and tokens in the UD version of the FTB corpus. More precisely, character offsets in the FTB-NE raw text were mapped to token offsets in the tokenized FTB-UD.
This alignment was done using case insensitive character-based comparison and were a mapping of a span in the raw text to a span in the tokenized corpus.
We used the inlined XML annotations to create offline, character-level NE annotations for each sentence, and reported the NE annotations at the token level in the FTB-UD using the mapping $M$ obtained.

We logged each error (i.e.~an unaligned NE or token) and then manually corrected the corpora, as those cases were always errors in either corpora and not alignment errors. We found 70 errors in FTB-NE and 3 errors in FTB-UD. Errors in FTB-NE were mainly XML entity problems (unhandled "\&", for instance) or slightly altered text (for example, a missing comma). Errors in FTB-UD were probably some XML artifacts.

%===============================================================================

% Corpus segmentation
% train process
% hyper-parameters optimization
% results
% analysis / comparison
% Model description
% Translate the article

\section{Benchmarking NER Models}

\begin{table*}
    %\ra{1.1}
    \centering\small
    \begin{tabular}{lrrr}
        \toprule
        \textsc{Model}                                                 & \textsc{Precision} & \textsc{Recall}   & \textsc{F1-Score} \\
        \midrule
        \multicolumn{4}{c}\emph{baseline}                                                                                           \\
        %         LNSAI &  84.64 & 68.51 & 75.73\\
        SEM (CRF)                                                      & 87.18              & 80.48             & 83.70             \\
        \midrule
        LSTM-seq2seq                                                   & 85.10              & 81.87             & 83.45             \\
        + FastText                                                     & 86.98              & 83.07             & 84.98             \\
        + FastText + FrELMo                                            & 89.49              & 87.48             & 88.47             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-WWM}           & 89.79              & 88.86             & 89.32             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-WWM} + FrELMo  & 90.00              & 88.60             & 89.30             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-WWM}           & 90.31              & 89.29             & 89.80             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-WWM} + FrELMo  & 90.11              & 88.86             & 89.48             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-SWM}           & 90.09              & 89.46             & 89.77             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-SWM} + FrELMo  & 90.11              & 88.95             & 89.53             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-SWM}           & 90.31              & 89.38             & 89.84             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-SWM} + FrELMo  & 90.64              & 89.46             & \underline{90.05} \\
        + FastText + CamemBERT\textsubscript{CCNET-500K-WWM}           & \underline{90.68}  & 89.03             & 89.85             \\
        + FastText + CamemBERT\textsubscript{CCNET-500K-WWM} + FrELMo  & 90.13              & 88.34             & 89.23             \\
        + FastText + CamemBERT\textsubscript{CCNET-LARGE-WWM}          & 90.39              & 88.51             & 89.44             \\
        + FastText + CamemBERT\textsubscript{CCNET-LARGE-WWM} + FrELMo & 89.72              & 88.17             & 88.94             \\
        \midrule
        \multicolumn{4}{c}\emph{LSTM-CRF + embeddings}                                                                              \\
        LSTM-CRF                                                       & 85.87              & 81.35             & 83.55             \\
        + FastText                                                     & 88.53              & 84.63             & 86.53             \\
        + FastText + FrELMo                                            & 88.89              & 88.43             & 88.66             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-WWM}           & 90.47              & 88.51             & 89.48             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-WWM} + FrELMo  & 89.70              & 88.77             & 89.24             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-WWM}           & 90.24              & 89.46             & 89.85             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-WWM} + FrELMo  & 89.38              & 88.69             & 89.03             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-SWM}           & \textbf{90.96}     & \underline{89.55} & \textbf{90.25}    \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-SWM} + FrELMo  & 89.44              & 88.51             & 88.98             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-SWM}           & 90.09              & 88.69             & 89.38             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-SWM} + FrELMo  & 88.18              & 87.65             & 87.92             \\
        + FastText + CamemBERT\textsubscript{CCNET-500K-WWM}           & 89.46              & 88.69             & 89.07             \\
        + FastText + CamemBERT\textsubscript{CCNET-500K-WWM} + FrELMo  & 90.11              & 88.86             & 89.48             \\
        + FastText + CamemBERT\textsubscript{CCNET-LARGE-WWM}          & 89.19              & 88.34             & 88.76             \\
        + FastText + CamemBERT\textsubscript{CCNET-LARGE-WWM} + FrELMo & 89.03              & 88.34             & 88.69             \\
        \midrule
        \multicolumn{4}{c}\emph{fine-tuning}                                                                                        \\
        mBERT                                                          & 80.35              & 84.02             & 82.14             \\ %% Qu'est-ce que c'est ?

        CamemBERT\textsubscript{OSCAR-BASE-WWM}                        & 89.36              & 89.18             & 89.27             \\
        CamemBERT\textsubscript{CCNET-500K-WWM}                        & 89.35              & 88.81             & 89.08             \\
        CamemBERT\textsubscript{CCNET-LARGE-WWM}                       & 88.76              & \textbf{89.58}    & 89.39             \\
        %CamemBERT\textsubscript{CCNet} & 88.32 & 86.87 & 87.59\\ 
        %CamemBERT\textsubscript{OSCAR} & 86.79 & 87.91 & 87.34\\ 
        \bottomrule
    \end{tabular}
    \caption{Results on the test set for the best development set scores.}
    \label{tab:results_ordered}
\end{table*}

\begin{table*}
    %\ra{1.1}
    \centering\small
    \begin{tabular}{lrrr}
        \toprule
        \textsc{Model}                                         & \textsc{Precision} & \textsc{Recall}   & \textsc{F1-Score} \\
        \midrule
        \multicolumn{4}{c}\emph{shuf 1}                                                                                     \\
        SEM(dev)                                               & 92.96              & 87.84             & 90.33             \\
        LSTM-CRF+CamemBERT\textsubscript{OSCAR-BASE-SWM}(dev)  & \underline{93.77}  & \underline{94.00} & \underline{93.89} \\
        SEM(test)                                              & 91.88              & 87.14             & 89.45             \\
        LSTM-CRF+CamemBERT\textsubscript{OSCAR-BASE-SWM}(test) & \textbf{92.59}     & \textbf{93.96}    & \textbf{93.27}    \\
        \midrule
        \multicolumn{4}{c}\emph{shuf 2}                                                                                     \\
        SEM(dev)                                               & 91.67              & 85.96             & 88.73             \\
        LSTM-CRF+CamemBERT\textsubscript{OSCAR-BASE-SWM}(dev)  & \underline{93.15}  & \underline{94.21} & \underline{93.68} \\
        SEM(test)                                              & 90.57              & 87.76             & 89.14             \\
        LSTM-CRF+CamemBERT\textsubscript{OSCAR-BASE-SWM}(test) & \textbf{92.63}     & \textbf{94.31}    & \textbf{93.46}    \\
        \midrule
        \multicolumn{4}{c}\emph{shuf 3}                                                                                     \\
        SEM(dev)                                               & 92.53              & 88.75             & 90.60             \\
        LSTM-CRF+CamemBERT\textsubscript{OSCAR-BASE-SWM}(dev)  & \underline{94.85}  & \underline{95.82} & \underline{95.34} \\
        SEM(test)                                              & 90.68              & 85.00             & 87.74             \\
        LSTM-CRF+CamemBERT\textsubscript{OSCAR-BASE-SWM}(test) & \textbf{91.30}     & \textbf{92.67}    & \textbf{91.98}    \\
        \bottomrule
    \end{tabular}
    \caption{Results on the test set for the best development set scores.}
    \label{tab:results_shuffled}
\end{table*}


\subsection{Experiments}
% Yoann: write a short paragraph that says (i) that we use the SEM system described below as a strong baseline, and why it is a strong baseline (was SEM the state of the art before? how do you know it was?), and (ii) that we compare SEM to a series of neural models that use fastText, CamemBERT (a French BERT-like model) and/or FrELMo (a French EMLo model) embeddings.
% done

We used SEM \cite{dupont-2017-exploration} as our strong baseline because, to the best of our knowledge, it was the previous state-of-the-art for named entity recognition on the FTB-NE corpus.
% TODO: I see reviewers claiming we could, therefore should, have retrained SpaCy.
Other French NER systems are available, such as the one given by SpaCy. However, it was trained on another corpus called WikiNER, making the results non-comparable.
We can also cite the system of \cite{stern-etal-2012-joint}. This system was trained on another newswire (AFP) using the same annotation guidelines, so the results given in this article are not directly comparable. This model was trained on FTB-NE in \newcite{stern-2013-identification} (table C.7, page 303), but the article is written in French. The model yielded an F1-score of 0.7564, which makes it a weaker baseline than SEM.
We can cite yet another NER system, namely grobid-ner.\footnote{\url{https://github.com/kermitt2/grobid-ner\#corpus-lemonde-ftb-french}} It was trained on the FTB-NE and yields an F1-score of 0.8739. Two things are to be taken into consideration: the tagset was slightly modified and scores were averaged over a 10-fold cross validation. To see why this is important for FTB-NE, see section \ref{subsubsec:shuffling}.

In this section, we will compare our strong baseline with a series of neural models. We will use the two current state-of-the-art neural architectures for NER, namely seq2seq and LSTM-CRFs models. We will use various pre-trained embeddings in said architectures: fastText, \camembert (a French BERT-like model) and FrELMo (a French ELMo model) embeddings.


\subsubsection{SEM}
SEM \cite{dupont-2017-exploration} is a tool that relies on linear-chain CRFs \cite{lafferty-etal-2001-conditional} to perform tagging. SEM uses Wapiti \cite{lavergne-etal-2010-practical} v1.5.0 as linear-chain CRFs implementation. SEM uses the following features for NER:
\begin{itemize}
    \item token, prefix/suffix from 1 to 5 and a Boolean isDigit features in a [-2, 2] window;
    \item previous/next common noun in sentence;
    \item 10 gazetteers (including NE lists and trigger words for NEs) applied with some priority rules in a [-2, 2] window;
    \item a "fill-in-the-gaps" gazetteers feature where tokens not found in any gazetteer are replaced by their POS, as described in \cite{raymond-fayolle-2010-reconnaissance}. This features used token unigrams and token bigrams in a [-2, 2] a window.
    \item tag unigrams and bigrams.
\end{itemize}

We trained our own SEM model by using SEM features on gold tokenization and optimized L1 and L2 penalties on the development set. The metric used to estimate convergence of the model is the error on the development set ($1 - accuracy$). Our best result on the development set was obtained using the rprop algorithm, a 0.1 L1 penalty and a 0.1 L2 penalty.

SEM also uses an NE mention broadcasting post-processing (mentions found at least once are used as a gazetteer to tag unlabeled mentions), but we did not observe any improvement using this post-processing on the best hyperparameters on the development set.


\subsubsection{Neural models}

In order to study the relative impact of different word vector representations and different architectures, we trained a number of NER neural models that differ in multiple ways. They use zero to three of the following vector representations: FastText non-contextual embeddings \cite{bojanowski-etal-2017-enriching}, the FrELMo contextual language model obtained by training the ELMo architecture on the OSCAR large-coverage Common-Crawl-based corpus developed by \newcite{ortiz-suarez-etal-2019-asynchronous}, and one of multiple \camembert language models \cite{martin-etal-2020-camembert}. \camembert models are transformer-based models based on an architecture similar to that of RoBERTa \cite{liu-etal-2019-roberta}, an improvement over the widely used and successful BERT model \cite{devlin-etal-2019-bert}. The \camembert models we use in our experiments differ in multiple ways:
\begin{itemize}
    \item Training corpus: OSCAR (cited above) or CCNet, another Common-Crawl-based corpus \cite{wenzek-etal-2020-ccnet} classified by language, of an almost identical size ($\sim$32 billion tokens); although extracted using similar pipelines from Common Crawl, they differ slightly in so far that OSCAR better reflects the variety of genre and style found in Common Crawl, whereas CCNet was designed to better match the style of Wikipedia; moreover, OSCAR is freely available, whereas only the scripts necessary to rebuild CCNet can be downloaded freely. For comparison purposes, we also display the results of an experiment using the mBERT multilingual BERT model trained on the Wikpiedias for over 100 languages.
    \item Model size: following \newcite{devlin-etal-2019-bert}, we use both ``BASE'' and ``LARGE'' models; these models differ by their number of layers (12 vs.~24), hidden dimensions (768 vs.~1024), attention heads (12 vs.~16) and, as a result, their number of parameters (110M vs.~340M).
    \item Masking strategy: the objective function used to train a \camembert model is a masked language model objective. However, BERT-like architectures like \camembert rely on a fixed vocabulary of explicitly predefined size obtained by an algorithm that splits rarer words into subwords, which are part of the vocabulary together with more frequent words. As a result, it is possible to use a whole-word masked language objective (the model is trained to guess missing words, which might be made of more than one subword) or a subword masked language objective (the model is trained to guess missing subwords). Our models use the acronyms WWM and SWM respectively to indicate the type of masking they used.
\end{itemize}

We use these word vector representations in three types of architectures:
\begin{itemize}
    \item Fine-tuning architectures: in this case, we add a dedicated linear layer to the first subword token of each word, and the whole architecture is then fine-tuned to the NER task on the training data.
    \item Embedding architectures: word vectors produced by language models are used as word embeddings. We use such embeddings in two types of LSTM-based architectures: an LSTM fed to a seq2seq layer and an LSTM fed to a CRF layer. In such configurations, the use of several word representations at the same time is possible, using concatenation as a combination operator. For instance, in Table~\ref{tab:results_ordered}, the model FastText + CamemBERT\textsubscript{OSCAR-BASE-WWM} under the header ``\emph{LSTM-CRF + embeddings} corresponds to a model using the LSTM-CRF architecture and, as embeddings, the concatenation of FastText embeddings, the output of the \camembert ``BASE'' model trained on OSCAR with a whole-word masking objective, and the output of the FrELMo language model.
\end{itemize}

For our neural models, we optimized hyperparameters using F1-score on development set as our convergence metric.

We train each model three times with three different seeds, select the best seed on the development set, and report the results of this seed on the test set in Table~\ref{tab:results_ordered}.

\subsubsection{Results}

\paragraph{Word Embeddings:} Results obtained by SEM and by our neural models are shown in table \ref{tab:results_ordered}. First important result that should be noted is that LSTM+CRF and LSTM+seq2seq models have similar performances to that of the SEM (CRF) baseline when they are not augmented with any kind of embeddings. Just adding classical fastText word embeddings dramatically increases the performance of the model.

\paragraph{ELMo Embeddings:} Adding contextualized ELMo embeddings increases again the performance for both architectures. However we note that the difference is not as big as in the case of the pair with/without fastText word embeddings for the LSTM-CRF. For the seq2seq model, it is the contrary: adding ELMo gives a good improvement while fastText does not improve the results as much.

\paragraph{\camembert Embeddings:} Adding the \camembert embeddings always increases the performance of the model LSTM based models. However, as opposed to adding ELMo, the difference with/without \camembert is equally considerable for both the LSTM-seq2seq and LSTM-CRF. In fact adding \camembert embeddings increases the original scores far more than ELMo embeddings does, so much so that the state-of-the-art model is the LSTM + CRF + FastText + CamemBERT\textsubscript{OSCAR-BASE-SWM}.

\paragraph{\camembert + FrELMo:} Contrary to the results given in \newcite{strakova-etal-2019-neural}, adding ELMo to \camembert did not have a positive impact on the performances of the models. Our hypothesis for these results is that, contrary to \newcite{strakova-etal-2019-neural}, we trained ELMo and \camembert on the same corpus. We think that, in our case, ELMo either does not bring any new information or even interfere with \camembert.
% hyp: BERT pas entraîné avec le même corpus ==> ELMo n'apporte rien, voire crée des interférences

\paragraph{Base vs large:} an interesting observation is that using large model negatively impacts the performances of the models. One possible reason could be that, because the models are larger, the information is more sparsely distributed and that training on the FTB-NE, a relatively small corpus, is harder.
% While the the impact of SWM vs WWM is limited in seq2seq architecture, it has an importan t impact LSTM-CRF, be it positive (OSCAR-base) or negative (CCNET-base + FrELMo).

% Pedro: Add commentary on the results


\subsubsection{Impact of shuffling the data}
\label{subsubsec:shuffling}

One important thing about the FTB is that the underlying text is made of articles from the newspaper Le Monde that are chronologically ordered. Moreover, the standard development and test sets are at the end of the corpus, which means that they are made of articles that are more recent than those found in the training set. This means that a lot of entities in the development and test sets may be new and therefore unseen in the training set. To estimate the impact of this distribution, we shuffled the data, created a new training/development/test split of the same lengths than in the standard split, and retrained and reevaluated our models. We repeated this process 3 times to avoid unexpected biases. The raw results of this experiment are given in table \ref{tab:results_shuffled}. We can see that the shuffled splits result in improvements on all metrics, the improvement in F1-score on the test set ranging from 4.04 to 5.75 (or 25\% to 35\% error reduction) for our SEM baseline, and from 1.73 to 3.21 (or 18\% to 30\% error reduction) for our LSTM-CRF architectures, reaching scores comparable to the English state-of-the-art. This highlights a specific difficulty of the FTB-NE corpus where the development and test sets seem to contain non-negligible amounts of unknown entities. This specificity, however, allows to have a quality estimation which is more in line with real use cases, where unknown NEs are frequent. This is especially the case when processing newly produced texts with models trained on FTB-NE, as the text annotated in the FTB is made of articles around 20 years old.


%===============================================================================

\section{Conclusion}
\label{sec:conclusion}

% alignment
In this article, we introduce a new, more usable version of the named entity annotation layer of the French TreeBank. We aligned the named entity annotation to reference segmentation, which will allow to better integrate NER into the UD version of the FTB.

% benchmark
We establish a new state-of-the-art for French NER using state-of-the-art neural techniques and recently produced neural language models for French. Our best neural model reaches an F1-score which is 6.55 points higher (a 40\% error reduction) than the strong baseline provided by the SEM system.

% shuffling
We also highlight how the FTB-NE is a good approximation of a real use case. Its chronological partition increases the number of unseen entities allows to have a better estimation of the generalisation capacities of machine learning models than if it were randomised.

% perspective 1: capitalizing on UD
Integration of the NER annotations in the UD version of FTB would allow to train more refined model, either by using more information or through multitask learning by learning POS and NER at the same time. We could also use dependency relationships to provide additional information to a NE linking algorithm.

% perspective 2: investigate annotations?
One interesting point to investigate is that using Large embeddings overall has a negative impact on the models performances. It could be because larger models store information relevant to NER more sparingly, making it harder for trained models to capitalize them. We would like to investigate this hypothesis in future research.


%===============================================================================

\subsection*{Acknowledgments}

This work was partly funded by the French national ANR grant BASNUM (\mbox{ANR-18-CE38-0003}), as well as by the last author's chair in the PRAIRIE institute,\footnote{\url{http://prairie-institute.fr/}} funded by the French national ANR as part of the ``Investissements d’avenir'' programme under the reference \mbox{ANR-19-P3IA-0001}. The authors are grateful to Inria Sophia Antipolis - Méditerranée ``Nef'' \footnote{\url{https://wiki.inria.fr/wikis/ClustersSophia}} computation cluster for providing resources and support.