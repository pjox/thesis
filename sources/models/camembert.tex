%\renewcommand{\camembertoscar}{CamemBERT\xspace}%{CamemBERT\textsubscript{OSCAR}\xspace}

\section{Introduction}
%\footnote{Requests via: \url{https://camembert-model.fr/}}

Pretrained word representations have a long history in Natural Language Processing (NLP), from non-contextual \citep{brown-etal-1992-class,ando-zhang-2005-framework,mikolov-etal-2013-distributed,pennington-etal-2014-glove} to contextual word embeddings \citep{peters-etal-2018-deep,akbik-etal-2018-contextual}. Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are \emph{fine-tuned} for each application considered. This shift has resulted in large improvements in performance over a wide range of tasks \cite{devlin-etal-2019-bert,radford-etal-2019-language,liu-etal-2019-roberta,raffel2019exploring}.

These transfer learning methods exhibit clear advantages over more traditional task-specific approaches. In particular, they can be trained in an \emph{unsupervized} manner, thereby taking advantage of the information contained in large amounts of raw text.
Yet they come with implementation challenges, namely the amount of data and computational resources needed for pretraining, which can reach hundreds of gigabytes of text and require hundreds of GPUs \citep{yang2019xlnet,liu-etal-2019-roberta}. This has limited the availability of these state-of-the-art models to the English language, at least in the monolingual setting. This is particularly inconvenient as it hinders their practical use in NLP systems. It also prevents us from investigating their language modelling capacity, for instance in the case of morphologically rich languages.

Although multilingual models give remarkable results, they are often larger, and their results, as we will observe for French, can lag behind their monolingual counterparts for high-resource languages. %\cite{lample2019cross,conneau2019xlmr}.

In order to reproduce and validate results that have so far only been obtained for English, we take advantage of the newly available multilingual corpora OSCAR \cite{ortiz2019asynchronous} to train a monolingual language model for French, dubbed \camembert. We also train alternative versions of \camembert on different smaller corpora with different levels of homogeneity in genre and style in order to assess the impact of these parameters on downstream task performance.
\camembert uses the \roberta architecture \cite{liu-etal-2019-roberta}, an improved variant of the high-performing and widely used \bert architecture \cite{devlin-etal-2019-bert}.

We evaluate our model on four different downstream tasks for French: part-of-speech (POS) tagging, dependency parsing, named entity recognition (NER) and natural language inference (NLI).
\camembert improves on the state of the art in all four tasks compared to previous monolingual and multilingual approaches including \mbert, XLM and XLM-R, which confirms the effectiveness of large pretrained language models for French.

%Our contributions can be summarized as follows:
We make the following contributions:
\begin{itemize}
    \item First release of a monolingual \roberta model for the French language using recently introduced large-scale open source corpora from the Oscar collection and first outside the original \bert authors to release such a large model for an other language than English.\footnote{Released at:
              \mbox{\url{https://camembert-model.fr}} under the MIT open-source license.}

          %c'est déjà en conclusion
          %\newline
          %\mbox{\url{https://github.com/pytorch/fairseq}}\newline
          %\mbox{\url{https://github.com/huggingface/transformers}}
    \item We achieve state-of-the-art results on four downstream tasks: POS tagging, dependency parsing, NER and NLI, confirming the effectiveness of \bert-based language models for French.
    \item We demonstrate that small and diverse training sets can achieve similar performance to large-scale corpora, by analysing the importance of the pretraining corpus in terms of size and domain.%show that for the evaluated tasks, the pretraining domain is more important than the size %more than the dataset size, t%run detailed ablation studies highlighting the importance of the pretraining corpora, the model size, the masking setting, and the number of pretraining steps. 
          %\item We release several of our best-performing models\footnote{under MIT open-source license} %We make CamemBERT easy to use in a few line of codes and make it available in popular open-source libraries. 
          %as well as fine-tuned versions for each downstream task so that they can serve as strong baselines for future research and be useful for French NLP applications.
\end{itemize}

%\lm{Clementine's version of the contributions:}

%Our contribution can be summarised in the three following points. 
%Having trained several monolingual \roberta models for the French language using recent large-scale corpora, we evaluated them on four downstream tasks (POS tagging, dependency parsing, NER and NLI) and achieved state-of-the-art results in all tasks, therefore confirming the effectiveness of \bert-based language models for French. 
%Said models (the original CamemBert and its fine tuned counterparts) were then released with an MIT open-source license. With the same goal of reproducibility, we made CamemBERT easy to use in a few line of codes by releasing it in popular open-source libraries, so that they can serve as strong baselines for future research and be useful for French NLP applications.
%    \footnote{Models released at:
%    \mbox{\url{https://camembert-model.fr}}\newline
%    \mbox{\url{https://github.com/pytorch/fairseq}}\newline
%    \mbox{\url{https://github.com/huggingface/transformers}}}
%Finally, we also studied in detail the impact of several training parameters, such as the pretraining corpora, the model size, the masking setting, and the number of pretraining steps, and provided a detailed discussion of said points. 


\section{Previous work}
\label{relatedwork}
\subsection{Contextual Language Models}
\paragraph{From non-contextual to contextual word embeddings}
The first neural word vector representations were non-contextualized word embeddings, most notably
word2vec \citep{mikolov-etal-2013-distributed}, GloVe \cite{pennington-etal-2014-glove} and fastText \cite{mikolov2018advances}, which were designed to be used as input to task-specific neural architectures.
Contextualized word representations such as ELMo \cite{peters-etal-2018-deep} and flair \cite{akbik-etal-2018-contextual}, improved the representational power of word embeddings by taking context into account. Among other reasons, they improved the performance of models on many tasks by handling words polysemy.
This paved the way for larger contextualized models that replaced downstream architectures altogether in most tasks. Trained with language modeling objectives, these approaches range from LSTM-based architectures such as \cite{dai2015semisupervised}, to the successful transformer-based architectures such
as GPT2 \cite{radford-etal-2019-language}, \bert \cite{devlin-etal-2019-bert}, \roberta \cite{liu-etal-2019-roberta} and more recently ALBERT \cite{lan2019albert} and T5 \cite{raffel2019exploring}.


\paragraph{Non-English contextualized models}
\label{contextualmodelsforotherlanguages}
Following the success of large pretrained language models, they were extended to the multilingual setting with multilingual \bert (hereafter \mbert) \cite{devlin2018mbert}, a single multilingual model for 104 different languages trained on Wikipedia data, and later XLM \cite{lample2019cross}, which significantly improved unsupervized machine translation.
More recently XLM-R \cite{conneau2019xlmr}, extended XLM by training on 2.5TB of data and outperformed previous scores on multilingual benchmarks. They show that multilingual models can obtain results competitive with monolingual models by leveraging higher quality data from other languages on specific downstream tasks.

A few non-English monolingual models have been released: ELMo models for Japanese, Portuguese, German and Basque\footnote{\url{https://allennlp.org/elmo}} and BERT for Simplified and Traditional Chinese \cite{devlin2018mbert} and German \cite{chan2019german}.

However, to the best of our knowledge, no particular effort has been made toward training models for languages other than English at a scale similar to the latest English models (e.g.~\roberta trained on more than 100GB of data).

\paragraph{BERT and RoBERTa}
Our approach is based on \roberta \cite{liu-etal-2019-roberta} which itself is based on \bert \cite{devlin-etal-2019-bert}.
\bert is a multi-layer bidirectional Transformer encoder trained with a masked language modeling (MLM) objective, inspired by the Cloze task \cite{taylor1953cloze}.
It comes in two sizes: the \bertbase architecture and the \bertlarge architecture. The \bertbase architecture is 3 times smaller and therefore faster and easier to use while \bertlarge achieves increased performance on downstream tasks.
\roberta improves the original implementation of \bert by identifying key design choices for better performance, using dynamic masking, removing the next sentence prediction task, training with larger batches, on more data, and for longer.


\section{Downstream evaluation tasks}

In this section, we present the four downstream tasks that we use to evaluate \camembert, namely: Part-Of-Speech (POS) tagging, dependency parsing, Named Entity Recognition (NER) and Natural Language Inference (NLI). We also present the baselines that we will use for comparison.

%\lm{Merge all tasks together}
%\subsection{Part-of-speech tagging and dependency parsing}\label{subsection:pos_and_dp}

\paragraph{Tasks} POS tagging is a low-level syntactic task, which consists in assigning to each word its corresponding grammatical category. Dependency parsing consists in predicting the labeled syntactic tree in order to capture the syntactic relations between words.

For both of these tasks we run our experiments using the Universal Dependencies (UD)\footnote{\url{https://universaldependencies.org}} framework and its corresponding UD POS tag set \citep{petrov2011universal} and UD treebank collection \citep{ud22}, which was used for the CoNLL 2018 shared task \citep{seker2018universal}. We perform our evaluations on the four freely available French UD treebanks in UD~v2.2: GSD \citep{mcdonald13}, Sequoia\footnote{\url{https://deep-sequoia.inria.fr}} \citep{candito2012le,candito2014deep}, Spoken \citep{lacheret14,bawden14}\footnote{Speech transcript uncased that includes annotated disfluencies without punctuation}, and ParTUT \cite{sanguinetti2015PartTUT}. A brief overview of the size and content of each treebank can be found in Table \ref{treebanks-tab}.

\begin{table}[ht]
    \centering\small
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lccl}
            \toprule
            Treebank                         & \#Tokens                         & \#Sentences                     & \multicolumn{1}{l}{Genres} \\
            \midrule
                                             &                                  &                                 & Blogs, News                \\
            \multirow{-2}{*}[1.5pt]{GSD}     & \multirow{-2}{*}[1.5pt]{389,363} & \multirow{-2}{*}[1.5pt]{16,342} & Reviews, Wiki              \\ \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
                                             &                                  &                                 & Medical, News              \\
            \multirow{-2}{*}[0.7pt]{Sequoia} & \multirow{-2}{*}[0.7pt]{68,615}  & \multirow{-2}{*}[0.7pt]{3,099}  & Non-fiction, Wiki          \\ \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
            Spoken                           & 34,972                           & 2,786                           & Spoken                     \\ \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
            ParTUT                           & 27,658                           & 1,020                           & Legal, News, Wikis         \\ \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
            FTB                              & 350,930                          & 27,658                          & News                       \\
            \bottomrule
        \end{tabular}
    }
    \caption{Statistics on the treebanks used in POS tagging, dependency parsing, and NER (FTB).}\label{treebanks-tab}
\end{table}

We also evaluate our model in NER, which is a sequence labeling task predicting which words refer to real-world objects, such as people, locations, artifacts and organisations. We use the French Treebank\footnote{This dataset has only been stored and used on Inria's servers after signing the research-only agreement.} (FTB) \citep{abeille:03} in its 2008 version introduced by \citet{cc-clustering:09short} and with NER annotations by \citet{sagot2012annotation}. The FTB contains more than 11 thousand entity mentions distributed among 7 different entity types. A brief overview of the FTB can also be found in Table \ref{treebanks-tab}.

Finally, we evaluate our model on NLI, using the French part of the XNLI dataset \cite{conneau2018xnli}. NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the extension of the Multi-Genre NLI (MultiNLI) corpus \cite{williams2018broad} to 15 languages by translating the validation and test sets manually into each of those languages.
The English training set is machine translated for all languages other than English.
The dataset is composed of 122k train, 2490 development and 5010 test examples for each language.
As usual, NLI performance is evaluated using accuracy.


\paragraph{Baselines}
In dependency parsing and POS-tagging we compare our model with:

\begin{itemize}
    \item \emph{\mbert}: The multilingual cased version of \bert (see Section~\ref{contextualmodelsforotherlanguages}). We fine-tune \mbert on each of the treebanks with an additional layer for POS-tagging and dependency parsing, in the same conditions as our \camembert model.
    \item \emph{\xlmmlmtlm}: A multilingual pretrained language model from \citet{lample2019cross}, which showed better performance than \mbert on NLI. We use the version available in the Hugging's Face transformer library \cite{Wolf2019HuggingFacesTS}; like \mbert, we fine-tune it in the same conditions as our model.
    \item \emph{UDify} \cite{kondratyuk201975}: A multitask and multilingual model based on \mbert, UDify is trained simultaneously on 124 different UD treebanks, creating a single POS tagging and dependency parsing model that works across 75 different languages. We report the scores from \citet{kondratyuk201975} paper.
    \item \emph{UDPipe Future} \citep{straka2018udpipe}: An LSTM-based model ranked 3\textsuperscript{rd} in dependency parsing and 6\textsuperscript{th} in POS tagging at the CoNLL~2018 shared task \citep{seker2018universal}. We report the scores from \citet{kondratyuk201975} paper.
    \item \emph{UDPipe Future + \mbert + Flair} \citep{straka2019evaluating}: The original UDPipe Future implementation using \mbert and Flair as feature-based contextualized word embeddings. We report the scores from \citet{straka2019evaluating} paper.
\end{itemize}

%UDPipe Future+\mbert+Flair settles the state-of-the-art for most UD languages including French for both Universal POS tagging and dependency parsing. Finally, we compare our model to UDPipe Future \cite{straka2018udpipe}, a  model ranked 3rd in dependency parsing and 6th in POS tagging during the CoNLL~2018 shared task \cite{seker2018universal}. UDPipe Future acts as a strong baseline that does not make use of any pretrained contextual embedding. 

In French, no extensive work has been done on NER due to the limited availability of annotated corpora. Thus we compare our model with the only recent available baselines set by \citet{dupont2018exploration}, who trained both CRF \citep{lafferty2001conditional} and BiLSTM-CRF \citep{lample2016neural} architectures on the FTB and enhanced them using heuristics and pretrained word embeddings. Additionally, as for POS and dependency parsing, we compare our model to a fine-tuned version of \mbert for the NER task.

For XNLI, we provide the scores of \mbert which has been reported for French by \citet{wu2019beto}.
We report scores from \xlmmlmtlm (described above), the best model from \citet{lample2019cross}. %\xlmmlmtlm is smaller than \xlmmultimlm from the same authors (12 layers, hidden size 1024, 8 attention heads, 270M parameters vs.~550M parameters) but authors did not report scores for \xlmmultimlm on XNLI.
We also report the results of \mbox{XLM-R} \cite{conneau2019xlmr}.

%and the English and French Masked Language Model \footnote{we plan to release experiments on XLM-17 and XLM-R} version of XLM referred as \xlmEnFr \cite{lample2019cross}, in order to demonstrate the usefulness of a dedicated version of \bert for French. \xlmEnFr provides us the performance of a multilingual masked language model pretrained on a smaller corpora which is the concatenation of Wikipedia English and French. We finetune those models using the same architecture enhancement and optimization process as we do on \camembert. We then compare our models to UDify \cite{kondratyuk201975} and UDPipe Future+\mbert+Flair \cite{straka2019evaluating}. UDify is a multitask and multilingual model based on \mbert while UDPipe Future+\mbert+Flair is just the original UDPipe Future implementation using \mbert and Flair as contextualized word embeddings. UDify pushed dependency parsing and POS to the extreme case of training one single model based on \mbert for all UD languages reaching state-of-the-art results for many treebanks. 

%Most of the advances in NER have been achieved on English, particularly focusing on the CoNLL 2003 \cite{tjong2003introduction} and the Ontonotes v5 \cite{pradhan2012conll,pradhan2013towards} English corpora. NER is a task that was traditionally tackled using Conditional Random Fields (CRF) \cite{lafferty2001conditional} which are quite suited for NER; CRFs were later used as decoding layers for Bi-LSTM architectures \cite{huang2015bidirectional,lample2016neural} showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualised word embeddings which yet again brought major improvements to the task \cite{peters-etal-2018-deep,akbik-etal-2018-contextual}. Finally, large pretrained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures \cite{devlin-etal-2019-bert,baevski2019cloze}.

% In non-English NER the CoNLL 2002 shared task included NER corpora for Spanish and Dutch corpora \cite{tjong2002introduction} while the CoNLL 2003 included a German corpus \cite{tjong2003introduction}. Here the recent efforts of \cite{strakova2019neural} settled the state of the art for Spanish and Dutch, while \cite{akbik-etal-2018-contextual} did it for German.

%The NER-annotated FTB contains more than 12k sentences and more than 350k tokens extracted from articles of the newspaper \textit{Le Monde} published between 1989 and 1995. In total, it contains 11,636 entity mentions distributed among 7 different types of entities, namely: 2025 mentions of ``Person'', 3761 of ``Location'', 2382 of ``Organisation'', 3357 of ``Company'', 67 of ``Product'', 15 of ``POI'' (Point of Interest) and 29 of ``Fictional Character''. 

%GSD \cite{mcdonald13} is the second-largest treebank available for French after the FTB (described in subsection \ref{ner-section}), it contains data from blogs, news articles, reviews, and Wikipedia. The Sequoia treebank\footnote{\url{https://deep-sequoia.inria.fr}} \cite{candito2012le,candito2014deep} contains more than 3000 sentences, from the French Europarl, the regional newspaper \emph{L’Est Républicain}, the French Wikipedia and documents from the European Medicines Agency. Spoken is a corpus converted automatically from the Rhapsodie treebank\footnote{\url{https://www.projet-rhapsodie.fr}} \cite{lacheret14,bawden14} with manual corrections. It consists of 57 sound samples of spoken French with orthographic transcription and phonetic transcription aligned with sound (word boundaries, syllables, and phonemes), syntactic and prosodic annotations. 
%Finally, ParTUT is a conversion of a multilingual parallel treebank developed at the University of Turin, and consisting of a variety of text genres, including talks, legal texts, and Wikipedia articles, among others; ParTUT data is derived from the already-existing parallel treebank Par(allel)TUT \cite{sanguinetti2015PartTUT}. Treebanks statistics are summarized in Table~\ref{treebanks-tab}.

%We evaluate the performance of our models using the standard UPOS accuracy for POS tagging, and Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS) for dependency parsing. We assume gold tokenisation and gold word segmentation as provided in the UD treebanks. (Move to section 4, or section 3 baselines)


%\lm{I think we should still be very clear why we don't compare to SOTA}


%\lm{Is that still true?}
%We will compare to the more recent cross-lingual language model XLM \cite{lample2019cross}, as well as the state-of-the-art CoNLL 2018 shared task results with predicted tokenisation and segmentation in an updated version of the paper.


% \subsection{Named Entity Recognition}\label{ner-section}
% Named Entity Recognition (NER) is a sequence labeling task that consists in predicting which words refer to real-world objects, such as people, locations, artifacts and organisations. We use the French Treebank\footnote{This dataset has only been stored and used on Inria's servers after signing the research-only agreement.} (FTB)  \cite{abeille:03} in its 2008 version introduced by \newcite{cc-clustering:09short} and with NER annotations by \newcite{sagot2012annotation}.
% The NER-annotated FTB contains more than 12k sentences and more than 350k tokens extracted from articles of the newspaper \textit{Le Monde} published between 1989 and 1995. In total, it contains 11,636 entity mentions distributed among 7 different types of entities, namely: 2025 mentions of ``Person'', 3761 of ``Location'', 2382 of ``Organisation'', 3357 of ``Company'', 67 of ``Product'', 15 of ``POI'' (Point of Interest) and 29 of ``Fictional Character''. 

% A large proportion of the entity mentions in the treebank are multi-word entities. We therefore report the 3 metrics that are commonly used to evaluate models: precision, recall, and F1 score. Here precision measures the percentage of entities found by the system that are correctly tagged, recall measures the percentage of named entities present in the corpus that are found and the F1 score combines both precision and recall measures giving a general idea of a model's performance.

% \subsection{Natural Language Inference}
% We evaluate our model on Natural Language Inference (NLI), using the French part of the XNLI dataset \cite{conneau2018xnli}.
% NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence.

% The XNLI dataset is the extension of the Multi-Genre NLI (MultiNLI) corpus \cite{williams2018broad} to 15 languages by translating the validation and test sets manually into each of those languages.
% The English training set is machine translated for all languages.
% The dataset is composed of 122k train, 2490 valid and 5010 test examples.
% As usual, NLI performance is evaluated using accuracy.

% To evaluate a model on a language other than English (such as French), we consider the two following settings:
% \lm{Remove translate-test, it was originally included only to put RoBERTa in the table which was state of the art at the time in this setting. Not true anymore with XLM-R}
% \paragraph{TRANSLATE-TEST:} The French test set is machine translated into English, and then used with an English classification model.
% This setting provides a reasonable, although imperfect, way to circumvent the absence of data set for French, and results in strong baseline scores.
% \paragraph{TRANSLATE-TRAIN:} The French model is fine-tuned on the machine-translated French training set and then evaluated on the French test set.
% This is the setting that we use with our \camembert models.

%{\color{red} Add analysis on tokenization differences between OSCAR and CCNet}
%\bm{former previous work for 'as embedding' is commented here : SHOULD BE INTEGRATED SOMEWHERE IN PART 3}
%As it is the case with contextualized word embeddings like ELMo \cite{peters-etal-2018-deep} and Flair \cite{akbik-etal-2018-contextual}, one can use a frozen version of transformer models like BERT or RoBERTa as if they were feature based embeddings, notable examples of this type of utilization are the original BERT paper, in which the authors use BERT in this way to do NER \cite{devlin-etal-2019-bert}; the work of \newcite{straka2019evaluating} in which the English BERT and \mbert are plugged as feature-based embeddings to the UDPipe Future architecture obtaining state-of-the-art for part-of-speech tagging and dependency parsing across a wide range of languages; and the work of \newcite{strakova2019neural} which again uses both the English BERT and \mbert as feature based embeddings coupled with \newcite{lample2016neural} architecture. \bm{I remove the SOTA mention here as it's not the right place} % obtaining state-of-the-art results for both flat and nested NER in 5 different languages. 
%In order to obtain a representation for a given token, all these implementations first compute the representations of the subwords of a token by taking the mean of the representations given by the four last layers, and then generate the representation for the token by taking the mean of the representations of the subwords of a given token.



\section{\camembert: a French Language Model}\label{sec:Camembert}
In this section, we describe the pretraining data, architecture, training objective and optimisation setup we use for \camembert.


\subsection{Training data}
%\paragraph{Data description}
%\lm{Should we say "We train a model on OSCAR", put a real emphasis on OSCAR, and then rapidly, say in section 5, we compare also to CCNET and Wikipedia}
Pretrained language models benefits from being trained on large datasets \cite{devlin2018mbert,liu-etal-2019-roberta,raffel2019exploring}. %can be significantly improved by using more data
We therefore use the French part of the OSCAR corpus \cite{ortiz2019asynchronous}, a pre-filtered and pre-classified version of Common Crawl.\footnote{\url{https://commoncrawl.org/about/}} %and compare two pre-classified and pre-filtered versions of it: OSCAR \cite{ortiz2019asynchronous} and CCNet \cite{wenzek2019ccnet}. 
%%We also compare, controlling for the data set size, these two dataset to the French Wikipedia dataset. 

\paragraph{OSCAR} is a set of monolingual corpora extracted from Common Crawl snapshots\iffalse, specifically from the plain text \emph{WET} format distributed by Common Crawl which removes all the HTML tags and converts the text formatting to UTF-8\fi{}. It follows the same approach as \cite{grave2018learning} by using a language classification model based on the fastText linear classifier \cite{grave2017bag,joulin2016fasttext} pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 languages. No other filtering is done. We use a non-shuffled version of the French data, which amounts to 138GB of raw text and 32.7B tokens after subword tokenization.% (more details in Table~\ref{table:oscar_vs_ccnet}).

%% WIKIPEDIA

%\paragraph{Wikipedia}. This corpus comprises the entirety of the French Wikipedia as of April 2019. It was taken from the official Wikipedia dumps \footnote{\url{https://dumps.wikimedia.org/backup-index.html}}, and HTML tags and tables were removed using Giuseppe Attardi's template expansion tool \emph{WikiExtractor}\footnote{\url{https://github.com/attardi}}
%% no mention of 4GB  

%RANDOM SAMPLES  ref section 5 

%Both OSCAR and CCNet are sets of monolingual corpora extracted from Common Crawl snapshots, specifically from the plain text \emph{WET} format distributed by Common Crawl which removes all the HTML tags and converts the text formatting to UTF-8. OSCAR and CCnet follow the same approach as \newcite{grave2018learning} by using a language classification model for the fastText linear classifier \cite{joulin2016fasttext,grave2017bag} pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 different languages.

%Even though the pipeline are similar and they both obtain a similar amount of data in total for all the languages, 3.2 TB on both cases, there are a number on improvements made in the CCNet pipeline that can contribute to the quality of the documents inside each monolingual corpora. Mainly, CCNET is filtered with a language model trained on Wikipedia by removing sentences with low perplexity and classified in three parts \emph{head, middle and tail} based on this perplexity. 

%For instance, deduplication is performed before classification, by first normalizing the text so that the deduplication process can get rid of common website elements such as menus, cookie warnings and contact information \cite{wenzek2019ccnet}. Moreover CCNet adds an additional step for filtering ``bad'' content. They use a 5-gram Kneser-Ney model \cite{heafield2011kenlm} trained on Wikipedia (after SentencePiece tokenization) in order to filter out paragraphs with low perplexity. They split the the corpus of each language in three even parts which they call \emph{head, middle and tail}, where \emph{head} is the part with highest perplexity according to their language model. Although this steps filters out bad content, it introduce a bias towards Wikipedia-like texts in the \emph{head} part of the pretraining data. 
%OSCAR, by contrast, does not perform any language model based filtering. This results in OSCAR being much closer to the original raw Crawl, and more diverse in terms of domain, genre and text quality than CCNet. We point to \cite{wenzek2019ccnet,ortiz2019asynchronous} for more descriptions of the data.  %performs deduplication after the language classification step and does not introduce any specialized filtering scheme, they just choose to keep the paragraphs having 100 or more characters. This results in OSCAR being much closer to the original raw Crawl, and more diverse in terms of domain, genre and text quality than CCNet.


%For CCNet, we use the \textsc{head} split (top 33\% of documents in terms of filtering perplexity) of the French data. This amounts to 135GB of raw text, which we then tokenise into 31.9B SentencePiece tokens. For OSCAR we use a non shuffled version of the French data which amounts to 138GB of raw text and 32.7B SentencePiece tokens, see table~\ref{table:oscar_vs_ccnet}.



\subsection{Pre-processing}
We segment the input text data into subword units using SentencePiece \cite{kudo2018sentencepiece}.
SentencePiece is an extension of Byte-Pair encoding (BPE) \cite{sennrich2016neural} and WordPiece \cite{kudo2018subword} that does not require pre-tokenization (at the word or token level), thus removing the need for language-specific tokenisers.
We use a vocabulary size of 32k subword tokens. These subwords are learned on $10^7$ sentences sampled randomly from the pretraining dataset.
We do not use subword regularisation (i.e.~sampling from multiple possible segmentations) for the sake of simplicity.


\subsection{Language Modeling}

\paragraph{Transformer}
Similar to \roberta and \bert, \camembert is a multi-layer bidirectional Transformer \cite{vaswani2017attention}. Given the widespread usage of Transformers, we do not describe them here and refer the reader to \citep{vaswani2017attention}.
\camembert uses the original architectures of \bertbase (12 layers, 768 hidden dimensions, 12 attention heads, 110M parameters) and \bertlarge (24 layers, 1024 hidden dimensions, 16 attention heads, 335M parameters).
\camembert is very similar to \roberta, the main difference being the use of whole-word masking and the usage of SentencePiece tokenization \cite{kudo2018sentencepiece} instead of WordPiece \cite{schuster2012japanese}.

\paragraph{Pretraining Objective}
We train our model on the Masked Language Modeling (MLM) task.
Given an input text sequence composed of $N$ tokens $x_1, ..., x_N$, we select 15\% of tokens for possible replacement. Among those selected tokens, 80\% are replaced with the special \texttt{<MASK>} token, 10\% are left unchanged and 10\% are replaced by a random token. The model is then trained to predict the initial masked tokens using cross-entropy loss.

Following the \roberta approach, we dynamically mask tokens instead of fixing them statically for the whole dataset during preprocessing. This improves variability and makes the model more robust when training for multiple epochs.

Since we use SentencePiece to tokenize our corpus, the input tokens to the model are a mix of whole words and subwords.
An upgraded version of \bert\footnote{\url{https://github.com/google-research/bert/blob/master/README.md}} and \citet{joshi2019spanbert} have shown that masking whole words instead of individual subwords leads to improved performance.
Whole-word Masking (WWM) makes the training task more difficult because the model has to predict a whole word rather than predicting only part of the word given the rest.
We train our models using WWM by using whitespaces in the initial untokenized text as word delimiters.

WWM is implemented by first randomly sampling 15\% of the words in the sequence and then considering all subword tokens in each of this 15\% for candidate replacement. This amounts to a proportion of selected tokens that is close to the original 15\%.
These tokens are then either replaced by \texttt{<MASK>} tokens (80\%), left unchanged (10\%) or replaced by a random token.

Subsequent work has shown that the next sentence prediction (NSP) task originally used in \bert does not improve downstream task performance \cite{lample2019cross,liu-etal-2019-roberta}, thus we also remove it.
j

%\subsection{Optimisation} 
%integrated to experimental setup

\paragraph{Optimisation}
Following \citep{liu-etal-2019-roberta}, we optimize the model using Adam \cite{kingma2014adam} ($\beta_1 = 0.9$, $\beta_2 = 0.98$) for 100k steps with large batch sizes of 8192 sequences, each sequence containing at most 512 tokens.
We enforce each sequence to only contain complete paragraphs (which correspond to lines in the our pretraining dataset).

\paragraph{Pretraining}
We use the \roberta implementation in the fairseq library \cite{ott2019fairseq}.
Our learning rate is warmed up for 10k steps up to a peak value of $0.0007$ instead of the original $0.0001$ given our large batch size, and then fades to zero with polynomial decay.
Unless otherwise specified, our models use the BASE architecture, and are pretrained for 100k backpropagation steps on 256 Nvidia V100 GPUs (32GB each) for a day.
We do not train our models for longer due to practical considerations, even though the performance still seemed to be increasing.
%We trained models on the OSCAR, CCNet and Wikipedia datasets with Whole Word Masking\footnote{As no significant differences were observed during preliminary experiments between SVW and WWM, we only analyse WWM trained models. cf. Appendix {?}}.

\subsection{Using \camembert for downstream tasks}
We use the pretrained \camembert in two ways. In the first one, which we refer to as \textit{fine-tuning}, we fine-tune the model on a specific task in an end-to-end manner. In the second one, referred to as \textit{feature-based embeddings} or simply \textit{embeddings}, we extract frozen contextual embedding vectors from \camembert.
These two complementary approaches shed light on the quality of the pretrained hidden representations captured by \camembert.


\paragraph{Fine-tuning}
For each task, we append the relevant predictive layer on top of \camembert's  architecture. Following the work done on \bert \cite{devlin-etal-2019-bert}, for sequence tagging and sequence labeling we append a linear layer that respectively takes as input the last hidden representation of the \texttt{<s>} special token and the last hidden representation of the first subword token of each word.
For dependency parsing, we plug a bi-affine graph predictor head as inspired by \citet{dozat2017deep}. We refer the reader to this article for more details on this module.
We fine-tune on XNLI by adding a classification head composed of one hidden layer with a non-linearity and one linear projection layer, with input dropout for both.

We fine-tune \camembert independently for each task and each dataset. We optimize the model using the Adam optimiser \cite{kingma2014adam} with a fixed learning rate. We run a grid search on a combination of learning rates and batch sizes. We select the best model on the validation set out of the 30 first epochs.
For NLI we use the default hyper-parameters provided by the authors of RoBERTa on the MNLI task.\footnote{More details at \url{https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md}.}
Although this might have pushed the performances even further, we do not apply any regularisation techniques such as weight decay, learning rate warm-up or discriminative fine-tuning, except for NLI. We show that fine-tuning \camembert in a straightforward manner leads to state-of-the-art results on all tasks and outperforms the existing \bert-based models in all cases.
The POS tagging, dependency parsing, and NER experiments are run using Hugging Face's Transformer library extended to support \camembert and dependency parsing \cite{Wolf2019HuggingFacesTS}.
The NLI experiments use the fairseq library following the \roberta implementation.


\paragraph{Embeddings}

%\lm{This is the Experimental setup section, I would put more emphasis on our method instead of the litterature}
%As it is the case with contextualized word embeddings like ELMo \cite{peters-etal-2018-deep} and Flair \cite{akbik-etal-2018-contextual}, one can use a frozen version of transformer models like BERT or RoBERTa as if they were feature based embeddings, notable examples of this type of utilization are the original BERT paper, in which the authors use BERT in this way to do NER \cite{devlin-etal-2019-bert}; the work of \newcite{straka2019evaluating} in which the English BERT and \mbert are plugged as feature-based embeddings to the UDPipe Future architecture obtaining state-of-the-art for part-of-speech tagging and dependency parsing across a wide range of languages; and the work of \newcite{strakova2019neural} which again uses both the English BERT and \mbert as feature based embeddings coupled with \newcite{lample2016neural} architecture. \bm{I remove the SOTA mention here as it's not the right place} % obtaining state-of-the-art results for both flat and nested NER in 5 different languages. 
%In order to obtain a representation for a given token, all these implementations first compute the representations of the subwords of a token by taking the mean of the representations given by the four last layers, and then generate the representation for the token by taking the mean of the representations of the subwords of a given token.
Following \citet{strakova2019neural} and \citet{straka2019evaluating} for \mbert and the English BERT, we make use of \camembert in a feature-based embeddings setting.
In order to obtain a representation for a given token, we first compute the average of each sub-word’s representations in the last four layers of the Transformer, and then average the resulting sub-word vectors.
%compute the representations of the subwords of a token by taking the mean of the representations given by the four last layers, and then generate the representation for the token by taking the mean of the representations of the subwords of a given token.

We evaluate \camembert in the embeddings setting for POS tagging, dependency parsing and NER; using the open-source implementations of \newcite{straka2019evaluating} and \newcite{strakova2019neural}.\footnote{UDPipe Future is available at \url{https://github.com/CoNLL-UD-2018/UDPipe-Future}, and the code for nested NER is available at \url{https://github.com/ufal/acl2019_nested_ner}.}


%We also experiment training a BASE model longer for 500k steps, \camembertccnetlong, as advised in \cite{liu-etal-2019-roberta}. 
%Finally we trained a LARGE model for 100k steps, \camembertccnetlarge. Note that the LARGE model's performance was still improving after 100k steps, but as LARGE models are significantly slower to train, we didn't train for more, for practical constraints.
%The model trained longer and the large models are trained using CCNet due to the cleaner nature of CCNet.

%\bm{Do,e}
\section{Evaluation of \camembert}


In this section, we measure the performance of our models by evaluating them on the four aforementioned tasks: POS tagging, dependency parsing, NER and NLI.


%Table~\ref{table:all_models} summarize the models that we trained.


%\begin{table}[t]
%\centering\small
%\resizebox{\columnwidth}{!}{
%\begin{tabular}{lcccc}
%\toprule
%Model name               & Model & Training & \#training & Masking\\
%                         & type & Corpus & steps & strategy\\
%\midrule
%\camembertoscarswm       & BASE & OSCAR & 100k & subword   \\
%\camembertccnetswm       & BASE & CCNet & 100k & subword   \\
%\camembertoscar          & BASE & OSCAR & 100k & whole-word\\
%\camembertccnet          & BASE & CCNet & 100k & whole-word\\
%\camembertccnetlong      & BASE & CCNet & 500k & whole-word\\
%\camembertccnetlarge     & LARGE& CCNet & %100k & whole-word\\
%\bottomrule
%\end{tabular}
%}
%\caption{Overview of the \camembert models trained and discussed in this paper.\label{table:all_models}}
%\end{table}



\begin{table*}[ht]
    \small\centering
    \resizebox{\linewidth}{!}{
        \begin{tabu}{ l  c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c  @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c }
            \toprule
                                                                   & \multicolumn{2}{c @{\hspace{0.5cm}}}{\textsc{GSD}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Sequoia}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Spoken}} & \multicolumn{2}{c @{\hspace{0.35cm}}}{\textsc{ParTUT}}                                                                                 \\
            \cmidrule(l{2pt}r{0.4cm}){2-3}\cmidrule(l{-0.2cm}r{0.4cm}){4-5}\cmidrule(l{-0.2cm}r{0.4cm}){6-7}\cmidrule(l{-0.2cm}r{2pt}){8-9}
            \multirow{-2}{*}[1pt]{\textsc{Model}}                  & \textsc{UPOS}                                      & \textsc{LAS}                                           & \textsc{UPOS}                                         & \textsc{LAS}                                           & \textsc{UPOS}     & \textsc{LAS}      & \textsc{UPOS}     & \textsc{LAS}      \\
            \midrule
            \mbert  (fine-tuned)                                   & 97.48                                              & 89.73                                                  & 98.41                                                 & 91.24                                                  & 96.02             & 78.63             & 97.35             & 91.37             \\
            \xlmmlmtlm (fine-tuned)                                & 98.13                                              & 90.03                                                  & 98.51                                                 & 91.62                                                  & 96.18             & 80.89             & 97.39             & 89.43             \\ % 10138744 new XLM %& 97.71             & -                 & 98.51          & 91.62             & 95.29             & 74.17             & 96.84             & 89.22             \\ % 10138744 new XLM  % partut sequoia parsing 10138787 
            UDify \cite{kondratyuk201975}                          & 97.83                                              & \underline{91.45}                                      & 97.89                                                 & 90.05                                                  & 96.23             & 80.01             & 96.12             & 88.06             \\
            %\xlmEnFr & 97.51 & 93.49 & 90.72 & 98.30 &  93.62 & 91.565.42 & 96.53 & 93.03 & 90.64 \\ 
            UDPipe Future \cite{straka2018udpipe}                  & 97.63                                              & 88.06                                                  & 98.79                                                 & 90.73                                                  & 95.91             & 77.53             & 96.93             & 89.63             \\
            \: + mBERT + Flair  (emb.) \cite{straka2019evaluating} & \underline{97.98}                                  & 90.31                                                  & \textbf{99.32}                                        & 93.81                                                  & \textbf{97.23}    & \underline{81.40} & \underline{97.64} & \underline{92.47} \\
            \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
            \camembert (fine-tuned)                                & \textbf{98.18}                                     & \textbf{92.57}                                         & \underline{99.29}                                     & \textbf{94.20}                                         & 96.99             & 81.37             & \textbf{97.65}    & \textbf{93.43}    \\ % 10125734 : POS best seed PARSING gsd 10126431 PARSING other : 10126429
            UDPipe Future \mbox{+ \camembert} (embeddings)         & 97.96                                              & 90.57                                                  & 99.25                                                 & \underline{93.89}                                      & \underline{97.09} & \textbf{81.81}    & 97.50             & 92.32             \\
            \bottomrule
        \end{tabu}
    }
    \caption{\textbf{POS} and \textbf{dependency parsing} scores on 4 French treebanks, reported on test sets assuming gold tokenization and segmentation (best model selected on validation out of 4). Best scores in bold, second best underlined.}%\lm{uniformize the notations of XLM models}}%\comment{Report best seed and not average?}
    \label{tab:pos_and_dp_results}
\end{table*}

\begin{table}[ht]
    \centering \small
    %        \resizebox{\columnwidth}{!}{
    \scalebox{0.9}{
        \begin{tabu}{lc}
            \toprule
            Model                                  & F1                \\
            \midrule
            SEM (CRF) \cite{dupont2018exploration} & 85.02             \\
            LSTM-CRF \cite{dupont2018exploration}  & 85.57             \\
            \mbert (fine-tuned)                    & 87.35             \\
            \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
            \camembert (fine-tuned)                & \underline{89.08} \\% 10129644  %91.30 dev 10129153 (2 seeds only)
            LSTM+CRF+\camembert (embeddings)       & \textbf{89.55}    \\
            %            \midrule
            %            \multicolumn{2}{c}{\em Supplement: subword masking model}\\
            %            LSTM+CRF+\camembertoscarswm (embeddings)  & \textbf{90.25} \\
            \bottomrule
        \end{tabu}
    }
    \caption{\textbf{NER} scores on the FTB (best model selected on validation out of 4). Best scores in bold, second best underlined.
        \label{table:ner_ablation}}
\end{table}

\begin{table}[ht]
    \centering\small
    %    \resizebox{\columnwidth}{!}{
    \scalebox{0.89}{
        \begin{tabu}{lcc}
            \toprule
            Model                                             & Acc.             & \#Params \\
            \midrule
            %BiLSTM-max \cite{conneau2018xnli} & 68.3 & - \\
            \mbert \cite{devlin-etal-2019-bert}                      & 76.9             & 175M     \\
            \xlmmlmtlm \cite{lample2019cross}                 & \underline{80.2} & 250M     \\
            XLM-R\textsubscript{BASE} \cite{conneau2019xlmr}  & 80.1             & 270M     \\
            \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
            \camembert (fine-tuned)                           & \textbf{82.5}    & 110M     \\
            \midrule
            \multicolumn{3}{c}{\em Supplement: LARGE models}                                \\
            XLM-R\textsubscript{LARGE} \cite{conneau2019xlmr} & \underline{85.2} & 550M     \\
            \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
            \camembertccnetlarge (fine-tuned)                 & \textbf{85.7}    & 335M     \\
            \bottomrule
        \end{tabu}
    }
    \caption{\textbf{NLI} accuracy on the French XNLI test set (best model selected on validation out of 10). Best scores in bold, second best underlined.\label{table:xnli}}
\end{table}



%We report the scores of our \camembertccnetlong and \camembertccnetlarge models for all tasks. As mentioned above, \camembertccnetlong was trained 500k steps, whereas \camembertccnetlarge was trained only 100k steps, but is a LARGE model in the sense of \cite{devlin-etal-2019-bert}. Both models are trained on CCNet and use whole-word masking.

\paragraph{POS tagging and dependency parsing}
For POS tagging and dependency parsing, we compare \camembert with other models in the two settings: \textit{fine-tuning} and as \textit{feature-based embeddings}.
%On the one hand, our pretrained language model is \textit{fine-tuned} in an end-to-end manner. On the other hand, task-specific architectures that make use of \camembert or other comparable models as frozen contextual embeddings  %approach and as freezed embeddings as input to UDPipe Future, a task specific architecture  %in Table~\ref{tab:pos_and_dp_results} 
%(details of the two methods in section~\ref{subsection:pos_and_dp}).
We report the results in Table~\ref{tab:pos_and_dp_results}.

\camembert reaches state-of-the-art scores on all treebanks and metrics in both scenarios. The two approaches achieve similar scores, with a slight advantage for the fine-tuned version of \camembert, thus questioning the need for complex task-specific architectures such as UDPipe Future.

Despite a much simpler optimisation process and no task specific architecture, fine-tuning \camembert outperforms UDify on all treebanks and sometimes by a large margin (e.g. +4.15\% LAS on Sequoia and +5.37 LAS on ParTUT).
\camembert also reaches better performance  than other multilingual pretrained models such as \mbert and \xlmmlmtlm on all treebanks.%\footnote{We expected higher scores for \xlmmultimlm than \mbert given its better reported performance on the XNLI benchmark. Although we applied the exact same fine-tuning procedure to \camembert, \xlmmultimlm and \mbert, its performance in POS and dependency parsing is in some cases significantly lower}.

\camembert achieves overall slightly better results than the previous state-of-the-art and task-specific architecture UDPipe Future+\mbert+Flair, except for POS tagging on Sequoia and POS tagging on Spoken, where \camembert lags by 0.03\% and 0.14\% UPOS respectively.
UDPipe Future+\mbert+Flair uses the contextualized string embeddings Flair \citep{akbik-etal-2018-contextual}, which are in fact pretrained contextualized character-level word embeddings specifically designed to handle misspelled words as well as subword structures such as prefixes and suffixes. This design choice might explain the difference in score for POS tagging with CamemBERT, especially for the Spoken treebank where words are not capitalized, a factor that might pose a problem for CamemBERT which was trained on capitalized data, but that might be properly handle by Flair on the UDPipe Future+\mbert+Flair model.

%\camembert also demonstrates higher performances than \mbert on those tasks. We observe a larger error reduction for parsing than for tagging.% For POS tagging, we observe error reductions of respectively 0.71\% for GSD, 0.81\% for Sequoia,  0.7\% for Spoken and 0.28\% for ParTUT. For parsing, we observe error reductions in LAS of 2.96\% for GSD, 3.33\%  for Sequoia, 1.70\% for Spoken and 1.65\% for ParTUT. 
\paragraph{Named-Entity Recognition}
For NER, we similarly evaluate \camembert in the fine-tuning setting and as input embeddings to the task specific architecture LSTM+CRF. We report these scores in Table~\ref{table:ner_ablation}.

In both scenarios, \camembert achieves higher F1 scores than the traditional CRF-based architectures, both non-neural and neural, and than fine-tuned multilingual BERT models.\footnote{\xlmmlmtlm is a lower-case model. Case is crucial for NER, therefore we do not report its low performance (84.37\%)}

Using \camembert as embeddings to the traditional LSTM+CRF architecture gives slightly higher scores than by fine-tuning the model (89.08 vs.~89.55).
This demonstrates that although \camembert can be used successfully without any task-specific architecture, it can still produce high quality contextualized embeddings that might be useful in scenarios where powerful downstream architectures exist.


%Previous work with \mbert showed increased performance in NER for German, Dutch and Spanish when it is used as contextualised word embedding for an NER-specific model \cite{strakova2019neural}.
%Our results show that it can also be fine-tuning without adding any additional architecture and with better results than the baselines.
%Even though it gives worse results than our fine-tuned \camembert and our ``\camembert as embeddings'' architecture, it is still interesting to achieve this kind of performance by just fine-tuning a pretrained language model. 


\paragraph{Natural Language Inference}
On the XNLI benchmark, we compare \camembert to previous state-of-the-art multilingual models in the fine-tuning setting. In addition to the standard \camembert model with a BASE architecture, we train another model with the LARGE architecture, referred to as \camembertccnetlarge, for a fair comparison with XLM-R\textsubscript{LARGE}.
This model is trained with the \ccnet corpus, described in Sec.~\ref{sec:origin_and_size}, for 100k steps.\footnote{We train our LARGE model with the \ccnet corpus for practical reasons. Given that BASE models reach similar performance when using \oscar or \ccnet as pretraining corpus (Appendix Table~\ref{tab:ablation}), we expect an \oscar LARGE model to reach comparable scores.} We expect that training the model for longer would yield even better performance.

\camembert reaches higher accuracy than its BASE counterparts reaching +5.6\% over \mbert, +2.3 over \xlmmlmtlm, and +2.4 over XLM-R\textsubscript{BASE}. \camembert also uses as few as half as many parameters (110M vs. 270M for XLM-R\textsubscript{BASE}).

\camembertccnetlarge achieves a state-of-the-art accuracy of 85.7\% on the XNLI benchmark, as opposed to 85.2, for the recent XLM-R\textsubscript{LARGE}.

\camembert uses fewer parameters than multilingual models, mostly because of its smaller vocabulary size (e.g. 32k vs. 250k for XLM-R).
Two elements might explain the better performance of \camembert over XLM-R.
Even though XLM-R was trained on an impressive amount of data (2.5TB), only 57GB of this data is in French, whereas we used 138GB of French data.
Additionally XLM-R also handles 100 languages, and the authors show that when reducing the number of languages to 7, they can reach 82.5\% accuracy for French XNLI with their BASE architecture.



\paragraph{Summary of \camembert's results}
\camembert improves the state of the art for the 4 downstream tasks considered, thereby confirming on French the usefulness of Transformer-based models. We obtain these results when using \camembert as a fine-tuned model or when used as contextual embeddings with task-specific architectures.
This questions the need for more complex downstream architectures, similar to what was shown for English \cite{devlin-etal-2019-bert}.
Additionally, this suggests that \camembert is also able to produce high-quality representations out-of-the-box without further tuning.

\begin{table*}[ht]
    \small\centering
    \resizebox{\textwidth}{!}{
        \tabulinesep =_1pt^1pt
        \begin{tabu}{ l l @{\hspace{0.7cm}}  c  c  @{\hspace{0.7cm}} c  c  @{\hspace{0.7cm}} c  c @{\hspace{0.7cm}} c  c @{\hspace{0.7cm}} c c @{\hspace{0.7cm}} c @{\hspace{0.7cm}} c @{\hspace{0.7cm}}}
            \toprule
                                                    &                                      & \multicolumn{2}{c @{\hspace{0.5cm}}}{\textsc{GSD}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Sequoia}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Spoken}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{ParTUT}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{\textbf{Average}}} & NER               & NLI                                                                                                                            \\
            \cmidrule(l{2pt}r{0.4cm}){3-4}\cmidrule(l{-0.2cm}r{0.4cm}){5-6}\cmidrule(l{-0.2cm}r{0.4cm}){7-8}\cmidrule(l{-0.2cm}r{0.4cm}){9-10}\cmidrule(l{-0.2cm}r{0.4cm}){11-12}\cmidrule(l{-0.2cm}r{0.4cm}){13-13}\cmidrule(l{-0.2cm}r{0.4cm}){14-14}
            \multirow{-2}{*}[2pt]{\textsc{Dataset}} & \multirow{-2}{*}[2pt]{\textsc{Size}} & \textsc{UPOS}                                      & \textsc{LAS}                                           & \textsc{UPOS}                                         & \textsc{LAS}                                          & \textsc{UPOS}                                                   & \textsc{LAS}      & \textsc{UPOS}              & \textsc{LAS}      & \textsc{UPOS}     & \textsc{LAS}      & \textsc{F1}       & \textsc{Acc.}     \\
            \midrule

            \multicolumn{10}{l}{\hspace*{6mm}\em Fine-tuning}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\[0.5mm]
            %\oscar                                  & 0.1GB                                  & 98.12 & 92.28 & 98.52 &  90.09 & 96.45 &  76.01 & 95.08 & 87.49 & - & - & 83.25 & 72.98\\ 
            %100MB & 98.12 & 92.28 & 98.52 &  90.09 & 96.45 &  76.01 & 95.08 & 87.49  & 83.25 & 72.98\\
            Wiki                                    & 4GB                                  & 98.28                                              & 93.04                                                  & 98.74                                                 & 92.71                                                 & 96.61                                                           & 79.61             & 96.20                      & 89.67             & 97.45             & 88.75             & 89.86             & 78.32             \\ %  10137841 10137842 parsing, ppos 10138173  tagging
            \ccnet                                  & 4GB                                  & 98.34                                              & 93.43                                                  & 98.95                                                 & 93.67                                                 & 96.92                                                           & \textbf{82.09}    & 96.50                      & \textbf{90.98}    & 97.67             & \textbf{90.04}    & 90.46             & \textbf{82.06}    \\
            \oscar                                  & 4GB                                  & \underline{98.35}                                  & \underline{93.55}                                      & \underline{98.97}                                     & \underline{93.70}                                     & \underline{96.94}                                               & \underline{81.97} & \underline{96.58}          & 90.28             & \underline{97.71} & 89.87             & \underline{90.65} & \underline{81.88} \\
            \tabucline[\hbox{$\scriptstyle \cdot$}]{-}
            %\ccnet & 135GB & \underline{98.36} &  90.57 & 98.97 & 94.04 & 96.98 &  82.07 & 96.39  & \textbf{91.18} & 90.13 & \textbf{82.22} \\%  -  &  -  &
            \oscar                                  & 138GB                                & \textbf{98.39}                                     & \textbf{93.80}                                         & \textbf{98.99}                                        & \textbf{94.00}                                        & \textbf{97.17}                                                  & 81.18             & \textbf{96.63}             & \underline{90.56} & \textbf{97.79}    & \underline{89.88} & \textbf{91.55}    & 81.55             \\
            \midrule
            \multicolumn{11}{l}{\hspace*{6mm}\em Embeddings (with UDPipe Future (tagging, parsing) or LSTM+CRF (NER))}                                                                                                                                                                                                                                                                                                                                                                                                                          \\[0.5mm]
            %\oscar                                  & 0.1GB                                  & 98.04 & 91.95 & 98.73 & 92.60 & 96.96 & 81.02 & - & - & - & - & 89.78 & - \\ 
            Wiki                                    & 4GB                                  & 98.09                                              & 92.31                                                  & 98.74                                                 & 93.55                                                 & 96.24                                                           & 78.91             & 95.78                      & 89.79             & 97.21             & 88.64             & 91.23             & -                 \\
            \ccnet                                  & 4GB                                  & \textbf{98.22}                                     & \textbf{92.93}                                         & \underline{99.12}                                     & \underline{94.65}                                     & 97.17                                                           & \textbf{82.61}    & \underline{\textbf{96.74}} & \underline{89.95} & \underline{97.81} & \underline{90.04} & \textbf{92.30}    & -                 \\
            \oscar                                  & 4GB                                  & \underline{98.21}                                  & \underline{92.77}                                      & \underline{99.12}                                     & \textbf{94.92}                                        & \underline{97.20}                                               & \underline{82.47} & \underline{\textbf{96.74}} & \textbf{90.05}    & \textbf{97.82}    & \textbf{90.05}    & \underline{91.90} & -                 \\
            \tabucline[\hbox{$\scriptstyle \cdot$}]{-}
            %\ccnet & 135GB & 98.27   & 92.94  &   99.05  &  94.51   & 97.04  & 82.09  & \underline{96.68}  & 89.89 & 91.88  & - \\  -  &  -  &
            \oscar                                  & 138GB                                & 98.18                                              & \underline{92.77}                                      & \textbf{99.14}                                        & 94.24                                                 & \textbf{97.26}                                                  & 82.44             & 96.52                      & 89.89             & 97.77             & 89.84             & 91.83             & -                 \\

            \bottomrule
        \end{tabu}
    }
    \caption{Results on the four tasks using language models pre-trained on data sets of varying homogeneity and size, reported on validation sets (average of 4 runs for POS tagging, parsing and NER, average of 10 runs for NLI).}

    \label{tab:ablation_data_size}
\end{table*}


\section{Impact of corpus origin and size}
\label{sec:origin_and_size}

In this section we investigate the influence of the homogeneity and size of the pretraining corpus on downstream task performance. With this aim, we train alternative version of \camembert by varying the pretraining datasets. For this experiment, we fix the number of pretraining steps to 100k, and allow the number of epochs to vary accordingly (more epochs for smaller dataset sizes). All models use the BASE architecture.

In order to investigate the need for homogeneous clean data versus more diverse and possibly noisier data, we use alternative sources of pretraining data in addition to \oscar:
\begin{itemize}
    \item \textbf{Wikipedia}, which is homogeneous in terms of genre and style. We use the official 2019 French Wikipedia dumps\footnote{ \url{https://dumps.wikimedia.org/backup-index.html}.}. We remove HTML tags and tables using Giuseppe Attardi's  \emph{WikiExtractor}.\footnote{ \url{https://github.com/attardi/wikiextractor}.}
    \item \textbf{\ccnet} \cite{wenzek2019ccnet}, a dataset extracted from Common Crawl with a different filtering process than for \oscar. It was built using a language model trained on Wikipedia, in order to filter out bad quality texts such as code or tables.\footnote{We use the \textsc{head} split, which corresponds to the top 33\% of documents in terms of filtering perplexity.} As this filtering step biases the noisy data from Common Crawl to more Wikipedia-like text, we expect \ccnet to act as a middle ground between the unfiltered ``noisy'' \oscar dataset, and the ``clean'' Wikipedia dataset. As a result of the different filtering processes, \ccnet contains longer documents on average compared to \oscar with smaller---and often noisier---documents weeded out.
\end{itemize}
Table~\ref{table:corpora_statistics} summarizes statistics of these different corpora.

\begin{table}[ht]
    \centering\small
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lcccccc}
            \toprule
            Corpus    & Size  & \#tokens & \#docs & \multicolumn{3}{c}{Tokens/doc}                 \\
                      &       &          &        & \multicolumn{3}{c}{Percentiles:}               \\
                      &       &          &        & 5\%                              & 50\% & 95\% \\
            \midrule
            Wikipedia & 4GB   & 990M     & 1.4M   & 102                              & 363  & 2530 \\
            CCNet     & 135GB & 31.9B    & 33.1M  & 128                              & 414  & 2869 \\
            OSCAR     & 138GB & 32.7B    & 59.4M  & 28                               & 201  & 1946 \\
            \bottomrule
        \end{tabular}
    }
    \caption{Statistics on the pretraining datasets used.}
    \label{table:corpora_statistics}
\end{table}

In order to make the comparison between these three sources of pretraining data, we randomly sample 4GB of text (at the document level) from \oscar and \ccnet, thereby creating samples of both Common-Crawl-based corpora of the same size as the French Wikipedia. These smaller 4GB samples also provides us a way to investigate the impact of pretraining data size. Downstream task performance for our alternative versions of \camembert are provided in Table~\ref{tab:ablation_data_size}.
The upper section reports scores in the fine-tuning setting while the lower section reports scores for the embeddings.

\subsection{Common Crawl vs.~Wikipedia?}
\label{subsec:homogeneityimpact}

Table~\ref{tab:ablation_data_size} clearly shows that models trained on the 4GB versions of \oscar and \ccnet (Common Crawl) perform consistently better than the the one trained on the French Wikipedia. This is true both in the fine-tuning and embeddings setting. Unsurprisingly, the gap is larger on tasks involving texts whose genre and style are more divergent from those of Wikipedia, such as tagging and parsing on the Spoken treebank.
The performance gap is also very large on the XNLI task, probably as a consequence of the larger diversity of Common-Crawl-based corpora in terms of genres and topics. XNLI is indeed based on multiNLI which covers a range of genres of spoken and written text.

The downstream task performances of the models trained on the 4GB version of \ccnet and \oscar are much more similar.\footnote{We provide the results of a model trained on the whole \ccnet corpus in the Appendix. The conclusions are similar when comparing models trained on the full corpora: downstream results are similar when using \oscar or \ccnet.}


\subsection{How much data do you need?}
\label{subsec:sizeimpact}

An unexpected outcome of our experiments is that the model trained ``only'' on the 4GB sample of \oscar performs similarly  to the standard \camembert trained on the whole 138GB \oscar.
The only task with a large performance gap is NER, where  ``138GB'' models are better by 0.9 F1 points. This could be due to the higher number of named entities present in the larger corpora, which is beneficial for this task. On the contrary, other tasks don't seem to gain from the additional data.
%In settings where the language model is used as embeddings, the ``4GB'' model actually performs better than the standard ``138GB'' \camembert more often than the other way round, although differences in scores are rarely striking. For fine-tuning settings, the standard \camembert usually performs better than the 4GB-based one, but here again, differences are always small. 

In other words, when trained on corpora such as \oscar and \ccnet, which are heterogeneous in terms of genre and style, 4GB of uncompressed text is large enough as pretraining corpus to reach state-of-the-art results with the BASE architecure, better than those obtained with \mbert (pretrained on 60GB of text).\footnote{The OSCAR-4GB model gets slightly better XNLI accuracy than the full OSCAR-138GB model (81.88 vs. 81.55). This might be due to the random seed used for pretraining, as each model is pretrained only once.} This calls into question the need to use a very large corpus such as \oscar or \ccnet when training a monolingual Transformer-based language model such as BERT or \roberta.
Not only does this mean that the computational (and therefore environmental) cost of training a state-of-the-art language model can be reduced, but it also means that \camembert-like models can be trained for all languages for which a Common-Crawl-based corpus of 4GB or more can be created. \oscar is available in 166 languages, and provides such a corpus for 38 languages. Moreover, it is possible that slightly smaller corpora (e.g.~down to 1GB) could also prove sufficient to train high-performing language models.
We obtained our results with BASE architectures. Further research is needed to confirm the validity of our findings on larger architectures and other more complex natural language understanding tasks.
However, even with a BASE architecture and 4GB of training data, the validation loss is still decreasing beyond 100k steps (and 400 epochs). This suggests that we are still under-fitting the 4GB pretraining dataset, training longer might increase downstream performance.

\section{Discussion}

Since the pre-publication of this work \cite{martinetal2019Camembert},
many monolingual language models have appeared, e.g.
\cite{le_at_al2019flaubert,FinBert2019,RobBERT2020}, for as much as 30
languages \cite{horvyteam:2020:arxiv}. In almost all tested
configurations they displayed better results than multilingual
language models such as \mbert \cite{pires_et_al2019multilingual}.
Interestingly, \newcite{le_at_al2019flaubert} showed that using their
FlauBert, a RoBERTa-based language model for French, which was trained on less
but more edited data, in conjunction to \camembert in an ensemble
system could improve the performance of a parsing model and  establish
a new state-of-the-art in constituency parsing of French, highlighting thus
the complementarity of both models.\footnote{We refer the reader to
    \cite{le_at_al2019flaubert} for a comprehensive benchmark and details
    therein.}
As it was the case for English when \bert was first released, the
availability of similar scale language models for French enabled
interesting applications, such as large scale anonymization of legal
texts, where \camembert-based models established a new
state-of-the-art on this task \cite{bennesty:2019:anonym}, or the first
large question answering experiments on a French Squad data set that
was released very recently \cite{fquad:2020:arXiv} where the authors matched human performance using \camembertlarge.
Being the first pre-trained language model that used the open-source Common Crawl Oscar
corpus and given its impact on the community,
\camembert paved the way for many works on monolingual language
models that followed. Furthermore, the availability of all its
training data favors reproducibility and is a step towards better
understanding such models.
In that spirit, we make the models used in our experiments available via our website and via the \texttt{huggingface} and \texttt{fairseq} APIs, in addition to the base \camembert model.

\section{Conclusion}
In this work, we investigated the feasibility of training a
Transformer-based language model for languages other than English.
Using French as an example, we trained \camembert, a language model
based on \roberta.
We evaluated \camembert on four downstream tasks
(part-of-speech tagging, dependency parsing, named entity recognition
and natural language inference) in which our best model reached or improved the state of the art in all tasks
considered, even when compared to strong multilingual models such as
\mbert, XLM and XLM-R, while also having fewer parameters.

Our experiments demonstrate that using web crawled
data with high variability is preferable to using Wikipedia-based data.  In addition we
showed that our models could reach surprisingly high performances with
as low as 4GB of pretraining data, questioning thus the need for large
scale pretraining corpora.  This shows that state-of-the-art
Transformer-based language models can be trained on languages with far
fewer resources than English, whenever a few gigabytes of data are
available. This paves the way for the rise of monolingual contextual
pre-trained language-models for under-resourced languages.  The
question of knowing whether pretraining on small domain specific
content will be a better option than transfer learning techniques such
as fine-tuning remains open and we leave it for future work.

%% ds: la formulation est un peu cheloue. C con qu'il y ait plus les \draftnote et tout...

Pretrained on pure open-source corpora, \camembert is freely available and distributed with the MIT license via popular NLP libraries (\href{https://github.com/pytorch/fairseq}{\texttt{fairseq}} and \href{https://github.com/huggingface/transformers}{\texttt{huggingface}}) as well as on our website \href{https://camembert-model.fr}{\texttt{camembert-model.fr}}.

%\ds{Say something like: Since the beginning of this work, more monolingual models have appeared such as FlauBERT, finish bert... - ds: I'm on it}
%We will publish an updated version in the near future where we will explore and release models trained for longer, with additional downstream tasks, baselines (e.g. XLM) and analysis, we will also train additional models with potentially cleaner corpora such as CCNet \cite{wenzek2019ccnet} for more accurate performance evaluation and more complete ablation.


\section*{Appendix}


\begin{table*}[ht]
    \small\centering
    \scalebox{0.8}{
        \begin{tabular}{ l l c c c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c  @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c }
            \toprule
                                                    &                                         &                                       &                                         & \multicolumn{2}{c @{\hspace{0.5cm}}}{\textsc{GSD}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Sequoia}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Spoken}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{ParTUT}} & \textsc{NER}      & \textsc{NLI}                                                                                      \\
            \cmidrule(l{2pt}r{0.4cm}){5-6}\cmidrule(l{-0.2cm}r{0.4cm}){7-8}\cmidrule(l{-0.2cm}r{0.4cm}){9-10}\cmidrule(l{-0.2cm}r{0.4cm}){11-12}\cmidrule(l{-0.2cm}r{0.4cm}){13-13} \cmidrule(l{-0.2cm}r{2pt}){14-14}
            \multirow{-2}{*}[2pt]{\textsc{Dataset}} & \multirow{-2}{*}[2pt]{\textsc{Masking}} & \multirow{-2}{*}[2pt]{\textsc{Arch.}} & \multirow{-2}{*}[2pt]{\textsc{\#Steps}} & \textsc{UPOS}                                      & \textsc{LAS}                                           & \textsc{UPOS}                                         & \textsc{LAS}                                          & \textsc{UPOS}     & \textsc{LAS}      & \textsc{UPOS}     & \textsc{LAS}      & \textsc{F1}       & \textsc{Acc.}     \\
            \midrule

            \multicolumn{11}{l}{\hspace*{6mm}\em Fine-tuning}                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\[0.5mm]
            \toprule
            OSCAR                                   & Subword                                 & \textsc{Base}                         & 100k                                    & \textbf{98.25}                                     & 92.29                                                  & \underline{99.25}                                     & 93.70                                                 & 96.95             & 79.96             & \underline{97.73} & \textbf{92.68}    & 89.23             & 81.18             \\
            OSCAR                                   & Whole-word                              & \textsc{Base}                         & 100k                                    & \underline{98.21}                                  & 92.30                                                  & 99.21                                                 & \underline{94.33}                                     & 96.97             & 80.16             & \textbf{97.78}    & 92.65             & 89.11             & 81.92             \\
            CCNET                                   & Subword                                 & \textsc{Base}                         & 100k                                    & 98.02                                              & 92.06                                                  & \textbf{99.26}                                        & 94.13                                                 & 96.94             & 80.39             & 97.55             & \underline{92.66} & 89.05             & 81.77             \\
            CCNET                                   & Whole-word                              & \textsc{Base}                         & 100k                                    & 98.03                                              & \underline{\textbf{92.43}}                             & 99.18                                                 & 94.26                                                 & \underline{96.98} & \underline{80.89} & 97.46             & 92.33             & \underline{89.27} & 81.92             \\
            CCNET                                   & Whole-word                              & \textsc{Base}                         & 500k                                    & \underline{98.21}                                  & \underline{\textbf{92.43}}                             & 99.24                                                 & \textbf{94.60}                                        & 96.69             & \textbf{80.97}    & 97.65             & 92.48             & 89.08             & \underline{83.43} \\
            CCNET                                   & Whole-word                              & \textsc{Large}                        & 100k                                    & 98.01                                              & 91.09                                                  & 99.23                                                 & 93.65                                                 & \textbf{97.01}    & \underline{80.89} & 97.41             & 92.59             & \textbf{89.39}    & \textbf{85.29}    \\


            \midrule
            \multicolumn{11}{l}{\hspace*{6mm}\em Embeddings (with UDPipe Future (tagging, parsing) or LSTM+CRF (NER))}                                                                                                                                                                                                                                                                                                                                                                                                                \\[0.5mm]
            OSCAR                                   & Subword                                 & \textsc{Base}                         & 100k                                    & \underline{\textbf{98.01}}                         & 90.64                                                  & \textbf{99.27}                                        & 94.26                                                 & \underline{97.15} & \textbf{82.56}    & \textbf{97.70}    & \underline{92.70} & \textbf{90.25}    & -                 \\
            OSCAR                                   & Whole-word                              & \textsc{Base}                         & 100k                                    & 97.97                                              & 90.44                                                  & \underline{99.23}                                     & 93.93                                                 & 97.08             & 81.74             & 97.50             & 92.28             & 89.48             & -                 \\
            CCNET                                   & Subword                                 & \textsc{Base}                         & 100k                                    & 97.87                                              & \textbf{90.78}                                         & 99.20                                                 & \underline{94.33}                                     & \textbf{97.17}    & \underline{82.39} & \underline{97.54} & 92.51             & 89.38             & -                 \\
            CCNET                                   & Whole-word                              & \textsc{Base}                         & 100k                                    & 97.96                                              & \underline{90.76}                                      & \underline{99.23}                                     & \textbf{94.34}                                        & 97.04             & 82.09             & 97.39             & \textbf{92.82}    & \underline{89.85} & -                 \\
            CCNET                                   & Whole-word                              & \textsc{Base}                         & 500k                                    & 97.84                                              & 90.25                                                  & 99.14                                                 & 93.96                                                 & 97.01             & 82.17             & 97.27             & 92.28             & 89.07             & -                 \\
            CCNET                                   & Whole-word                              & \textsc{Large}                        & 100k                                    & \underline{\textbf{98.01}}                         & 90.70                                                  & \underline{99.23}                                     & 94.01                                                 & 97.04             & 82.18             & 97.31             & 92.28             & 88.76             & -                 \\
            % ablation : 
            % %% ["10125734", "10126431", "10126429", "10126640","10126641"] # 
            % large ls_pos = ["10129280","10129224"]+["10129215", "10129223"]
            % ls_long = ["10129339", "10129416", "10129415", "10129415"]
            %  ls_long gsd :  ["10129611"] ( 2 seeds only) 
            \bottomrule
        \end{tabular}}
    \caption{Performance reported on \textbf{Test sets} for all trained models (\textbf{average} over multiple fine-tuning seeds).}
    \label{tab:all_results}
\end{table*}

In the appendix, we analyse different design choices of \camembert (Table~\ref{tab:ablation}), namely with respect to the use of whole-word masking, the training dataset, the model size, and the number of training steps in complement with the analyses of the impact of corpus origin an size (Section~\ref{sec:origin_and_size}. In all the ablations, all scores come from at least 4 averaged runs. For POS tagging and dependency parsing, we average the scores on the 4 treebanks.
We also report all averaged test scores of our different models in Table~\ref{tab:all_results}.

\begin{table*}[!htbp]
    \centering\small
    \scalebox{1}{
        \begin{tabular}{lcccc @{\hspace{0.7cm}} cccc}
            \toprule
            \textsc{Dataset}     & \textsc{Masking}          & \textsc{Arch.}               & \#\textsc{Param.}   & \#\textsc{Steps}    & \textsc{UPOS}  & \textsc{LAS}   & \textsc{NER}   & \textsc{XNLI}  \\
            \midrule
            \multicolumn{9}{l}{\hspace*{6mm}\em Masking Strategy}                                                                                                                                           \\
            {\color{gray}\oscar} & Subword                   & {\color{gray}\textsc{Base}}  & {\color{gray}110M}  & {\color{gray}100k}  & 97.78          & 89.80          & \textbf{91.55} & 81.04          \\
            {\color{gray}\oscar} & Whole-word                & {\color{gray}\textsc{Base}}  & {\color{gray}110M}  & {\color{gray}100k}  & \textbf{97.79} & \textbf{89.88} & 91.44          & \textbf{81.55} \\
            \midrule
            \multicolumn{9}{l}{\hspace*{6mm}\em Model Size}                                                                                                                                                 \\
            {\color{gray}\ccnet} & {\color{gray}Whole-word}  & \textsc{Base}                & 110M                & {\color{gray}100k}  & 97.67          & 89.46          & 90.13          & 82.22          \\
            {\color{gray}\ccnet} & {\color{gray} Whole-word} & \textsc{Large}               & 335M                & {\color{gray} 100k} & \textbf{97.74} & \textbf{89.82} & \textbf{92.47} & \textbf{85.73} \\
            \midrule
            \multicolumn{9}{l}{\hspace*{6mm}\em Dataset}                                                                                                                                                    \\
            \ccnet               & {\color{gray} Whole-word} & {\color{gray}\textsc{Base}}  & {\color{gray}110M}  & {\color{gray}100k}  & 97.67          & 89.46          & 90.13          & \textbf{82.22} \\
            \oscar               & {\color{gray} Whole-word} & {\color{gray}\textsc{Base}}  & {\color{gray}110M}  & {\color{gray}100k}  & \textbf{97.79} & \textbf{89.88} & \textbf{91.44} & 81.55          \\
            \midrule
            \multicolumn{9}{l}{\hspace*{6mm}\em Number of Steps}                                                                                                                                            \\
            {\color{gray}\ccnet} & {\color{gray} Whole-word} & {\color{gray} \textsc{Base}} & {\color{gray} 110M} & 100k                & \textbf{98.04} & 89.85          & 90.13          & 82.20          \\
            {\color{gray}\ccnet} & {\color{gray} Whole-word} & {\color{gray} \textsc{Base}} & {\color{gray} 110M} & 500k                & 97.95          & \textbf{90.12} & 91.30          & \textbf{83.04} \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparing scores on the \textbf{Validation sets} of different design choices. POS tagging and parsing datasets are averaged. (average over multiple fine-tuning seeds).
        \label{tab:ablation}}
\end{table*}


\section{Impact of Whole-Word Masking}
In Table~\ref{tab:ablation}, we compare models trained using the traditional subword masking with whole-word masking.
Whole-Word Masking positively impacts downstream performances for NLI (although only by 0.5 points of accuracy). To our surprise, this Whole-Word Masking scheme does not benefit much lower level task such as Name Entity Recognition, POS tagging and Dependency Parsing.
%These very small improvements 

%As reported for English on other downstream tasks, whole word masking improves downstream performances for all tasks but NER as seen in Table \ref{ablation_table}. NER is highly sensitive to capitalisation, prefixes, suffixes and other subword features that could be used by a model to correctly identify entity mentions. Thus the added information by learning the masking at a subword level rather than at whole-word level seems to have a detrimental effect on downstream NER results. 


\section{Impact of model size}
Table~\ref{tab:ablation} compares models trained with the BASE and LARGE architectures.
These models were trained with the \ccnet corpus (135GB) for practical reasons.
We confirm the positive influence of larger models on the NLI and NER tasks. The LARGE architecture leads to respectively 19.7\% error reduction and 23.7\%.
To our surprise, on POS tagging and dependency parsing, having three time more parameters doesn't lead to a significant  difference compared to the BASE model.
\newcite{tenney-etal-2019-bert} and \newcite{jawahar2019does} have shown that low-level syntactic capabilities are learnt in lower layers of \bert while higher level semantic representations are found in upper layers of \bert.
POS tagging and dependency parsing probably do not benefit from adding more layers as the lower layers of the BASE architecture already capture what is necessary to complete these tasks.


\section{Impact of training dataset}

Table~\ref{tab:ablation} compares models trained on \ccnet and on \oscar.
The major difference between the two datasets is the additional filtering step of \ccnet that favors Wikipedia-Like texts.
The model pretrained on \oscar gets slightly better results on POS tagging and dependency parsing, but gets a larger +1.31 improvement on NER.
The \ccnet model gets better performance on NLI (+0.67).

\section{Impact of number of steps}
\label{sec:nbsteps}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{static/media/models/camembert/plot_steps_impact_4.pdf}
    \caption{Impact of number of pretraining steps on downstream performance for \camembert.}.
    \label{fig:n_steps_impact}
\end{figure}


Figure~\ref{fig:n_steps_impact} displays the evolution of downstream task performance with respect to the number of steps. % (1 step being 1 back-propagation update of a batch of 8192 sequences).
All scores in this section are averages from at least 4 runs with different random seeds. For POS tagging and dependency parsing, we also average the scores on the 4 treebanks.

We evaluate our model at every epoch (1 epoch equals 8360 steps). We report the masked language modelling perplexity along with downstream performances.
Figure~\ref{fig:n_steps_impact}, suggests that the more complex the task the more impactful the number of steps is. We observe an early plateau for dependency parsing and NER at around 22k steps, while for NLI, even if the marginal improvement with regard to pretraining steps becomes smaller, the performance is still slowly increasing at 100k steps.

In Table~\ref{tab:ablation}, we compare two models trained on \ccnet, one for 100k steps and the other for 500k steps to evaluate the influence of the total number of steps.
The model trained for 500k steps does not increase the scores much from just training for 100k steps in POS tagging and parsing.
The increase is slightly higher for XNLI (+0.84).


Those results suggest that low level syntactic representation are captured early in the language model training process while it needs more steps to extract complex semantic information as needed for NLI.