\chapter{SinNer CLEF-HIPE2020}

\section{CRFs and Contextualized Word Embeddings for NER}
\label{sec:method}


\subsection{CRF model (run3)}

SEM (Segmenteur-Étiqueteur Markovien)\footnote{available at: \url{https://github.com/YoannDupont/SEM}}\footnote{translates to: Markovian Tokenizer-Tagger (MTT).} \cite{dupont-2017-exploration} is a free NLP tool that relies on linear-chain CRFs \cite{lafferty-etal-2001-conditional} to perform tagging. SEM uses \textsc{Wapiti} \cite{lavergne-etal-2010-practical} v1.5.0\footnote{available at: \url{https://github.com/Jekub/Wapiti}} as linear-chain CRFs implementation. For this particular NER task, SEM uses the following features:
\begin{itemize}
    \item token, prefix/suffix from 1 to 5 and a Boolean isDigit features in a [-2, 2] window; %TODO: prefix, character ?
    \item previous/next common noun in sentence;
    \item 10 gazetteers (including NE lists and trigger words for NEs) applied with some priority rules in a [-2, 2] window;
    \item a ``fill-in-the-gaps'' gazetteers feature where tokens not found in any gazetteer are replaced by their POS, as described in \cite{raymond-fayolle-2010-reconnaissance}. This feature used token unigrams and token bigrams in a [-2, 2] a window.
    \item tag unigrams and bigrams.
\end{itemize}

We trained a CLEF HIPE specific model by optimizing L1 and L2 penalties on the development set. The metric used to estimate convergence of the model is the error on the development set ($1 - accuracy$).
% Our best result on the development set was obtained using the rprop algorithm, a 0.1 L1 penalty and a 0.1 L2 penalty.
For French, our optimal L1 and L2 penalties were 0.5 and 0.0001 respectively (default Wapiti parameters).
For German, our optimal L1 and L2 penalties were 1.0 and 0.0001 respectively.

One interest of SEM is that it has a built-in sentence tokenizer for French using a rule-based approach. By default, CLEF-HIPE provides a newline segmentation that is the output of the OCR. As a result, some NE mentions span across multiple segments, making it very hard to identify them correctly. It is to be expected that models trained (and labelling on) sentences would yield better performances than those trained (and labelling on) segments. SEM makes it simple to switch between different sequence segmentations, which allowed us to label sentences and output segments.
SEM's sentence segmentation engine works using mainly local rules to determine whether a token is the last of a sequence (eg: is a dot preceded by a known title abbreviation?). It also uses non-local rules to remember whether a token is between parentheses or French quotes to not segment automatically within them. Since we work at token level, we had to adapt some rules to fit CLEF-HIPE tokenization. For example, SEM decides at tokenization stage whether a dot is a strong punctuation or part of a larger token, as for abbreviations. This has the advantage of making sentence segmentation easier. CLEF-HIPE tokenization systematically separates dots, so we adapted some sentence segmentation rules, for example: we decided not to consider a dot as a sentence terminator if the previous token was in a lexica of titles or functions. No specific handling of OCR errors were done.
Another interest is that SEM has an NE mention broadcasting process. Mentions found at least once in a document are used as a gazetteer to tag unlabeled mentions within said document. When a new mention overlaps and is strictly longer than an already found mention, the new mention will replace the previous one in the document.

\subsection{ELMo-LSTM-CRF (run1 and run2)}

The LSTM-CRF is a model originally proposed by Lample et al. \cite{lample-etal-2016-neural} it consists of a Bi-LSTM encoder pre-appended by both character level word embeddings and pre-trained word embeddings, and a CRF decoder layer. For our experiments, we follow the same approach as Ortiz Suárez et al. \cite{ortiz-suarez-etal-2020-establishing} by using the Bi-LSTM-CRF implementation of Straková et al. \cite{strakova-etal-2019-neural} which is open source and readily available\footnote{Available at: \url{https://github.com/ufal/acl2019_nested_ner}.}, and pre-appending contextualized word-embeddings to the model. For French we pre-append the FrELMo model \cite{ortiz-suarez-etal-2020-establishing}, which is the standard ELMo \cite{peters-etal-2018-deep} implementation\footnote{Available at: \url{https://github.com/allenai/bilm-tf}} trained on the French OSCAR\footnote{Available at: \url{https://oscar-corpus.com}} corpus \cite{ortiz-suarez-etal-2020-monolingual} \cite{ortiz-suarez-etal-2019-asynchronous}. For German we pre-append the German ELMo \cite{may-2019-german}, which is again the standard ELMo implementation but trained on the German Wikipedia.

Contrary to the approach of Ortiz Suárez et al. \cite{ortiz-suarez-etal-2020-establishing}, we do not use the CamemBERT model \cite{martin-etal-2020-camembert} for French or the German BERT \cite{chan-etal-2019-german}. Both of these models are BERT-based and as such they are limited to a 512-token contextualized window. Moreover, they both use SentencePiece \cite{kudo-richardson-2018-sentencepiece} meaning that tokens are actually subwords, which considerably increases the number of tokens per sentence, specially for the longer ones, thus decreasing the contextual windows of both CamemBERT and the German BERT. SentencePiece also introduces the problem of a fixed-size vocabulary, which in the case of this shared task might negatively impact the performance of said models, as they could struggle handling OCR problems or just non-standard vocabulary. Since our main goal was to reconstruct the sentences and use long contextualized sequences we opted to use ELMo which can easily handle longer sequences with it's standard implementation and actually has a dynamic vocabulary thanks to the CNN character embedding layer, thus it might be better equipped to handle non-standard orthography and OCR problems.

For the fixed word embeddings we used the Common Crawl-based FastText embeddings \cite{grave-etal-2018-learning} originally trained by Facebook as opposed to the embeddings provided by the HIPE shared task, as we obtained better dev scores using the original FastText embeddings for both French and German.

We used the standard hyperparameters originally\footnote{\url{https://github.com/ufal/acl2019_nested_ner/blob/master/tagger.py\#L484}.} used by Straková et al. \cite{strakova-etal-2019-neural}. Namely a batch size of 8, a dropout of 0.5, a learning rate of 0.001 and 10 epochs. The difference between run 1 and 2, is that run 1 uses the data as is, while run 2 uses the reconstructed sentences.

\section{Results and Discussion}
\label{sec:results}

\subsection{Official shared task results}

The results of our 3 runs compared to the best run on the NERC-coarse shared-task for French and German are given in Table \ref{tab:results-raw} (strict scenario).
For both tasks, we are the third best ranking team.
% We are happy about those results as we
We only did very minimal adaptation of existing systems. We did not modify tokenization for any language. The most notable change was to use custom sentence segmentation instead of given segments for French and using some additional lexica as features for our CRF model in German (for French, we only used existing SEM lexica). Other than that, we only optimized hyper-parameters on the dev set. This clearly illustrates the power of contextual embeddings and today's neural network architectures. This is encouraging in terms of usability of SotA models on real-world data.

\begin{table}
    \centering
    \begin{tabular}{p{0.11\linewidth}x{0.11\linewidth}x{0.11\linewidth}x{0.11\linewidth}x{0.11\linewidth}x{0.11\linewidth}x{0.11\linewidth}}
        \toprule
        \multirow{2}{*}{\textsc{run}} & \multicolumn{3}{c}{\textsc{French}} & \multicolumn{3}{c}{\textsc{German}}                                                                             \\
        \cmidrule(l{0.4cm}r{0.4cm}){2-4}\cmidrule(l{0.4cm}r{0.4cm}){5-7}
                                      & P                                   & R                                   & F1               & P                & R                & F1               \\
        \midrule
        winner                        & 83.1                                & 84.9                                & 84.0             & 79.0             & 80.5             & 79.7             \\
        run 1                         & \underline{77.8}                    & \underline{79.4}                    & \underline{78.6} & \underline{63.1} & \textbf{66.6}    & \underline{64.8} \\
        run 2                         & \textbf{78.8}                       & \textbf{80.2}                       & \textbf{79.5}    & \textbf{65.8}    & \underline{65.8} & \textbf{65.8}    \\
        run 3                         & 70.2                                & 57.9                                & 63.5             & 64.4             & 43.8             & 52.1             \\
        \midrule
        average                       & 70.2                                & 66.7                                & 67.6             & 63.8             & 58.1             & 60.0             \\
        median                        & 71.5                                & 68.6                                & 68.6             & 66.8             & 57.7             & 64.5             \\
        \bottomrule
    \end{tabular}
    \caption{Strict results for our systems compared to the winning system (micro measures)}
    \label{tab:results-raw}
\end{table}

\subsection{Study of sequence segmentation}
\label{sec:sequence_seg}

%~ segments
%~ processed 29854 tokens with 1532 phrases; found: 1476 phrases; correct: 1196.
%~ accuracy: 96.58%; precision: 81.03%; recall: 78.07%; FB1: 79.52
%~  loc: precision: 85.21%; recall: 87.52%; FB1: 86.35 568
%~  org: precision: 70.62%; recall: 62.78%; FB1: 66.47 160
%~  pers: precision: 80.24%; recall: 76.88%; FB1: 78.52 663
%~  prod: precision: 62.96%; recall: 39.53%; FB1: 48.57 27
%~  time: precision: 86.21%; recall: 78.12%; FB1: 81.97 58

%~ sentences
%~ processed 29854 tokens with 1365 phrases; found: 1319 phrases; correct: 1114.
%~ accuracy: 97.61%; precision: 84.46%; recall: 81.61%; FB1: 83.01
%~  loc: precision: 87.73%; recall: 87.08%; FB1: 87.41 538
%~  org: precision: 71.33%; recall: 65.64%; FB1: 68.37 150
%~  pers: precision: 84.64%; recall: 82.09%; FB1: 83.35 547
%~  prod: precision: 75.86%; recall: 56.41%; FB1: 64.71 29
%~  time: precision: 90.91%; recall: 87.72%; FB1: 89.29 55

% \begin{table}
% \centering
% \begin{tabular}{|l|ccc|}
% \hline
% segmentation & P     & R     & F1    \\
% \hline
% segments     & 81.03 & 81.61 & 79.52 \\
% sentences    & 84.46 & 84.46 & 83.01 \\
% \hline
% \end{tabular}
% \caption{result difference between segments and SEM sentence segmentation on dev}
% \label{tab:segment-vs-sentences}
% \end{table}

In this section, we evaluate the influence of sequence segmentation on system performances. This evaluation is done for French only, as we used SEM to provide sentence segmentation and SEM could only provide a proper sentence segmentation for that language. As can be seen in table \ref{tab:segment-vs-sentences}, sentence segmentation allows to improve results by 3.5 F1 points. This is due to the fact that some entities were split across multiple segments in the original data. Using a custom sentence segmentation allows to have entities in a single sequence. This segmentation is applied both with training data and evaluation data, so that our systems can access a more proper context for named entities. The cost of using another segmentation is relatively cheap, as SEM can process nearly 1GB of raw text per hour.

A per entity comparison is also available in Table \ref{tab:segment-vs-sentences}.
One can see that the improvement of sentence segmentation is not very significant for locations (Loc). It is due to two facts : (i) locations are usually small in number of tokens and therefore less prone to be separated in two segments and (ii) there was less room from improvement since they were the easiest entity type to detect (86.35\% F1-score).
To the contrary, entities of type ``product'' (Prod), usually longer in tokens, were very hard to predict with only 48.57\% F1-measure and benefited the most from segmentation in sentences (+16 percentage points in F1-measure).


%~ 2.52 -0.44  1.06
%~ 0.71  2.86  1.90
%~ 4.40  5.21  4.83
%~ 12.90 16.88 16.14
%~ 4.70  9.60  7.32

\begin{table}
    \scalebox{0.92}{
        \begin{tabular}{@{\hspace{0.15cm}} l @{\hspace{0.2cm}}  @{\hspace{0.2cm}} r @{\hspace{0.15cm}}  @{\hspace{0.15cm}} r @{\hspace{0.2cm}}  @{\hspace{0.2cm}} r @{\hspace{0.15cm}}  @{\hspace{0.15cm}} r @{\hspace{0.2cm}}  @{\hspace{0.2cm}} r @{\hspace{0.15cm}}  @{\hspace{0.15cm}} r@ {\hspace{0.15cm}} }% YD: I like that one better
            %\begin{tabular}{lclclcl}%Better aligned this way ?
            \toprule
            \multirow{2}{*}{\textsc{Type}} & \multicolumn{2}{c}{\textsc{P}} & \multicolumn{2}{c}{\textsc{R}} & \multicolumn{2}{c}{\textsc{F1}}                                                                                                \\
            \cmidrule(l{-0.15cm}r{0.3cm}){2-3}\cmidrule(l{-0.15cm}r{0.3cm}){4-5}\cmidrule(l{-0.15cm}r{0.15cm}){6-7}
                                           & \multicolumn{1}{c}{Segments}   & \multicolumn{1}{c}{Sentences}  & \multicolumn{1}{c}{Segments}    & \multicolumn{1}{c}{Sentences} & \multicolumn{1}{c}{Segments} & \multicolumn{1}{c}{Sentences} \\
            \midrule
            Loc                            & 85.21                          & 87.73 (+2.52)                  & 87.52                           & 87.08 (-0.44)                 & 86.35                        & 87.41 (+1.06)                 \\
            Org                            & 70.62                          & 71.33 (+0.71)                  & 62.78                           & 65.64 (+2.86)                 & 66.47                        & 68.37 (+1.90)                 \\
            Pers                           & 80.24                          & 84.64 (+4.40)                  & 76.88                           & 82.09 (+5.21)                 & 78.52                        & 83.35 (+4.83)                 \\
            Prod                           & 62.96                          & 75.86 (+12.90)                 & 39.53                           & 56.41 (+16.88)                & 48.57                        & 64.71 (+16.14)                \\
            Time                           & 86.21                          & 90.91 (+4.70)                  & 78.12                           & 87.72 (+9.60)                 & 81.97                        & 89.29 (+7.32)                 \\
            \midrule
            Global                         & 81.03                          & 84.46 (+3.43)                  & 81.61                           & 84.46 (+2.85)                 & 79.52                        & 83.01 (+3.49)                 \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison between segments and sentences on French dev dataset (run 1), strict scenario}
    \label{tab:segment-vs-sentences}
\end{table}


\subsection{To dev or not to dev?}
\label{sec:todev-ornot}

In Table \ref{tab:to-dev} we show the results that could have been obtained by training the Bi-LSTM model %TODO: checker si je dis pas de conneries
on both train and dev dataset. We used the same hyperparameters as we did for our official run. Despite the fact that it does not ensure the robustness of the system, the added-value seem to be quite disappointing\footnote{In particular, if we consider that it would not have given us a better ranking on any language.}. In German the gain may be a bit more significant, probably due to the smaller size of the training dataset.

\begin{table}
    \centering
    \begin{tabular}{lx{0.15\linewidth}x{0.15\linewidth}x{0.15\linewidth}x{0.15\linewidth}}
        \toprule
        \multirow{2}{*}{\textsc{metric}} & \multicolumn{2}{c}{\textsc{french}} & \multicolumn{2}{c}{\textsc{german}}                                     \\
        \cmidrule(l{0.15cm}r{0.15cm}){2-3}\cmidrule(l{0.15cm}r{0.15cm}){4-5}
                                         & not to dev                          & to dev                              & not to dev & to dev               \\
        \midrule
        P                                & 78.8                                & \textbf{79.5} (+0.7)                & 65.8       & \textbf{68.2} (+2.4) \\
        R                                & 80.2                                & \textbf{80.7} (+0.5)                & 65.8       & \textbf{66.1} (+0.3) \\
        F1                               & 79.5                                & \textbf{80.1} (+0.6)                & 65.8       & \textbf{67.1} (+1.3) \\
        \bottomrule
    \end{tabular}
    \caption{Results obtained on the test set (strict metric) with only the train set (not to dev) and with train+dev sets (to dev) with our best system (run 2)\label{tab:to-dev}}

\end{table}

\section{Conclusion}
\label{sec:concl}

In this article we presented three methods developed for the Named Entity Recognition task in French and German historical newspapers.
The first method relied on linear-chain CRFs while the other two methods use a Bidirectional LSTM and a bidirectional Language Model (ELMo).
The later outperformed the CRF model and achieved rank 3 on the NER task in both French and German.
We also showed that the type of sequences used has a significant influence on the results. When we segment in sentences rather than using the segments of the dataset as it is the results are systematically much better, with an exception for locations where the gain is marginal. This proves that sentence segmentation remains a key component of efficient NLP architectures, in particular for models taking advantage of the context.

As a future work it would be interesting to assess the importance of noise in the data. For instance, by comparing the results of NER on texts obtained via different OCR tools.
The influence of the qualitative jumps in the data, which is common in Digital Humanities, is an important aspect to evaluate the robustness of the system in real-world conditions rather than laboratory conditions.
We also plan to provide an in-depth analysis of the impact of word embeddings and neural architecture, as we only provided our best results in this paper.
