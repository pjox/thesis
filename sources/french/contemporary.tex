%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Contemporary French Corpora}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
    \begin{minipage}{0.66\textwidth}
        \begin{small}
            In which we present a part of the work of \citet{popa-fabre-etal-2020-french} who construct a balanced corpus for contemporary French that could be used for language modeling; and of \citet{ortiz-suarez-etal-2020-establishing} who aligned both the Universal Dependencies and the TEI-annotated NER version of the French Treebank, correcting multiple annotation mistakes and discrepancies, and who then converted the NER annotations to a more machine-ready CoNLL-like format that is more often used for training neural models.
        \end{small}
    \end{minipage}
    \vspace{0.5cm}
\end{center}

Having constructed a multilingual corpus out of web data that was in theory big enough to train a state-of-the-art language model \citep{liu-etal-2019-roberta} for a wide range of languages, and having addressed some of the quality concerns that some researchers had expressed about this type of corpus. We wanted to focus a little more on constructing resources specifically for Contemporary and Historical French, as this was the originally intended task when we first started working on OSCAR, which was always intended to be a French corpus only, but that ended being multilingual due to the multilingual nature of Common Crawl.

In this chapter we will present \emph{CaBeRnet} \citep{popa-fabre-etal-2020-french} A Contemporary French Balanced Corpus that is orders of magnitude smaller than the French OSCAR sub-corpus, but that as opposed to OSCAR, it is manually curated and specifically designed to be a linguistically balanced cross-genre corpus for the French language. We will also briefly present the work of \citet{ortiz-suarez-etal-2020-establishing} who aligned both the Universal Dependencies and the TEI-annotated NER version of the French Treebank, giving us a more consistent a more user-friendly NER French corpus that will be used for evaluation in later chapters.

\section{Contemporary French Balanced Corpora}
\label{sec:DescribeCorpora}

While working on OSCAR 2019,\footnote{All the work on \Cabernet was conducted prior to the existence of OSCAR 21.09 and OSCAR 22.01. As such, all the mentions of OSCAR in this chapter refer to OSCAR 2019.} the question of quality versus size of corpus caught our attention. We wanted to study in particular the issue of corpus ``representativeness'' in order to grasp to what extent a linguistically balanced cross-genre language sample would be sufficient to pre-train a language model. Here for ``representativeness'' we follow Biber's definition: \emph{“representativeness refers to the extent to which a sample includes the full range of variability in a population”} \citep{biber-1993-representativeness}.

To construct our corpora we adopt a balanced approach by sampling a wide spectrum of language use and its cross-genre variability, be it situational (e.g. format, author, addressee, purposes, settings or topics) or linguistic, e.g. linked to distributional parameters like frequencies of word classes and genres. In this fashion, we developed two corpora:
\begin{enumerate}
    \item The French Balanced Reference Corpus (\emph{CaBeRnet}), which includes a wide-ranging and balanced coverage of cross-genre language use to be maximally representative of the French language and therefore yield good generalizations from.
    \item The \emph{French Children Book Test} (CBT-fr), which includes both narrative material and oral language use as present in youth literature, and which could be used for domain-specific language model training.
\end{enumerate}

Both corpora are inspired by existing American and English corpora, respectively  COCA, the balanced Corpus of Contemporary American English \citep{davies-2009-the, davies-2010-the}, and the Children Book Test \citep[CBT]{hill-etal-2016-the}.

\subsection{\Cabernet} \label{subsec:DescribeCaBeRnet}

The \Cabernet corpus was inspired by the genre partition of the American balanced corpus COCA, \footnote{\url{https://www.english-corpora.org/coca/}} which at the end of 2019, when this study was conducted, contained over 618 million words of text (20 million words each year 1990-2019) and was equally divided among spoken, fiction, popular magazines, newspapers, and academic texts \citep{davies-2009-the, davies-2010-the}. A second reference, guiding our approach and sampling method, was one of the earliest precursors of balanced reference corpora: the BNC \citep{bnc-2007-the}, which covered a wide variety of genres, with the intention to be a representative sample of spoken and written language.

\begin{table}[ht]
    \centering\small
    \begin{tabular}{lrrr}                                                                                      \\\toprule
        {\textsc{\Cabernet Sub-set}} & {\textsc{Tokens}} & {\textsc{Unique Forms}} & {\textsc{TTR}} \\\midrule
        Oral                         & 122 864 888       & 291 744                 & 0.0024         \\
        Popular                      & 131 444 017       & 458 521                 & 0.0035         \\
        News                         & 132 708 943       & 462 971                 & 0.0035         \\
        Fiction                      & 198 343 802       & 983 195                 & 0.0050         \\
        Academic                     & 126 431 211       & 1 433 663               & 0.0113         \\
        \textit{Total}               & 711 792 861       & 2 558 513               & 0.0036         \\ \bottomrule
    \end{tabular}
    \caption{\label{Table_Morpho_CabernetSub} Comparison of number of unique forms in the different genres represented by \Cabernet partition. TTR: Type-Token Ration. Lemmatization and tokenization was performed as described in §\ref{sec:CompareCorpora}.}
\end{table}

\Cabernet was obtained by compiling existing data-sets and web-text extracted from different sources as detailed in this subsection. As shown in Table \ref{Table_Morpho_CabernetSub}, genres sources are evenly divided ($\sim$120 million words each) into spoken, fiction, magazine, newspaper, academic to achieve genre-balanced between oral and written modality in newspapers and popular written style, technical reports and Wikipedia entries, fiction, literature and academic production.

\paragraph{\Cabernet Oral} \label{subsec:DescribeCaBeRnetOral}
The oral sub-portion gathers both oral transcriptions (\textsc{ORFEO} and Rhapsodie\footnote{\textsc{ORFEO} corpus available at \url{www.cocoon.huma-num.fr/exist/crdo/} ; Rhapsodie corpus at \url{www.projet-rhapsodie.fr}.}) and Films subtitles (Open Subtitles.org), pruned from diacritics, interlocutors tagging and time stamps. To these transcriptions, we add the French European Parliament Proceedings (1996-2011), as presented in \citet{koehn-2005-europarl}, which contribute a sample of more complex oral style with longer sentences and richer vocabulary.

\paragraph{\Cabernet Popular Press} \label{subsec:DescribeCaBeRnetPop}
The whole sub-portion of Popular Press is gathered from an open data-set from the \textit{Est Républicain} (1999, 2002 and 2003), a regional press format\footnote{Corpus available at \url{www.cnrtl.fr/corpus/estrepublicain/}.}. It was selected to match popular style as it is characterized by easy-to-read press style and a wide range of every-day topics characterizing local regional french press.

\paragraph{\Cabernet Fiction \& Literature} \label{subsec:DescribeCaBeRnetFic}
The Fiction \& Literature sub-portion was compiled from March 2019's Wiki Source and WikiBooks dump and extracted using WikiExtractor.py, a script that extracts and cleans text from a WikiMedia database dumps, by performing template expansion and preprocessing of template definitions.\footnote{Script available at \url{https://github.com/attardi/wikiextractor}.}

\paragraph{\Cabernet News} \label{subsec:DescribeCaBeRnetNews}
The News sub-portion builds upon web crawled elements, including Wikimedia's NewsComments and WikiNews reports from the May 2019 WikiMedia dump, collected with a custom version of WikiExtractor.py. We also add newspaper's content gathered by the Chambers-Rostand Corpus (i.e. Le Monde 2002-2003, La Dépèche 2002-2003, L'Humanité 2002-2003) and \emph{Le Monde diplomatique}. This open-source corpora were assembled to represent a higher register of written news style from different political and thematic horizons. Several months of French Press Agency reports are also added (AFP, 2007-2011-2012), which contribute with a more simple and telegraphic style than the others newspaper written samples of the corpus.\footnote{This part of \Cabernet corpus is still subject to License restrictions. However, this restricted amount of AFP news reports can reasonably fall in the public domain.}

\paragraph{\Cabernet Academic} \label{subsec:DescribeCaBeRnetAcad}
The academic genre was also built from different sources including technical and educational texts from WikiBooks and Wikipedia dump (prior to 2016) for their thematic variety of highly specialized written production. The \textsc{ORFEO} Corpus offered a small sample of academic writings like PhD dissertations and scientific articles encompassing a wide choice of disciplinary topics, and the TALN Corpus\footnote{TALN proceedings corpus (about 2 million) builds on a subset of 586 scientific articles (from 2007 to 2013), namely TALN and RECITAL. Available at \url{redac.univ-tlse2.fr/corpus/taln_en.html}.} was included to represent more concise written style characterizing scientific abstracts and proceedings.

For all sub-portions of \Cabernet, visual inspection was performed to remove section titles, redundant meta-information linked to publishing schemes of each of the six news editor included. This was manually achieved by compiling a rich set of regular expressions specific of each textual source to obtain clean plain text as an output.

\subsection{French Children Book Test (CBT-fr)}
\label{subsec:DescribeCBT}

The French Children Book Test (CBT-fr) was built upon its original English version, the Children Book Test (CBT) \citep{hill-etal-2016-the}\footnote{This data-set can be found at \url{www.fb.ai/babi/}.}, which consists of books freely available from Project Gutenberg. \footnote{\url{{www.gutenberg.org}.}}

Using youth literature and children books guarantees a clear narrative structure, and a large amount of dialogues, which enriches with oral register the literary style of this corpus. The English version of this corpus was originally built as a benchmark data-set to test how well language models capture meaning in context. It contains 108 books, and a vocabulary size of 53,628 tokens.

The French version of CBT, named CBT-fr, was constructed to guarantee enough linguistic similarities between the collected books in the two languages. 104 freely available books were included. One third of the books were purposely chosen because they were classical translations of English literary classics. Chapter heads, titles, notes and all types of editorial information were removed to obtain a plain narrative text. The effort of keeping proportion, genre, domain, and time as equal as possible yields a multilingual set of comparable corpora with a similar balance and representativeness.

\begin{table}[ht]
    \centering\small
    \begin{tabular}{lr}                                                             \\\toprule
        {\textsc{Children Book Test - fr}}           & { \textsc{Words}} \\\midrule
        Number of different lemmas                   & 25 139            \\
        Total number of forms                        & 95 058            \\
        Mean number of forms per lemma               & 3.78              \\
        Number of lemmas having more than one form : & 14 128            \\
        Percentage of lemmas with multiple forms     & 56.20             \\
        \bottomrule
    \end{tabular}
    \caption{\label{Table_DescribeCBTfr} Lexical statistics of French CBT, performed as described in §\ref{sec:CompareCorpora}}
\end{table}

\subsection{Corpora Descriptive Comparison} \label{sec:CompareCorpora}

Having put together these two different balanced corpora, we wanted to perform a descriptive comparison between them, the French subcorpus of OSCAR 2019 (that we call OSCAR-fr for short) and Wikipedia (Wikipedia-fr). In order to perform this comparison we start by tokenizing all corpora. For this we used two different tokenizers: A standalone version of SEM, (Segmenteur-Étiqueteur Markovien) \citep{dupont-2017-exploration} and TreeTagger \citep{schmid-1999-improvements}. Both are based on cascades of regular expressions, and both perform tokenization and sentence splitting. The first was used for descriptive purposes because it technically allowed to segment and tokenize all corpora including OSCAR (23 billion words). Hence, all corpora were entirely segmented into sentences and tokenized using SEM.

While the second tokenization method was only run on 3 million words samples to automatically tag them with TreeTagger into part-of-speech and lemmatize them.\footnote{Based on the tag-set available at \url{https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/french-tagset.html}.} All corpora were randomly shuffled by sentence to then select samples of 3 million words, to be able to compare them in terms of lexical composition (Type-Token Ratio, see Table \ref{Table_MorphoRich}).

For Wikipedia-fr in particular we use a dump executed from April 2019, where HTML tags and tables were removed, together with template expansion using Attardi's tool WikiExtractor.

\subsubsection{Corpora Size and Composition}

Length of sentences is a simple measure to quantify both sentence syntactic complexity and genre. Hence, the number of sentences reported in Table \ref{Table_nb_Words} shows interesting patterns of distributions across genres.

\begin{table}[ht]
    \centering
    \begin{tabular}{lrrr}                                                                                 \\\toprule
        {\textsc{corpus}} & { \textsc{wordforms}} & { \textsc{tokens}} & { \textsc{sentences}} \\\midrule
        OSCAR-fr          & 23 212 459 287        & 27 439 082 933     & 1 003 261 066         \\
        Wiki-fr           & 665 599 545           & 802 283 130        & 21 775 351            \\
        \Cabernet         & 697 119 013           & 830 894 133        & 54 216 010            \\
        CBT-fr            & 5 697 584             & 6 910 201          & 317 239               \\\bottomrule
        %frWac       &  1,357,598,417  & 1,622,619,337  &  57,236,199  \\  
    \end{tabular}
    \caption{\label{Table_nb_Words} Comparing the corpora under study.}
\end{table}

As reported on Table \ref{Table_nb_Words}, in the Wikipedia-fr dataset (660 million words) sentences are relatively longer compared to other corpora. It has the advantage of having a comparable size to \Cabernet, but its homogeneity in terms of written genre is limited to Wikipedia's entries descriptive style.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Corpora Lexical Variety}

We also try to find a useful measure of complexity that measures lexical richness or variety in vocabulary. For this, we present the type-token ration (TTR) of the corpora we analyze. This measure, is generally used to assess language use aspects, like the amount of words used to communicate by language learners or children, it represents the total number of unique words (types/forms) divided by the total number of tokens in a given sample of text. Thus, the closer the TTR ratio is to 1, the greater the lexical richness of the corpus. Table \ref{Table_Morpho_CabernetSub} summarizes the lexical variety of the five sub-portions of \Cabernet, respectively taken as representative of Oral, Popular, Fiction, News, and Academic genres. 

Domain diversity of texts can be observed in the lexical statistics showing a gradual increase in the number of distinct lexical forms (cf. TTR). This pattern  reflects a generally acknowledged distributional pattern of vocabulary-size across genres. Oral style shows a poorer lexical variety compared to newspapers/magazines’ textual typology. The lexically rich fictional/classic literature is outreached by academic writing-style with its wide-ranging specialized vocabulary. All in all, Table \ref{Table_Morpho_CabernetSub} quantitatively suggests that the selected textual and oral materials are indeed representative of the five types of genres of CaBeRnet.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Corpora Morphological richness}

To select a measure that would help quantifying the different corpora morphological richness, we follow \citep{bonami-etal-2015-implicative}. Hence, the proportion of lemmas with multiple forms in a given vocabulary size was evaluated on randomly selected samples of 3-million-words from each of the analyzed corpora (see Table \ref{Table_MorphoRich}).

\begin{table}[ht]
    \centering
    \begin{tabular}{lrrrr}
        \toprule
        \textsc{3 M samples}  & \textsc{CBT-fr} & \textsc{\Cabernet} & \textsc{Wiki-fr} & \textsc{OSCAR-fr} \\
        \midrule
        nb of diff. lemmas    & 25 139          & 30 488             & 31 385           & 31 204         \\
        tot. nb forms         & 95 058          & 180 089            & 238 121          & 190 078        \\
        mean nb forms/lemma   & 3.78            & 6.19               & 7.85             & 6.40           \\
        nb lemmas $>$ 1 form  & 14 128          & 15 927             & 15 182           & 16 480         \\
        \% lemmas  $>$ 1 form & 56.20           & 52.24              & 48.37            & 52.81          \\
        \bottomrule
    \end{tabular}
    \caption{Lexical statistics on morphological richness over randomly selected samples of 3 million words from each corpus. nb : number}
    \label{Table_MorphoRich}
\end{table}

Table \ref{Table_MorphoRich} reports some more in-depth lexical and morphological statistics across corpora. Here we see that, although OSCAR is 34 times bigger than CaBeRnet, their total number of forms and the proportion of lemmas having more than one form in a 3-million-word sample are quite similar. FrWiki shows a radically different lexical distribution with numerous hapaxes but a lower morphological richness. Although its total number of forms is more than one third higher than in OSCAR and CaBeRnet samples, the proportion of lemmas having more than one distinct form is around four points below CaBeRnet and OSCAR. Comparatively, youth literature in CBT-fr shows the greatest morphological richness, with around 56\% of lemmas having more than one form.

Having performed this descriptive evaluation, we will evaluate how these corpora perform as pre-training datasets for language models in the following part of the thesis. For now, we will present a small improvement that we contributed to an existing Named Entity Recognition corpus in French.

\section{A named entity annotation layer for the UD version of the French TreeBank}

\citet{sagot-etal-2012-annotation} describe the addition to the French Treebank (FTB) \citep{abeille-etal-2003-building} in its FTB-UC version \citep{candito-etal-2010-statistical} of a new, freely available annotation layer providing named entity information in terms of span and type (NER) as well as reference (NE linking), using the Wikipedia-based \aleda \cite{sagot-stern-2012-aleda} as a reference entity database. This was the first freely available French corpus annotated with referential named entity information and the first freely available such corpus for the written journalistic genre. However, this annotation is provided in the form of an XML-annotated (TEI-annotated) text with sentence boundaries but no tokenization.

Since the publication of that named entity FTB annotation layer, the field has evolved in many ways. Firstly, most treebanks are now available as part of the \emph{Universal Dependencies} (UD)\footnote{\url{https://universaldependencies.org}} treebank collection \citep{zeman-etal-2021-universal}. Secondly, neural approaches have considerably improved the state of the art in natural language processing in general and in NER in particular. In this regard, the emergence of contextual language models has played a major role. However, surprisingly few neural French NER systems have been published.\footnote{Apart from the systems we will present in the last part of this thesis, we are only aware of the \emph{entity-fishing} NER (and NE linking) system developed by Patrice Lopez, a \href{https://github.com/kermitt2/entity-fishing}{freely available} yet unpublished system.} This might be because of the fact that getting access to the FTB with its named entity layer as well as using this corpus were not straightforward tasks.

For a number of technical reasons, re-aligning the XML-format named entity FTB annotation layer created by \citet{sagot-etal-2012-annotation} with the \emph{``official''} version of the FTB or, later, with the version of the FTB provided within the Universal Dependency (UD) framework was not a straightforward task.\footnote{Note that the UD version of the FTB is freely downloadable, but does not include the original tokens or lemmas. Only people with access to the original FTB can restore this information, as required by the  intellectual property status of the source text.} Moreover, due to the intellectual property status of the source text in the FTB, the named entity annotations could only be provided to people having signed the FTB license, which prevented them from being made freely downloadable online.

Our goal in this section is to prove a new, easy-to-use UD-aligned version of the named entity annotation layer in the FTB. We describe the process whereby we re-aligned the named entity FTB annotations by \citet{sagot-etal-2012-annotation} with the UD version of the FTB \citep{candito-etal-2010-statistical}. This makes it possible to share these annotations in the form of a set of additional columns that can easily be pasted to the UD FTB file. This new version of the named entity FTB layer is much more readily usable than the original TEI encoded version, and will serve as a basis for our experiments in the last part of this thesis.


\subsection{Alignment to the UD version of the FTB}
\label{subsec:alignment}

The named entity (NE) annotation layer for the FTB was developed using an XML editor on the raw text of the FTB. Annotations are provided as inline XML elements within the sentence-segmented but non tokenized text. For creating our NER models, we first had to align these XML annotations with the already tokenized UD version of FTB.

Sentences were provided in the same order for both corpora, so we did not have to align them. For each sentence, we created a mapping $M$ between the raw text of the NE-annotated FTB (i.e.~after having removed all XML annotations) and tokens in the UD version of the FTB corpus. More precisely, character offsets in the FTB-NE raw text were mapped to token offsets in the tokenized FTB-UD. This alignment was done using case-insensitive character-based comparisons and were a mapping of a span in the raw text to a span in the tokenized corpus.

We used the inline XML annotations to create offline, character-level NE annotations for each sentence, and reported the NE annotations at the token level in the FTB-UD using the mapping $M$ obtained.

We logged each error (i.e.~an unaligned NE or token) and then manually corrected the corpora, as those cases were always errors in either corpora and not alignment errors. Likewise, we found 70 errors in FTB-NE and 3 errors in FTB-UD. Errors in FTB-NE were mainly XML entity problems (unhandled "\&", for instance) or slightly altered text (for example, a missing comma). Errors in FTB-UD were probably the result of some XML artifacts.

\section{Conclusion}
In this chapter we have presented two balanced corpora for Contemporary French, these corpora will be used as pre-training datasets for language models in the next part of the thesis and will serve as both a baseline and a benchmark for assessing the quality of OSCAR as a pre-training corpus, at least for French.

We also presented an alignment to the UD version of the French Treebank, that can be considered as a small quality-of-life improvement that will facilitate the usage of this dataset as an evaluation of the neural models that we will train in the upcoming part of the thesis. We also believe that this new version of the NER annotated FTB will be useful to other researcher that would like to evaluate NER architectures on this dataset.
