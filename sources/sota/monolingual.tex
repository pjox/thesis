%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Monolingual Related Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since the introduction of \emph{word2vec} \citep{mikolov-etal-2013-distributed}, many attempts have been made to create multilingual language representations; for fixed word embeddings the most remarkable works are those of \citep{al-rfou-etal-2013-polyglot} and \citep{bojanowski-etal-2017-enriching} who created word embeddings for a large quantity of languages using Wikipedia, and later \citep{grave-etal-2018-learning} who trained the fastText word embeddings for 157 languages using Common Crawl and who in fact showed that using crawled data significantly increased the performance of the embeddings especially for mid- to low-resource languages.

Regarding contextualized models, the most notable non-English contribution has been that of the mBERT \citep{devlin-etal-2019-bert}, which is distributed as (i)~a single multilingual model for 100 different languages trained on Wikipedia data, and as (ii)~a single multilingual model for both Simplified and Traditional Chinese. Four monolingual fully trained ELMo models have been distributed for Japanese, Portuguese, German and Basque\footnote{\url{https://allennlp.org/elmo}}; 44 monolingual ELMo models\footnote{\url{https://github.com/HIT-SCIR/ELMoForManyLangs}} where also released by the \emph{HIT-SCIR} team \citep{che-etal-2018-towards} during the \emph{CoNLL 2018 Shared Task} \citep{zeman-etal-2018-conll}, but their training sets where capped at 20 million words. A German BERT \citep{chan-etal-2019-german} as well as a French BERT model (called CamemBERT) \citep{martin-etal-2020-camembert} have also been released. In general no particular effort in creating a set of high-quality monolingual contextualized representations has been shown yet, or at least not on a scale that is comparable with what was done for fixed word embeddings.

For dependency parsing and POS tagging the most notable non-English specific contribution is that of the \emph{CoNLL 2018 Shared Task} \citep{zeman-etal-2018-conll}, where the 1\textsuperscript{st} place (LAS Ranking) was awarded to the \emph{HIT-SCIR} team \citep{che-etal-2018-towards} who used \citet{dozat-manning-2017-deep}'s \emph{Deep Bi-affine parser} and its extension described in \citep{dozat-etal-2017-stanfords}, coupled with deep contextualized ELMo embeddings \citep{peters-etal-2018-deep} (capping the training set at 20 million words). The 1\textsuperscript{st} place in universal POS tagging was awarded to \citet{smith-etal-2018-82} who used two separate instances of \citet{bohnet-etal-2018-morphosyntactic}'s tagger.

More recent developments in POS tagging and parsing include those of \citet{straka-strakova-2019-evaluating} which couples another CoNLL 2018 shared task participant, UDPipe 2.0 \citep{straka-2018-udpipe}, with mBERT greatly improving the scores of the original model, and UDify \citep{kondratyuk-straka-2019-75}, which adds an extra attention layer on top of mBERT plus a Deep Bi-affine attention layer for dependency parsing and a Softmax layer for POS tagging. UDify is actually trained by concatenating the training sets of 124 different UD treebanks, creating a single POS tagging and dependency parsing model that works across 75 different languages.
