\chapter{A First Evaluation of the OSCAR Corpus}
\label{chap:monolingual}

\begin{center}
    \begin{minipage}{0.66\textwidth}
        \begin{small}
            In which we present the work of \citet{ortiz-suarez-etal-2020-monolingual}, who propose the first evaluation of OSCAR 2019 as a pretraining corpus for language modeling. This evaluation was done by selecting OSCAR subcorpora for 5 morphologically and tipologically different mid-ressource languages and pre-training monolingual ELMo models \citep{peters-etal-2018-deep} for each of them. These ELMo models are then attached to the UDPipe 2.0 architecture \citep{straka-2018-udpipe,straka-strakova-2019-evaluating} and evaluated in dependency parsing and POS tagging.
        \end{small}
    \end{minipage}
    \vspace{0.5cm}
\end{center}

Having released OSCAR 2019, the first thing that we wanted to do with it was to evaluate how good it actually was for what it was mainly intended, that is, the pre-training of contextualized word embeddings that had just become available at the time we started working on OSCAR 2019. Such models included ULMFiT \citep{howard-ruder-2018-universal}, ELMo \citep{peters-etal-2018-deep} and BERT \citep{devlin-etal-2019-bert} among others at that time. For this first evaluation we decided to train ELMo contextualized word embeddings for 5 languages: Bulgarian, Catalan, Danish, Finnish and Indonesian. We train one set of embeddings using only Wikipedia data, and another set using only OSCAR 2019 data. We chose these languages primarily because they are morphologically and typologically different from one another, but also because all of the OSCAR 2019 subcorpora for these languages were of a sufficiently manageable size such that the ELMo pre-training was doable in less than one month. Contrary to \emph{HIT-SCIR} team \citep{che-etal-2018-towards}, we do not impose any cap on the amount of data, and instead use the entirety of Wikipedia or OSCAR 2019 for each of our 5 chosen languages.

\section{Corpora}

\begin{table}[t!]
    \centering\small
    \scalebox{0.91}{
        \begin{tabular}{lrrrr}\toprule
            Language   & \multicolumn{1}{l}{Size} & \multicolumn{1}{l}{\#Ktokens} & \multicolumn{1}{l}{\#Kwords} & \multicolumn{1}{l}{\#Ksentences} \\ \midrule
            Bulgarian  & 609M                     & 64,190                        & 54,748                       & 3,685                            \\
            Catalan    & 1.1G                     & 211,627                       & 179,108                      & 8,293                            \\
            Danish     & 338M                     & 60,644                        & 52,538                       & 3,226                            \\
            Finnish    & 669M                     & 89,580                        & 76,035                       & 6,847                            \\
            Indonesian & 488M                     & 80,809                        & 68,955                       & 4,298                            \\
            \bottomrule
        \end{tabular}
    }
    \caption{Size of Wikipedia corpora, measured in bytes, thousands of tokens, words and sentences.}
    \label{tab:Wikipedia}
\end{table}

Wikipedia is the biggest online multilingual open encyclopedia, comprising more than 40 million articles in 301 different languages. Because articles are curated by language and written in an open collaboration model, its text tends to be of very high-quality in comparison to other free online resources. This is why Wikipedia has been extensively used in various NLP applications \citep{wu-weld-2010-open,mihalcea-2007-using,al-rfou-etal-2013-polyglot,bojanowski-etal-2017-enriching}. We downloaded the XML Wikipedia dumps\footnote{XML dumps from April 4, 2019.} and extracted the plain-text from them using the \texttt{wikiextractor.py} script\footnote{Available \href{https://github.com/attardi/wikiextractor}{here}.} from Giuseppe Attardi. We present the number of words and tokens available for each of our 5 languages in Table \ref{tab:Wikipedia}. We decided against deduplicating the Wikipedia data as the corpora are already quite small. We tokenize the 5 corpora using \emph{UDPipe} \citep{straka-strakova-2017-tokenizing}.

As we did for Wikipedia, we tokenize OSCAR 2019 subcorpora for the 5 languages we chose for our study using UDPipe. Table \ref{tab:CC} provides quantitative information about the 5 resulting tokenized corpora.

\begin{table}[t]
    \centering\small
    \scalebox{0.91}{
        \begin{tabular}{lrrrr}\toprule
            Language   & \multicolumn{1}{l}{Size} & \multicolumn{1}{l}{\#Ktokens} & \multicolumn{1}{l}{\#Kwords} & \multicolumn{1}{l}{\#Ksentences} \\ \midrule
            Bulgarian  & 14G                      & 1,466,051                     & 1,268,115                    & 82,532                           \\
            Catalan    & 4.3G                     & 831,039                       & 729,333                      & 31,732                           \\
            Danish     & 9.7G                     & 1,828,881                     & 1,620,091                    & 99,766                           \\
            Finnish    & 14G                      & 1,854,440                     & 1,597,856                    & 142,215                          \\
            Indonesian & 16G                      & 2,701,627                     & 2,394,958                    & 140,138                          \\
            \bottomrule
        \end{tabular}
    }
    \caption{Size of OSCAR 2019 subcorpora, measured in bytes, thousands of tokens, words and sentences.}
    \label{tab:CC}
\end{table}

\subsection{Noisiness}

We wanted to address \citep{trinh-le-2018-a} and \citep{radford-etal-2019-language}'s criticisms of Common Crawl, so we devised a simple method to measure how noisy the OSCAR 2019 subcorpora were for our 5 languages. We randomly extract a number of lines from each corpus, such that the resulting random sample contains one million words.\footnote{We remove tokens that are capitalized or contain less than 4 UTF-8 encoded characters, allowing us to remove bias against Wikipedia, which traditionally contains a large quantity of proper nouns and acronyms.} We test if the words are in the corresponding \emph{GNU Aspell}\footnote{\url{http://aspell.net/}} dictionary. We repeat this task for each of the 5 languages, for both the OSCAR and the Wikipedia corpora. We compile in Table \ref{tab:OOV} the number of out-of-vocabulary tokens for each corpora.

As expected, this simple metric shows that in general the OSCAR samples contain more out-of-vocabulary words than the Wikipedia ones. However the difference in magnitude between the two is strikingly lower that one would have expected in view of the criticisms by \citet{trinh-le-2018-a} and \citet{radford-etal-2019-language}, thereby validating the usability of Common Crawl data when it is properly filtered, as was achieved by in the OSCAR 2019 corpus. We even observe that, for Danish, the number of out-of-vocabulary words in OSCAR is lower than that in Wikipedia.

\begin{table}[t]
    \centering\small
    \begin{tabular}{lrr}\toprule
        Language   & \multicolumn{1}{l}{OOV Wikipedia} & \multicolumn{1}{l}{OOV OSCAR 2019} \\ \midrule
        Bulgarian  & 60,879                            & 66,558                        \\
        Catalan    & 34,919                            & 79,678                        \\
        Danish     & 134,677                           & 123,299                       \\
        Finnish    & 266,450                           & 267,525                       \\
        Indonesian & 116,714                           & 124,607                       \\
        \bottomrule
    \end{tabular}
    \caption{Number of out-of-vocabulary words in random samples of 1M words for OSCAR 2019 and Wikipedia.}
    \label{tab:OOV}
\end{table}

\section{Experimental Setting}

The main goal of this paper is to show the impact of training data on contextualized word representations when applied in particular downstream tasks. To this end, we train different versions of the \emph{Embeddings from Language Models} (ELMo) \citep{peters-etal-2018-deep} for both the Wikipedia and OSCAR 2019 corpora, for each of our selected 5 languages. We save the models' weights at different number of epochs for each language, in order to test how corpus size affect the embeddings and to see whether and when overfitting happens when training elmo on smaller corpora.

We take each of the trained ELMo models and use them in conjunction with the UDPipe 2.0 \citep{straka-2018-udpipe,straka-strakova-2019-evaluating} architecture for dependency parsing and POS-tagging to test our models. We train UDPipe 2.0 using gold tokenization and segmentation for each of our ELMo models, the only thing that changes from training to training is the ELMo model as hyperparameters always remain at the default values (except for number of training tokens) \citep{peters-etal-2018-deep}.

\subsection{Contextualized word embeddings}

\emph{Embeddings from Language Models} (ELMo) \citep{peters-etal-2018-deep} is an LSTM-based language model. More precisely, it uses a bidirectional language model, which combines a forward and a backward LSTM-based language model. ELMo also computes a context-independent token representation via a CNN over characters.

We train ELMo models for Bulgarian, Catalan, Danish, Finnish and Indonesian using the OSCAR 2019 subcorpora on the one hand and the Wikipedia corpora on the other. We train each model for 10 epochs, as was done for the original English ELMo \citep{peters-etal-2018-deep}. We save checkpoints at 1\textsuperscript{st}, 3\textsuperscript{rd} and 5\textsuperscript{th} epoch in order to investigate some concerns about possible overfitting for smaller corpora (Wikipedia in this case) raised by the original ELMo authors.\footnote{\url{https://github.com/allenai/bilm-tf/issues/135}}

\subsection{UDPipe 2.0} \label{udpipe-future}

For our POS tagging and dependency parsing evaluation, we use UDPipe 2.0, which has a freely available and ready to use implementation.\footnote{\url{https://github.com/CoNLL-UD-2018/UDPipe-Future}} This architecture was submitted as a participant to the \emph{2018 CoNLL Shared Task} \citep{zeman-etal-2018-conll}, obtaining the 3\textsuperscript{rd} place in LAS ranking. UDPipe 2.0 is a multi-task model that predicts POS tags, lemmas and dependency trees jointly.

The original UDPipe 2.0 implementation calculates 3 different embeddings, namely:

\begin{itemize}
    \item \emph{Pre-trained word embeddings}: In the original implementation, the Wikipedia version of fastText embeddings is used \citep{bojanowski-etal-2017-enriching}; we replace them in favor of the newer Common-Crawl-based fastText embeddings trained by \citet{grave-etal-2018-learning}.
    \item \emph{Trained word embeddings}: Randomly initialized word representations that are trained with the rest of the network.
    \item \emph{Character-level word embeddings}: Computed using bi-directional GRUs of dimension 256. They represent every UTF-8 encoded character with two 256 dimensional vectors, one for the forward and one for the backward layer. This two vector representations are concatenated and are trained along the whole network.
\end{itemize}

After the CoNLL 2018 Shared Task, the UDPipe 2.0 authors added the option to concatenate contextualized representations to the embedding section of the network \citep{straka-strakova-2019-evaluating}, we use this new implementation and we concatenate our pretrained deep contextualized ELMo embeddings to the three embeddings mentioned above.

Once the embedding step is completed, the concatenation of all vector representations for a word are fed to two shared bidirectional LSTM \citep{hochreiter-schmidhuber-1997-long} layers. The output of these two BiLSTMS is then fed to two separate specific LSTMs:
\begin{itemize}
    \item The tagger- and lemmatizer-specific bidirectional LSTMs, with Softmax classifiers on top, which process its output and generate UPOS, XPOS, UFeats and Lemmas. The lemma classifier also takes the character-level word embeddings as input.

    \item The parser-specific bidirectional LSTM layer, whose output is then passed to a bi-affine attention layer \citep{dozat-manning-2017-deep} producing labeled dependency trees.
\end{itemize}

\subsection{Treebanks}

\begin{table}[t!]
    \centering\small
    \begin{tabular}{lrr}\toprule
        Treebank       & \multicolumn{1}{l}{\#Ktokens} & \multicolumn{1}{l}{\#Ksentences} \\ \midrule
        Bulgarian-BTB  & 156                           & 11                               \\
        Catalan-AnCora & 530                           & 17                               \\
        Danish-DDT     & 100                           & 6                                \\
        Finnish-FTB    & 159                           & 19                               \\
        Finnish-TDT    & 202                           & 15                               \\
        Indonesian-GSD & 121                           & 6                                \\
        \bottomrule
    \end{tabular}
    \caption{Size of treebanks, measured in thousands of tokens and sentences.}
    \label{tab:treeb}
\end{table}

To train the selected parser and tagger (cf.~Section \ref{udpipe-future}) and evaluate the pre-trained language models in our 5 languages, we run our experiments using the Universal Dependencies (UD)\footnote{\url{https://universaldependencies.org}} paradigm and its corresponding UD POS tag set \citep{petrov-etal-2012-universal}. We use all the treebanks available for our five languages in the UD treebank collection version 2.2 \citep{nivre-etal-2018-universal}, which was used for the CoNLL 2018 shared task, thus we perform our evaluation tasks in 6 different treebanks (see Table~\ref{tab:treeb} for treebank size information).
\begin{itemize}
    \item \emph{Bulgarian BTB}: Created at the Institute of Information and Communication Technologies, Bulgarian Academy of Sciences, it consists of legal documents, news articles and fiction pieces.
    \item \emph{Catalan-AnCora}: Built on top of the Spanish-Catalan \emph{AnCora corpus} \citep{taule-etal-2008-ancora}, it contains mainly news articles.
    \item \emph{Danish-DDT}: Converted from the \emph{Danish Dependency Treebank} \citep{buch-kromann-2003-the}. It includes news articles, fiction and non fiction texts and oral transcriptions.
    \item \emph{Finnish-FTB}: Consists of manually annotated grammatical examples from VISK\footnote{\url{http://scripta.kotus.fi/visk}} (The Web Version of the Large Grammar of Finnish).
    \item \emph{Finnish-TDT}: Based on the Turku Dependency Treebank (TDT). Contains texts from Wikipedia, Wikinews, news articles, blog entries, magazine articles, grammar examples, Europarl speeches, legal texts and fiction.
    \item \emph{Indonesian-GSD}: Includes mainly blog entries and news articles.
\end{itemize}


\section{Results \& Discussion}


\begin{table}[ht!]
    \centering\small
    \resizebox{0.45\linewidth}{!}{
        \begin{tabular}{@{}llccc@{}}\toprule
            Treebank       & Model       & UPOS              & UAS               & LAS               \\
            \midrule
                           & UDify       & 98.89             & 95.54             & 92.40             \\
                           & UDPipe 2.0  & 98.98             & 93.38             & 90.35             \\
            Bulgarian BTB  & +mBERT      & \underline{99.20} & \underline{95.34} & \underline{92.62} \\
                           & +\elmowiki  & 99.17             & 94.93             & 92.05             \\
                           & +\elmooscar & \textbf{99.40}    & \textbf{96.01}    & \textbf{93.56}    \\
            \midrule
                           & UDify       & 98.89             & \underline{94.25} & 92.33             \\
                           & UDPipe 2.0  & 98.88             & 93.22             & 91.06             \\
            Catalan-AnCora & +mBERT      & \textbf{99.06}    & \textbf{94.49}    & \underline{92.74} \\
                           & +\elmowiki  & \underline{99.05} & 93.99             & 92.24             \\
                           & +\elmooscar & \textbf{99.06}    & \textbf{94.49}    & \textbf{92.88}    \\
            \midrule
                           & UDify       & 97.50             & 87.76             & 84.50             \\
                           & UDPipe 2.0  & 97.78             & 86.88             & 84.31             \\
            Danish-DDT     & +mBERT      & 98.21             & \underline{89.32} & \underline{87.24} \\
                           & +\elmowiki  & \underline{98.45} & 89.05             & 86.92             \\
                           & +\elmooscar & \textbf{98.62}    & \textbf{89.84}    & \textbf{87.95}    \\
            \bottomrule
        \end{tabular}
    }
    ~
    \resizebox{0.45\linewidth}{!}{
        \begin{tabular}{@{}llccc@{}}\toprule
            Treebank       & Model       & UPOS              & UAS               & LAS               \\
            \midrule
                           & UDify       & 93.80             & 86.37             & 81.40             \\
                           & UDPipe 2.0  & 96.65             & 90.68             & 87.89             \\
            Finnish-FTB    & +mBERT      & 96.97             & 91.68             & 89.02             \\
                           & +\elmowiki  & \underline{97.27} & \underline{92.05} & \underline{89.62} \\
                           & +\elmooscar & \textbf{98.13}    & \textbf{93.81}    & \textbf{92.02}    \\
            \midrule
                           & UDify       & 94.43             & 86.42             & 82.03             \\
                           & UDPipe 2.0  & 97.45             & 89.88             & 87.46             \\
            Finnish-TDT    & +mBERT      & 97.57             & \underline{91.66} & \underline{89.49} \\
                           & +\elmowiki  & \underline{97.65} & 91.60             & 89.34             \\
                           & +\elmooscar & \textbf{98.36}    & \textbf{93.54}    & \textbf{91.77}    \\
            \midrule
                           & UDify       & 93.36             & 86.45             & 80.10             \\
                           & UDPipe 2.0  & 93.69             & 85.31             & 78.99             \\
            Indonesian-GSD & +mBERT      & \underline{94.09} & \underline{86.47} & \underline{80.40} \\
                           & +\elmowiki  & 93.94             & 86.16             & 80.10             \\
                           & +\elmooscar & \textbf{94.12}    & \textbf{86.49}    & \textbf{80.59}    \\
            \bottomrule
        \end{tabular}
    }
    \caption{Scores from \mbox{UDPipe~2.0} \protect\citep[from][]{kondratyuk-straka-2019-75}, the previous state-of-the-art models UDPipe 2.0+mBERT \protect\citep{straka-strakova-2019-evaluating} and UDify \protect\citep{kondratyuk-straka-2019-75}, and our ELMo-enhanced UDPipe 2.0 models. Test scores are given for UPOS, UAS and LAS in all five languages. Best scores are shown in bold, second best scores are underlined.}
    \label{tab:parse}
\end{table}

\subsection{Parsing and POS tagging results}
We use UDPipe 2.0 without contextualized embeddings as our baseline for POS tagging and dependency parsing. However, we did not train the model without contextualized word embedding ourselves. We instead take the scores as they are reported in \citep{kondratyuk-straka-2019-75}. We also compare our UDPipe 2.0 + ELMo models against the state-of-the-art results (assuming gold tokenization) for these languages, which are either UDify \citep{kondratyuk-straka-2019-75} or UDPipe 2.0 + mBERT \citep{straka-strakova-2019-evaluating}.

Results for UPOS, UAS and LAS are shown in Table \ref{tab:parse}. We obtain the state of the art for the three metrics in each of the languages with the UDPipe 2.0 + \elmooscar models. We also see that in every single case the UDPipe 2.0 + \elmooscar result surpasses the UDPipe 2.0 + \elmowiki one, suggesting that the size of the pre-training data plays an important role in downstream task results. This is also supports our hypothesis that the OSCAR corpus, being multi-domain, exhibits a better coverage of the different styles, genres and uses present at least in these 5 languages.

Taking a closer look at the results for Danish, we see that \elmowiki, which was trained with a mere 300MB corpus, does not show any sign of overfitting, as the UDPipe 2.0 + \elmowiki results considerably improve the UDPipe 2.0 baseline. This is the case for all of our \elmowiki models as we never see any evidence of a negative impact when we add them to the baseline model. In fact, the results of UDPipe 2.0 + \elmowiki give better than previous state-of-the-art results in all metrics for the Finnish-FTB and in UPOS for the Finnish-TDT. The results for Finnish are actually quite interesting, as mBERT was pre-trained on Wikipedia and here we see that the multilingual setting in which UDify was fine-tuned exhibits sub-baseline results for all metrics, and that the UDPipe + mBERT scores are often lower than those of our UDPipe 2.0 + \elmowiki. This actually suggests that even though the multilingual approach of mBERT (in pre-training) or UDify (in pre-training and fine-tuning) leads to better performance for  high-resource languages or languages that are closely related to high-resource languages, it might also significantly degrade the representations for more isolated or even simply more morphologically rich languages like Finnish. In contrast, our monolingual approach with UDPipe 2.0 + \elmooscar improves the previous SOTA considerably, by more than 2 points for some metrics. Note however that Indonesian, which might also be seen as a relatively isolated language, does not behave in the same way as Finnish.

\subsection{Impact of the number of training epochs}

An important topic we wanted to address with our experiments was that of \emph{overfitting} and the number of epochs one should train the contextualized embeddings for. The ELMo authors have expressed that increasing the number of training epochs is generally better, as they argue that training the ELMo model for longer reduces held-out perplexity and further improves downstream task performance.\footnote{Their comments on the matter can be found \href{https://github.com/allenai/bilm-tf/issues/135}{here}.} This is why we intentionally fully pre-trained the \elmowiki to the 10 epochs of the original ELMo paper, as its authors also expressed concern over the possibility of overfitting for smaller corpora. We thus save checkpoints for each of our ELMo model at the 1, 3, 5 and 10 epoch marks so that we can properly probe for overfitting. The scores of all checkpoints are reported in Table \ref{tab:ablation-monolingual}. Here again we do not train the UDPipe 2.0 baselines without embedding, we just report the scores published in \citet{kondratyuk-straka-2019-75}.

The first striking finding is that even though all our Wikipedia data sets are smaller than 1GB in size (except for Catalan), none of the \elmowiki models show any sign of overfitting, as the results continue to improve for all metrics the more we train the ELMo models, with the best results consistently being those of the fully trained 10 epoch ELMos. For all of our Wikipedia models, but those of Catalan and Indonesian, we see sub-baseline results at 1 epoch; training the model for longer is better, even if the corpora are small in size.

\begin{table}[ht!]
    \centering\small
    \resizebox{0.45\linewidth}{!}{
        \begin{tabular}{@{}llccc@{}}\toprule
            Treebank       & Model            & UPOS              & UAS               & LAS               \\ \midrule
                           & UDPipe 2.0       & 98.98             & 93.38             & 90.35             \\
                           & +\elmowikione    & 98.81             & 93.60             & 90.21             \\
                           & +\elmowikithree  & 99.01             & 94.32             & 91.36             \\
                           & +\elmowikifive   & 99.03             & 94.32             & 91.38             \\
            Bulgarian BTB  & +\elmowikiten    & \underline{99.17} & \underline{94.93} & \underline{92.05} \\
                           & +\elmooscarone   & 99.28             & 95.45             & 92.98             \\
                           & +\elmooscarthree & 99.34             & 95.58             & 93.12             \\
                           & +\elmooscarfive  & 99.34             & 95.63             & 93.25             \\
                           & +\elmooscarten   & \textbf{99.40}    & \textbf{96.01}    & \textbf{93.56}    \\
            \midrule
                           & UDPipe 2.0       & 98.88             & 93.22             & 91.06             \\
                           & +\elmowikione    & 98.93             & 93.24             & 91.21             \\
                           & +\elmowikithree  & 99.02             & 93.75             & 91.93             \\
                           & +\elmowikifive   & 99.04             & 93.86             & 92.05             \\
            Catalan-AnCora & +\elmowikiten    & \underline{99.05} & \underline{93.99} & \underline{92.24} \\
                           & +\elmooscarone   & 99.07             & 93.92             & 92.29             \\
                           & +\elmooscarthree & \textbf{99.10}    & 94.29             & 92.69             \\
                           & +\elmooscarfive  & 99.07             & 94.38             & 92.75             \\
                           & +\elmooscarten   & 99.06             & \textbf{94.49}    & \textbf{92.88}    \\
            \midrule
                           & UDPipe 2.0       & 97.78             & 86.88             & 84.31             \\
                           & +\elmowikione    & 97.47             & 86.98             & 84.15             \\
                           & +\elmowikithree  & 98.03             & 88.16             & 85.81             \\
                           & +\elmowikifive   & 98.15             & 88.24             & 85.96             \\
            Danish-DDT     & +\elmowikiten    & \underline{98.45} & \underline{89.05} & \underline{86.92} \\
                           & +\elmooscarone   & 98.50             & 89.47             & 87.43             \\
                           & +\elmooscarthree & 98.59             & 89.68             & 87.77             \\
                           & +\elmooscarfive  & 98.59             & 89.46             & 87.64             \\
                           & +\elmooscarten   & \textbf{98.62}    & \textbf{89.84}    & \textbf{87.95}    \\
            %\iffalse
            \bottomrule
        \end{tabular}
    }
    ~
    \resizebox{0.45\linewidth}{!}{
        \begin{tabular}{@{}llccc@{}}\toprule
            Treebank       & Model            & UPOS              & UAS               & LAS               \\
            %\fi{}
            \midrule
                           & UDPipe 2.0       & 96.65             & 90.68             & 87.89             \\
                           & +\elmowikione    & 95.86             & 89.63             & 86.39             \\
                           & +\elmowikithree  & 96.76             & 91.02             & 88.27             \\
                           & +\elmowikifive   & 96.97             & 91.66             & 89.04             \\
            Finnish-FTB    & +\elmowikiten    & \underline{97.27} & \underline{92.05} & \underline{89.62} \\
                           & +\elmooscarone   & 97.91             & 93.41             & 91.43             \\
                           & +\elmooscarthree & 98.00             & \textbf{93.99}    & 91.98             \\
                           & +\elmooscarfive  & \textbf{98.15}    & 93.98             & \textbf{92.24}    \\
                           & +\elmooscarten   & 98.13             & 93.81             & 92.02             \\
            \midrule
                           & UDPipe 2.0       & 97.45             & 89.88             & 87.46             \\
                           & +\elmowikione    & 96.73             & 89.11             & 86.33             \\
                           & +\elmowikithree  & 97.55             & 90.84             & 88.50             \\
                           & +\elmowikifive   & 97.55             & 91.11             & 88.88             \\
            Finnish-TDT    & +\elmowikiten    & \underline{97.65} & \underline{91.60} & \underline{89.34} \\
                           & +\elmooscarone   & 98.27             & 93.03             & 91.29             \\
                           & +\elmooscarthree & 98.38             & \textbf{93.60}    & \textbf{91.83}    \\
                           & +\elmooscarfive  & \textbf{98.39}    & 93.57             & 91.80             \\
                           & +\elmooscarten   & 98.36             & 93.54             & 91.77             \\
            \midrule
                           & UDPipe 2.0       & 93.69             & 85.31             & 78.99             \\
                           & +\elmowikione    & 93.70             & 85.81             & 79.46             \\
                           & +\elmowikithree  & 93.90             & 86.04             & 79.72             \\
                           & +\elmowikifive   & 94.04             & 85.93             & 79.97             \\
            Indonesian-GSD & +\elmowikiten    & \underline{93.94} & \underline{86.16} & \underline{80.10} \\
                           & +\elmooscarone   & 93.95             & 86.25             & 80.23             \\
                           & +\elmooscarthree & 94.00             & 86.21             & 80.14             \\
                           & +\elmooscarfive  & \textbf{94.23}    & 86.37             & 80.40             \\
                           & +\elmooscarten   & 94.12             & \textbf{86.49}    & \textbf{80.59}    \\
            \bottomrule
        \end{tabular}
    }
    \caption{UPOS, UAS and LAS scores for the UDPipe 2.0 baseline reported by \protect\citep{kondratyuk-straka-2019-75}, plus the scores for checkpoints at 1, 3, 5 and 10 epochs for all the \elmooscar and \elmowiki. All scores are test scores. Best \elmooscar scores are shown in bold while best \elmowiki scores are underlined.}
    \label{tab:ablation-monolingual}
\end{table}

\elmooscar models exhibit exactly the same behavior as \elmowiki models where the scores continue to improve the longer they are pre-trained, except for the case of Finnish. Here we actually see an unexpected behavior where the model performance caps around the 3\textsuperscript{rd} to 5\textsuperscript{th} epoch. This is surprising because the Finnish OSCAR 2019 subcorpus is more than 20 times bigger than our smallest Wikipedia corpus, the Danish Wikipedia, that did not exhibit this behavior. As previously mentioned Finnish is morphologically richer than the other languages in which we trained ELMo, we hypothesize that the representation space given by the ELMo embeddings might not be sufficiently big to extract more features from the Finnish OSCAR subcorpus beyond the 5\textsuperscript{th} epoch mark, however in order to test this we would need to train a larger language model like BERT which is sadly beyond our computing infrastructure limits (cf. Subsection \ref{cost}). However we do note that pre-training our current language model architectures in a morphologically rich language like Finnish might actually better expose the limits of our existing approaches to language modeling.

One last thing that it is important to note with respect to the number of training epochs is that even though we fully pre-trained our \elmowikis and \elmooscars to the recommended 10 epoch mark, and then compared them against one another, the number of training steps between both pre-trained models differs drastically due to the big difference in corpus size (for Indonesian, for instance, 10 epochs correspond to 78K steps for \elmowiki and to 2.6M steps for OSCAR; the complete picture is provided in the Appendix, in Table~\ref{tab:steps}). In fact, we can see in Table \ref{tab:ablation-monolingual} that all the UDPipe 2.0 + \elmooscarone perform better than the UDPipe 2.0 + \elmowikione models across all metrics. Thus we believe that talking in terms of training steps as opposed to training epochs might be a more transparent way of comparing two pre-trained models.

\section{Conclusions}

In this chapter, we have explored the use of the Common-Crawl-based OSCAR 2019 corpora to train ELMo contextualized embeddings for five typologically diverse mid-resource languages. We have compared them with Wikipedia-based ELMo embeddings on two classical NLP tasks, POS tagging and parsing, using state-of-the-art neural architectures at the end of 2019. Our goal was to explore whether the noisiness level of Common Crawl data, often invoked to criticize the use of such data, could be compensated by its larger size; for some languages, the OSCAR 2019 corpus is several orders of magnitude larger than the corresponding Wikipedia. Firstly, we found that when properly filtered, Common Crawl data is not massively noisier than Wikipedia. Secondly, we show that embeddings trained using OSCAR 2019 data consistently outperform Wikipedia-based embeddings, to the extent that they allow us to improve the state of the art in POS tagging and dependency parsing for all the 6 chosen treebanks. Thirdly, we observe that more training epochs generally results in better embeddings even when the training data is relatively small, as is the case for Wikipedia.

Our experiments show that Common-Crawl-based data such as the OSCAR corpus can be used to train high-quality contextualized embeddings, even for languages for which more standard textual resources lack volume or genre variety. This could result in better performances in a number of NLP tasks for many non highly resourced languages. However, we are aware that this first evaluation of the OSCAR 2019 remains limited both in terms of methodology and in terms of the actual portion of the whole corpus that was evaluated. Automated evaluations like this one won't give us a complete assessment of the quality of the corpus beyond its usefulness for the training of contextualized embeddings, this is why in the following chapter we will discuss a more thorough and extensive audit of the OSCAR 2019 corpus as well as other web crawled corpora, that was the result of an international collaboration with a diverse team of more than 50 researchers.
