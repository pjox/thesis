\chapter{CamemBERT}

\section{\camembert: a French Language Model}\label{sec:Camembert}
In this section, we describe the pretraining data, architecture, training objective and optimisation setup we use for \camembert.

\subsection{Training data}
%\paragraph{Data description}
%\lm{Should we say "We train a model on OSCAR", put a real emphasis on OSCAR, and then rapidly, say in section 5, we compare also to CCNET and Wikipedia}
Pretrained language models benefits from being trained on large datasets \cite{devlin-etal-2019-bert,liu-etal-2019-roberta,raffel-etal-2020-exploring}. %can be significantly improved by using more data
We therefore use the French part of the OSCAR corpus \cite{ortiz-suarez-etal-2019-asynchronous}, a pre-filtered and pre-classified version of Common Crawl.\footnote{\url{https://commoncrawl.org/about/}} %and compare two pre-classified and pre-filtered versions of it: OSCAR \cite{ortiz-suarez-etal-2019-asynchronous} and CCNet \cite{wenzek-etal-2020-ccnet}. 
%%We also compare, controlling for the data set size, these two dataset to the French Wikipedia dataset. 

\paragraph{OSCAR} is a set of monolingual corpora extracted from Common Crawl snapshots\iffalse, specifically from the plain text \emph{WET} format distributed by Common Crawl which removes all the HTML tags and converts the text formatting to UTF-8\fi{}. It follows the same approach as \cite{grave-etal-2018-learning} by using a language classification model based on the fastText linear classifier \cite{joulin-etal-2016-fasttext,joulin-etal-2017-bag} pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 languages. No other filtering is done. We use a non-shuffled version of the French data, which amounts to 138GB of raw text and 32.7B tokens after subword tokenization.% (more details in Table~\ref{table:oscar_vs_ccnet}).

%% WIKIPEDIA

%\paragraph{Wikipedia}. This corpus comprises the entirety of the French Wikipedia as of April 2019. It was taken from the official Wikipedia dumps \footnote{\url{https://dumps.wikimedia.org/backup-index.html}}, and HTML tags and tables were removed using Giuseppe Attardi's template expansion tool \emph{WikiExtractor}\footnote{\url{https://github.com/attardi}}
%% no mention of 4GB  

%RANDOM SAMPLES  ref section 5 

%Both OSCAR and CCNet are sets of monolingual corpora extracted from Common Crawl snapshots, specifically from the plain text \emph{WET} format distributed by Common Crawl which removes all the HTML tags and converts the text formatting to UTF-8. OSCAR and CCnet follow the same approach as \newcite{grave-etal-2018-learning} by using a language classification model for the fastText linear classifier \cite{joulin-etal-2016-fasttext,joulin-etal-2017-bag} pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 different languages.

%Even though the pipeline are similar and they both obtain a similar amount of data in total for all the languages, 3.2 TB on both cases, there are a number on improvements made in the CCNet pipeline that can contribute to the quality of the documents inside each monolingual corpora. Mainly, CCNET is filtered with a language model trained on Wikipedia by removing sentences with low perplexity and classified in three parts \emph{head, middle and tail} based on this perplexity. 

%For instance, deduplication is performed before classification, by first normalizing the text so that the deduplication process can get rid of common website elements such as menus, cookie warnings and contact information \cite{wenzek-etal-2020-ccnet}. Moreover CCNet adds an additional step for filtering ``bad'' content. They use a 5-gram Kneser-Ney model \cite{heafield2011kenlm} trained on Wikipedia (after SentencePiece tokenization) in order to filter out paragraphs with low perplexity. They split the the corpus of each language in three even parts which they call \emph{head, middle and tail}, where \emph{head} is the part with highest perplexity according to their language model. Although this steps filters out bad content, it introduce a bias towards Wikipedia-like texts in the \emph{head} part of the pretraining data. 
%OSCAR, by contrast, does not perform any language model based filtering. This results in OSCAR being much closer to the original raw Crawl, and more diverse in terms of domain, genre and text quality than CCNet. We point to \cite{wenzek-etal-2020-ccnet,ortiz-suarez-etal-2019-asynchronous} for more descriptions of the data.  %performs deduplication after the language classification step and does not introduce any specialized filtering scheme, they just choose to keep the paragraphs having 100 or more characters. This results in OSCAR being much closer to the original raw Crawl, and more diverse in terms of domain, genre and text quality than CCNet.


%For CCNet, we use the \textsc{head} split (top 33\% of documents in terms of filtering perplexity) of the French data. This amounts to 135GB of raw text, which we then tokenise into 31.9B SentencePiece tokens. For OSCAR we use a non shuffled version of the French data which amounts to 138GB of raw text and 32.7B SentencePiece tokens, see table~\ref{table:oscar_vs_ccnet}.



\subsection{Pre-processing}
We segment the input text data into subword units using SentencePiece \cite{kudo-richardson-2018-sentencepiece}.
SentencePiece is an extension of Byte-Pair encoding (BPE) \cite{sennrich-etal-2016-neural} and WordPiece \cite{kudo-2018-subword} that does not require pre-tokenization (at the word or token level), thus removing the need for language-specific tokenisers.
We use a vocabulary size of 32k subword tokens. These subwords are learned on $10^7$ sentences sampled randomly from the pretraining dataset.
We do not use subword regularisation (i.e.~sampling from multiple possible segmentations) for the sake of simplicity.


\subsection{Language Modeling}

\paragraph{Transformer}
Similar to \roberta and \bert, \camembert is a multi-layer bidirectional Transformer \cite{vaswani-etal-2017-attention}. Given the widespread usage of Transformers, we do not describe them here and refer the reader to \citep{vaswani-etal-2017-attention}.
\camembert uses the original architectures of \bertbase (12 layers, 768 hidden dimensions, 12 attention heads, 110M parameters) and \bertlarge (24 layers, 1024 hidden dimensions, 16 attention heads, 335M parameters).
\camembert is very similar to \roberta, the main difference being the use of whole-word masking and the usage of SentencePiece tokenization \cite{kudo-richardson-2018-sentencepiece} instead of WordPiece \cite{schuster-nakajima-2012-japanese}.

\paragraph{Pretraining Objective}
We train our model on the Masked Language Modeling (MLM) task.
Given an input text sequence composed of $N$ tokens $x_1, ..., x_N$, we select 15\% of tokens for possible replacement. Among those selected tokens, 80\% are replaced with the special \texttt{<MASK>} token, 10\% are left unchanged and 10\% are replaced by a random token. The model is then trained to predict the initial masked tokens using cross-entropy loss.

Following the \roberta approach, we dynamically mask tokens instead of fixing them statically for the whole dataset during preprocessing. This improves variability and makes the model more robust when training for multiple epochs.

Since we use SentencePiece to tokenize our corpus, the input tokens to the model are a mix of whole words and subwords.
An upgraded version of \bert\footnote{\url{https://github.com/google-research/bert/blob/master/README.md}} and \citet{joshi-etal-2020-spanbert} have shown that masking whole words instead of individual subwords leads to improved performance.
Whole-word Masking (WWM) makes the training task more difficult because the model has to predict a whole word rather than predicting only part of the word given the rest.
We train our models using WWM by using whitespaces in the initial untokenized text as word delimiters.

WWM is implemented by first randomly sampling 15\% of the words in the sequence and then considering all subword tokens in each of this 15\% for candidate replacement. This amounts to a proportion of selected tokens that is close to the original 15\%.
These tokens are then either replaced by \texttt{<MASK>} tokens (80\%), left unchanged (10\%) or replaced by a random token.

Subsequent work has shown that the next sentence prediction (NSP) task originally used in \bert does not improve downstream task performance \cite{conneau-lample-2019-cross,liu-etal-2019-roberta}, thus we also remove it.
j

%\subsection{Optimisation} 
%integrated to experimental setup

\paragraph{Optimisation}
Following \citep{liu-etal-2019-roberta}, we optimize the model using Adam \cite{kingma-ba-2015-adam} ($\beta_1 = 0.9$, $\beta_2 = 0.98$) for 100k steps with large batch sizes of 8192 sequences, each sequence containing at most 512 tokens.
We enforce each sequence to only contain complete paragraphs (which correspond to lines in the our pretraining dataset).

\paragraph{Pretraining}
We use the \roberta implementation in the fairseq library \cite{ott-etal-2019-fairseq}.
Our learning rate is warmed up for 10k steps up to a peak value of $0.0007$ instead of the original $0.0001$ given our large batch size, and then fades to zero with polynomial decay.
Unless otherwise specified, our models use the BASE architecture, and are pretrained for 100k backpropagation steps on 256 Nvidia V100 GPUs (32GB each) for a day.
We do not train our models for longer due to practical considerations, even though the performance still seemed to be increasing.
%We trained models on the OSCAR, CCNet and Wikipedia datasets with Whole Word Masking\footnote{As no significant differences were observed during preliminary experiments between SVW and WWM, we only analyse WWM trained models. cf. Appendix {?}}.

\subsection{Using \camembert for downstream tasks}
We use the pretrained \camembert in two ways. In the first one, which we refer to as \textit{fine-tuning}, we fine-tune the model on a specific task in an end-to-end manner. In the second one, referred to as \textit{feature-based embeddings} or simply \textit{embeddings}, we extract frozen contextual embedding vectors from \camembert.
These two complementary approaches shed light on the quality of the pretrained hidden representations captured by \camembert.


\paragraph{Fine-tuning}
For each task, we append the relevant predictive layer on top of \camembert's  architecture. Following the work done on \bert \cite{devlin-etal-2019-bert}, for sequence tagging and sequence labeling we append a linear layer that respectively takes as input the last hidden representation of the \texttt{<s>} special token and the last hidden representation of the first subword token of each word.
For dependency parsing, we plug a bi-affine graph predictor head as inspired by \citet{dozat-manning-2017-deep}. We refer the reader to this article for more details on this module.
We fine-tune on XNLI by adding a classification head composed of one hidden layer with a non-linearity and one linear projection layer, with input dropout for both.

We fine-tune \camembert independently for each task and each dataset. We optimize the model using the Adam optimiser \cite{kingma-ba-2015-adam} with a fixed learning rate. We run a grid search on a combination of learning rates and batch sizes. We select the best model on the validation set out of the 30 first epochs.
For NLI we use the default hyper-parameters provided by the authors of RoBERTa on the MNLI task.\footnote{More details at \url{https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md}.}
Although this might have pushed the performances even further, we do not apply any regularisation techniques such as weight decay, learning rate warm-up or discriminative fine-tuning, except for NLI. We show that fine-tuning \camembert in a straightforward manner leads to state-of-the-art results on all tasks and outperforms the existing \bert-based models in all cases.
The POS tagging, dependency parsing, and NER experiments are run using Hugging Face's Transformer library extended to support \camembert and dependency parsing \cite{wolf-etal-2019-huggingface}.
The NLI experiments use the fairseq library following the \roberta implementation.


\paragraph{Embeddings}

%\lm{This is the Experimental setup section, I would put more emphasis on our method instead of the litterature}
%As it is the case with contextualized word embeddings like ELMo \cite{peters-etal-2018-deep} and Flair \cite{akbik-etal-2018-contextual}, one can use a frozen version of transformer models like BERT or RoBERTa as if they were feature based embeddings, notable examples of this type of utilization are the original BERT paper, in which the authors use BERT in this way to do NER \cite{devlin-etal-2019-bert}; the work of \newcite{straka-strakova-2019-evaluating} in which the English BERT and \mbert are plugged as feature-based embeddings to the UDPipe Future architecture obtaining state-of-the-art for part-of-speech tagging and dependency parsing across a wide range of languages; and the work of \newcite{strakova-etal-2019-neural} which again uses both the English BERT and \mbert as feature based embeddings coupled with \newcite{lample-etal-2016-neural} architecture. \bm{I remove the SOTA mention here as it's not the right place} % obtaining state-of-the-art results for both flat and nested NER in 5 different languages. 
%In order to obtain a representation for a given token, all these implementations first compute the representations of the subwords of a token by taking the mean of the representations given by the four last layers, and then generate the representation for the token by taking the mean of the representations of the subwords of a given token.
Following \citet{strakova-etal-2019-neural} and \citet{straka-strakova-2019-evaluating} for \mbert and the English BERT, we make use of \camembert in a feature-based embeddings setting.
In order to obtain a representation for a given token, we first compute the average of each sub-word’s representations in the last four layers of the Transformer, and then average the resulting sub-word vectors.
%compute the representations of the subwords of a token by taking the mean of the representations given by the four last layers, and then generate the representation for the token by taking the mean of the representations of the subwords of a given token.

We evaluate \camembert in the embeddings setting for POS tagging, dependency parsing and NER; using the open-source implementations of \newcite{straka-strakova-2019-evaluating} and \newcite{strakova-etal-2019-neural}.\footnote{UDPipe Future is available at \url{https://github.com/CoNLL-UD-2018/UDPipe-Future}, and the code for nested NER is available at \url{https://github.com/ufal/acl2019_nested_ner}.}


%We also experiment training a BASE model longer for 500k steps, \camembertccnetlong, as advised in \cite{liu-etal-2019-roberta}. 
%Finally we trained a LARGE model for 100k steps, \camembertccnetlarge. Note that the LARGE model's performance was still improving after 100k steps, but as LARGE models are significantly slower to train, we didn't train for more, for practical constraints.
%The model trained longer and the large models are trained using CCNet due to the cleaner nature of CCNet.

%\bm{Do,e}
\section{Evaluation of \camembert}


In this section, we measure the performance of our models by evaluating them on the four aforementioned tasks: POS tagging, dependency parsing, NER and NLI.


%Table~\ref{table:all_models} summarize the models that we trained.


%\begin{table}[t]
%\centering\small
%\resizebox{\columnwidth}{!}{
%\begin{tabular}{lcccc}
%\toprule
%Model name               & Model & Training & \#training & Masking\\
%                         & type & Corpus & steps & strategy\\
%\midrule
%\camembertoscarswm       & BASE & OSCAR & 100k & subword   \\
%\camembertccnetswm       & BASE & CCNet & 100k & subword   \\
%\camembertoscar          & BASE & OSCAR & 100k & whole-word\\
%\camembertccnet          & BASE & CCNet & 100k & whole-word\\
%\camembertccnetlong      & BASE & CCNet & 500k & whole-word\\
%\camembertccnetlarge     & LARGE& CCNet & %100k & whole-word\\
%\bottomrule
%\end{tabular}
%}
%\caption{Overview of the \camembert models trained and discussed in this paper.\label{table:all_models}}
%\end{table}



\begin{table*}[ht]
    \small\centering
    \resizebox{\linewidth}{!}{
        \begin{tabu}{ l  c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c  @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c }
            \toprule
                                                                   & \multicolumn{2}{c @{\hspace{0.5cm}}}{\textsc{GSD}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Sequoia}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Spoken}} & \multicolumn{2}{c @{\hspace{0.35cm}}}{\textsc{ParTUT}}                                                                                 \\
            \cmidrule(l{2pt}r{0.4cm}){2-3}\cmidrule(l{-0.2cm}r{0.4cm}){4-5}\cmidrule(l{-0.2cm}r{0.4cm}){6-7}\cmidrule(l{-0.2cm}r{2pt}){8-9}
            \multirow{-2}{*}[1pt]{\textsc{Model}}                  & \textsc{UPOS}                                      & \textsc{LAS}                                           & \textsc{UPOS}                                         & \textsc{LAS}                                           & \textsc{UPOS}     & \textsc{LAS}      & \textsc{UPOS}     & \textsc{LAS}      \\
            \midrule
            \mbert  (fine-tuned)                                   & 97.48                                              & 89.73                                                  & 98.41                                                 & 91.24                                                  & 96.02             & 78.63             & 97.35             & 91.37             \\
            \xlmmlmtlm (fine-tuned)                                & 98.13                                              & 90.03                                                  & 98.51                                                 & 91.62                                                  & 96.18             & 80.89             & 97.39             & 89.43             \\ % 10138744 new XLM %& 97.71             & -                 & 98.51          & 91.62             & 95.29             & 74.17             & 96.84             & 89.22             \\ % 10138744 new XLM  % partut sequoia parsing 10138787 
            UDify \cite{kondratyuk-straka-2019-75}                          & 97.83                                              & \underline{91.45}                                      & 97.89                                                 & 90.05                                                  & 96.23             & 80.01             & 96.12             & 88.06             \\
            %\xlmEnFr & 97.51 & 93.49 & 90.72 & 98.30 &  93.62 & 91.565.42 & 96.53 & 93.03 & 90.64 \\ 
            UDPipe Future \cite{straka-2018-udpipe}                  & 97.63                                              & 88.06                                                  & 98.79                                                 & 90.73                                                  & 95.91             & 77.53             & 96.93             & 89.63             \\
            \: + mBERT + Flair  (emb.) \cite{straka-strakova-2019-evaluating} & \underline{97.98}                                  & 90.31                                                  & \textbf{99.32}                                        & 93.81                                                  & \textbf{97.23}    & \underline{81.40} & \underline{97.64} & \underline{92.47} \\
            \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
            \camembert (fine-tuned)                                & \textbf{98.18}                                     & \textbf{92.57}                                         & \underline{99.29}                                     & \textbf{94.20}                                         & 96.99             & 81.37             & \textbf{97.65}    & \textbf{93.43}    \\ % 10125734 : POS best seed PARSING gsd 10126431 PARSING other : 10126429
            UDPipe Future \mbox{+ \camembert} (embeddings)         & 97.96                                              & 90.57                                                  & 99.25                                                 & \underline{93.89}                                      & \underline{97.09} & \textbf{81.81}    & 97.50             & 92.32             \\
            \bottomrule
        \end{tabu}
    }
    \caption{\textbf{POS} and \textbf{dependency parsing} scores on 4 French treebanks, reported on test sets assuming gold tokenization and segmentation (best model selected on validation out of 4). Best scores in bold, second best underlined.}%\lm{uniformize the notations of XLM models}}%\comment{Report best seed and not average?}
    \label{tab:pos_and_dp_results}
\end{table*}

\begin{table}[ht]
    \centering \small
    %        \resizebox{\columnwidth}{!}{
    \scalebox{0.9}{
        \begin{tabu}{lc}
            \toprule
            Model                                  & F1                \\
            \midrule
            SEM (CRF) \cite{dupont-2017-exploration} & 85.02             \\
            LSTM-CRF \cite{dupont-2017-exploration}  & 85.57             \\
            \mbert (fine-tuned)                    & 87.35             \\
            \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
            \camembert (fine-tuned)                & \underline{89.08} \\% 10129644  %91.30 dev 10129153 (2 seeds only)
            LSTM+CRF+\camembert (embeddings)       & \textbf{89.55}    \\
            %            \midrule
            %            \multicolumn{2}{c}{\em Supplement: subword masking model}\\
            %            LSTM+CRF+\camembertoscarswm (embeddings)  & \textbf{90.25} \\
            \bottomrule
        \end{tabu}
    }
    \caption{\textbf{NER} scores on the FTB (best model selected on validation out of 4). Best scores in bold, second best underlined.
        \label{table:ner_ablation}}
\end{table}

\begin{table}[ht]
    \centering\small
    %    \resizebox{\columnwidth}{!}{
    \scalebox{0.89}{
        \begin{tabu}{lcc}
            \toprule
            Model                                             & Acc.             & \#Params \\
            \midrule
            %BiLSTM-max \cite{conneau-etal-2018-xnli} & 68.3 & - \\
            \mbert \cite{devlin-etal-2019-bert}               & 76.9             & 175M     \\
            \xlmmlmtlm \cite{conneau-lample-2019-cross}                 & \underline{80.2} & 250M     \\
            XLM-R\textsubscript{BASE} \cite{conneau-etal-2020-unsupervised}  & 80.1             & 270M     \\
            \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
            \camembert (fine-tuned)                           & \textbf{82.5}    & 110M     \\
            \midrule
            \multicolumn{3}{c}{\em Supplement: LARGE models}                                \\
            XLM-R\textsubscript{LARGE} \cite{conneau-etal-2020-unsupervised} & \underline{85.2} & 550M     \\
            \tabucline[\hbox {$\scriptstyle \cdot$}]{-}
            \camembertccnetlarge (fine-tuned)                 & \textbf{85.7}    & 335M     \\
            \bottomrule
        \end{tabu}
    }
    \caption{\textbf{NLI} accuracy on the French XNLI test set (best model selected on validation out of 10). Best scores in bold, second best underlined.\label{table:xnli}}
\end{table}



%We report the scores of our \camembertccnetlong and \camembertccnetlarge models for all tasks. As mentioned above, \camembertccnetlong was trained 500k steps, whereas \camembertccnetlarge was trained only 100k steps, but is a LARGE model in the sense of \cite{devlin-etal-2019-bert}. Both models are trained on CCNet and use whole-word masking.

\paragraph{POS tagging and dependency parsing}
For POS tagging and dependency parsing, we compare \camembert with other models in the two settings: \textit{fine-tuning} and as \textit{feature-based embeddings}.
%On the one hand, our pretrained language model is \textit{fine-tuned} in an end-to-end manner. On the other hand, task-specific architectures that make use of \camembert or other comparable models as frozen contextual embeddings  %approach and as freezed embeddings as input to UDPipe Future, a task specific architecture  %in Table~\ref{tab:pos_and_dp_results} 
%(details of the two methods in section~\ref{subsection:pos_and_dp}).
We report the results in Table~\ref{tab:pos_and_dp_results}.

\camembert reaches state-of-the-art scores on all treebanks and metrics in both scenarios. The two approaches achieve similar scores, with a slight advantage for the fine-tuned version of \camembert, thus questioning the need for complex task-specific architectures such as UDPipe Future.

Despite a much simpler optimisation process and no task specific architecture, fine-tuning \camembert outperforms UDify on all treebanks and sometimes by a large margin (e.g. +4.15\% LAS on Sequoia and +5.37 LAS on ParTUT).
\camembert also reaches better performance  than other multilingual pretrained models such as \mbert and \xlmmlmtlm on all treebanks.%\footnote{We expected higher scores for \xlmmultimlm than \mbert given its better reported performance on the XNLI benchmark. Although we applied the exact same fine-tuning procedure to \camembert, \xlmmultimlm and \mbert, its performance in POS and dependency parsing is in some cases significantly lower}.

\camembert achieves overall slightly better results than the previous state-of-the-art and task-specific architecture UDPipe Future+\mbert+Flair, except for POS tagging on Sequoia and POS tagging on Spoken, where \camembert lags by 0.03\% and 0.14\% UPOS respectively.
UDPipe Future+\mbert+Flair uses the contextualized string embeddings Flair \citep{akbik-etal-2018-contextual}, which are in fact pretrained contextualized character-level word embeddings specifically designed to handle misspelled words as well as subword structures such as prefixes and suffixes. This design choice might explain the difference in score for POS tagging with CamemBERT, especially for the Spoken treebank where words are not capitalized, a factor that might pose a problem for CamemBERT which was trained on capitalized data, but that might be properly handle by Flair on the UDPipe Future+\mbert+Flair model.

%\camembert also demonstrates higher performances than \mbert on those tasks. We observe a larger error reduction for parsing than for tagging.% For POS tagging, we observe error reductions of respectively 0.71\% for GSD, 0.81\% for Sequoia,  0.7\% for Spoken and 0.28\% for ParTUT. For parsing, we observe error reductions in LAS of 2.96\% for GSD, 3.33\%  for Sequoia, 1.70\% for Spoken and 1.65\% for ParTUT. 
\paragraph{Named-Entity Recognition}
For NER, we similarly evaluate \camembert in the fine-tuning setting and as input embeddings to the task specific architecture LSTM+CRF. We report these scores in Table~\ref{table:ner_ablation}.

In both scenarios, \camembert achieves higher F1 scores than the traditional CRF-based architectures, both non-neural and neural, and than fine-tuned multilingual BERT models.\footnote{\xlmmlmtlm is a lower-case model. Case is crucial for NER, therefore we do not report its low performance (84.37\%)}

Using \camembert as embeddings to the traditional LSTM+CRF architecture gives slightly higher scores than by fine-tuning the model (89.08 vs.~89.55).
This demonstrates that although \camembert can be used successfully without any task-specific architecture, it can still produce high quality contextualized embeddings that might be useful in scenarios where powerful downstream architectures exist.


%Previous work with \mbert showed increased performance in NER for German, Dutch and Spanish when it is used as contextualised word embedding for an NER-specific model \cite{strakova-etal-2019-neural}.
%Our results show that it can also be fine-tuning without adding any additional architecture and with better results than the baselines.
%Even though it gives worse results than our fine-tuned \camembert and our ``\camembert as embeddings'' architecture, it is still interesting to achieve this kind of performance by just fine-tuning a pretrained language model. 


\paragraph{Natural Language Inference}
On the XNLI benchmark, we compare \camembert to previous state-of-the-art multilingual models in the fine-tuning setting. In addition to the standard \camembert model with a BASE architecture, we train another model with the LARGE architecture, referred to as \camembertccnetlarge, for a fair comparison with XLM-R\textsubscript{LARGE}.
This model is trained with the \ccnet corpus, described in Sec.~\ref{sec:origin_and_size}, for 100k steps.\footnote{We train our LARGE model with the \ccnet corpus for practical reasons. Given that BASE models reach similar performance when using \oscar or \ccnet as pretraining corpus (Appendix Table~\ref{tab:ablation}), we expect an \oscar LARGE model to reach comparable scores.} We expect that training the model for longer would yield even better performance.

\camembert reaches higher accuracy than its BASE counterparts reaching +5.6\% over \mbert, +2.3 over \xlmmlmtlm, and +2.4 over XLM-R\textsubscript{BASE}. \camembert also uses as few as half as many parameters (110M vs. 270M for XLM-R\textsubscript{BASE}).

\camembertccnetlarge achieves a state-of-the-art accuracy of 85.7\% on the XNLI benchmark, as opposed to 85.2, for the recent XLM-R\textsubscript{LARGE}.

\camembert uses fewer parameters than multilingual models, mostly because of its smaller vocabulary size (e.g. 32k vs. 250k for XLM-R).
Two elements might explain the better performance of \camembert over XLM-R.
Even though XLM-R was trained on an impressive amount of data (2.5TB), only 57GB of this data is in French, whereas we used 138GB of French data.
Additionally XLM-R also handles 100 languages, and the authors show that when reducing the number of languages to 7, they can reach 82.5\% accuracy for French XNLI with their BASE architecture.



\paragraph{Summary of \camembert's results}
\camembert improves the state of the art for the 4 downstream tasks considered, thereby confirming on French the usefulness of Transformer-based models. We obtain these results when using \camembert as a fine-tuned model or when used as contextual embeddings with task-specific architectures.
This questions the need for more complex downstream architectures, similar to what was shown for English \cite{devlin-etal-2019-bert}.
Additionally, this suggests that \camembert is also able to produce high-quality representations out-of-the-box without further tuning.

\begin{table*}[ht]
    \small\centering
    \resizebox{\textwidth}{!}{
        \tabulinesep =_1pt^1pt
        \begin{tabu}{ l l @{\hspace{0.7cm}}  c  c  @{\hspace{0.7cm}} c  c  @{\hspace{0.7cm}} c  c @{\hspace{0.7cm}} c  c @{\hspace{0.7cm}} c c @{\hspace{0.7cm}} c @{\hspace{0.7cm}} c @{\hspace{0.7cm}}}
            \toprule
                                                    &                                      & \multicolumn{2}{c @{\hspace{0.5cm}}}{\textsc{GSD}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Sequoia}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{Spoken}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{ParTUT}} & \multicolumn{2}{c @{\hspace{0.7cm}}}{\textsc{\textbf{Average}}} & NER               & NLI                                                                                                                            \\
            \cmidrule(l{2pt}r{0.4cm}){3-4}\cmidrule(l{-0.2cm}r{0.4cm}){5-6}\cmidrule(l{-0.2cm}r{0.4cm}){7-8}\cmidrule(l{-0.2cm}r{0.4cm}){9-10}\cmidrule(l{-0.2cm}r{0.4cm}){11-12}\cmidrule(l{-0.2cm}r{0.4cm}){13-13}\cmidrule(l{-0.2cm}r{0.4cm}){14-14}
            \multirow{-2}{*}[2pt]{\textsc{Dataset}} & \multirow{-2}{*}[2pt]{\textsc{Size}} & \textsc{UPOS}                                      & \textsc{LAS}                                           & \textsc{UPOS}                                         & \textsc{LAS}                                          & \textsc{UPOS}                                                   & \textsc{LAS}      & \textsc{UPOS}              & \textsc{LAS}      & \textsc{UPOS}     & \textsc{LAS}      & \textsc{F1}       & \textsc{Acc.}     \\
            \midrule

            \multicolumn{10}{l}{\hspace*{6mm}\em Fine-tuning}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\[0.5mm]
            %\oscar                                  & 0.1GB                                  & 98.12 & 92.28 & 98.52 &  90.09 & 96.45 &  76.01 & 95.08 & 87.49 & - & - & 83.25 & 72.98\\ 
            %100MB & 98.12 & 92.28 & 98.52 &  90.09 & 96.45 &  76.01 & 95.08 & 87.49  & 83.25 & 72.98\\
            Wiki                                    & 4GB                                  & 98.28                                              & 93.04                                                  & 98.74                                                 & 92.71                                                 & 96.61                                                           & 79.61             & 96.20                      & 89.67             & 97.45             & 88.75             & 89.86             & 78.32             \\ %  10137841 10137842 parsing, ppos 10138173  tagging
            \ccnet                                  & 4GB                                  & 98.34                                              & 93.43                                                  & 98.95                                                 & 93.67                                                 & 96.92                                                           & \textbf{82.09}    & 96.50                      & \textbf{90.98}    & 97.67             & \textbf{90.04}    & 90.46             & \textbf{82.06}    \\
            \oscar                                  & 4GB                                  & \underline{98.35}                                  & \underline{93.55}                                      & \underline{98.97}                                     & \underline{93.70}                                     & \underline{96.94}                                               & \underline{81.97} & \underline{96.58}          & 90.28             & \underline{97.71} & 89.87             & \underline{90.65} & \underline{81.88} \\
            \tabucline[\hbox{$\scriptstyle \cdot$}]{-}
            %\ccnet & 135GB & \underline{98.36} &  90.57 & 98.97 & 94.04 & 96.98 &  82.07 & 96.39  & \textbf{91.18} & 90.13 & \textbf{82.22} \\%  -  &  -  &
            \oscar                                  & 138GB                                & \textbf{98.39}                                     & \textbf{93.80}                                         & \textbf{98.99}                                        & \textbf{94.00}                                        & \textbf{97.17}                                                  & 81.18             & \textbf{96.63}             & \underline{90.56} & \textbf{97.79}    & \underline{89.88} & \textbf{91.55}    & 81.55             \\
            \midrule
            \multicolumn{11}{l}{\hspace*{6mm}\em Embeddings (with UDPipe Future (tagging, parsing) or LSTM+CRF (NER))}                                                                                                                                                                                                                                                                                                                                                                                                                          \\[0.5mm]
            %\oscar                                  & 0.1GB                                  & 98.04 & 91.95 & 98.73 & 92.60 & 96.96 & 81.02 & - & - & - & - & 89.78 & - \\ 
            Wiki                                    & 4GB                                  & 98.09                                              & 92.31                                                  & 98.74                                                 & 93.55                                                 & 96.24                                                           & 78.91             & 95.78                      & 89.79             & 97.21             & 88.64             & 91.23             & -                 \\
            \ccnet                                  & 4GB                                  & \textbf{98.22}                                     & \textbf{92.93}                                         & \underline{99.12}                                     & \underline{94.65}                                     & 97.17                                                           & \textbf{82.61}    & \underline{\textbf{96.74}} & \underline{89.95} & \underline{97.81} & \underline{90.04} & \textbf{92.30}    & -                 \\
            \oscar                                  & 4GB                                  & \underline{98.21}                                  & \underline{92.77}                                      & \underline{99.12}                                     & \textbf{94.92}                                        & \underline{97.20}                                               & \underline{82.47} & \underline{\textbf{96.74}} & \textbf{90.05}    & \textbf{97.82}    & \textbf{90.05}    & \underline{91.90} & -                 \\
            \tabucline[\hbox{$\scriptstyle \cdot$}]{-}
            %\ccnet & 135GB & 98.27   & 92.94  &   99.05  &  94.51   & 97.04  & 82.09  & \underline{96.68}  & 89.89 & 91.88  & - \\  -  &  -  &
            \oscar                                  & 138GB                                & 98.18                                              & \underline{92.77}                                      & \textbf{99.14}                                        & 94.24                                                 & \textbf{97.26}                                                  & 82.44             & 96.52                      & 89.89             & 97.77             & 89.84             & 91.83             & -                 \\

            \bottomrule
        \end{tabu}
    }
    \caption{Results on the four tasks using language models pre-trained on data sets of varying homogeneity and size, reported on validation sets (average of 4 runs for POS tagging, parsing and NER, average of 10 runs for NLI).}

    \label{tab:ablation_data_size}
\end{table*}


\section{Impact of corpus origin and size}
\label{sec:origin_and_size}

In this section we investigate the influence of the homogeneity and size of the pretraining corpus on downstream task performance. With this aim, we train alternative version of \camembert by varying the pretraining datasets. For this experiment, we fix the number of pretraining steps to 100k, and allow the number of epochs to vary accordingly (more epochs for smaller dataset sizes). All models use the BASE architecture.

In order to investigate the need for homogeneous clean data versus more diverse and possibly noisier data, we use alternative sources of pretraining data in addition to \oscar:
\begin{itemize}
    \item \textbf{Wikipedia}, which is homogeneous in terms of genre and style. We use the official 2019 French Wikipedia dumps\footnote{ \url{https://dumps.wikimedia.org/backup-index.html}.}. We remove HTML tags and tables using Giuseppe Attardi's  \emph{WikiExtractor}.\footnote{ \url{https://github.com/attardi/wikiextractor}.}
    \item \textbf{\ccnet} \cite{wenzek-etal-2020-ccnet}, a dataset extracted from Common Crawl with a different filtering process than for \oscar. It was built using a language model trained on Wikipedia, in order to filter out bad quality texts such as code or tables.\footnote{We use the \textsc{head} split, which corresponds to the top 33\% of documents in terms of filtering perplexity.} As this filtering step biases the noisy data from Common Crawl to more Wikipedia-like text, we expect \ccnet to act as a middle ground between the unfiltered ``noisy'' \oscar dataset, and the ``clean'' Wikipedia dataset. As a result of the different filtering processes, \ccnet contains longer documents on average compared to \oscar with smaller---and often noisier---documents weeded out.
\end{itemize}
Table~\ref{table:corpora_statistics} summarizes statistics of these different corpora.

\begin{table}[ht]
    \centering\small
        \begin{tabular}{lcccccc}
            \toprule
            Corpus    & Size  & \#tokens & \#docs & \multicolumn{3}{c}{Tokens/doc}                 \\
                      &       &          &        & \multicolumn{3}{c}{Percentiles:}               \\
                      &       &          &        & 5\%                              & 50\% & 95\% \\
            \midrule
            Wikipedia & 4GB   & 990M     & 1.4M   & 102                              & 363  & 2530 \\
            CCNet     & 135GB & 31.9B    & 33.1M  & 128                              & 414  & 2869 \\
            OSCAR     & 138GB & 32.7B    & 59.4M  & 28                               & 201  & 1946 \\
            \bottomrule
        \end{tabular}
    \caption{Statistics on the pretraining datasets used.}
    \label{table:corpora_statistics}
\end{table}

In order to make the comparison between these three sources of pretraining data, we randomly sample 4GB of text (at the document level) from \oscar and \ccnet, thereby creating samples of both Common-Crawl-based corpora of the same size as the French Wikipedia. These smaller 4GB samples also provides us a way to investigate the impact of pretraining data size. Downstream task performance for our alternative versions of \camembert are provided in Table~\ref{tab:ablation_data_size}.
The upper section reports scores in the fine-tuning setting while the lower section reports scores for the embeddings.

\subsection{Common Crawl vs.~Wikipedia?}
\label{subsec:homogeneityimpact}

Table~\ref{tab:ablation_data_size} clearly shows that models trained on the 4GB versions of \oscar and \ccnet (Common Crawl) perform consistently better than the the one trained on the French Wikipedia. This is true both in the fine-tuning and embeddings setting. Unsurprisingly, the gap is larger on tasks involving texts whose genre and style are more divergent from those of Wikipedia, such as tagging and parsing on the Spoken treebank.
The performance gap is also very large on the XNLI task, probably as a consequence of the larger diversity of Common-Crawl-based corpora in terms of genres and topics. XNLI is indeed based on multiNLI which covers a range of genres of spoken and written text.

The downstream task performances of the models trained on the 4GB version of \ccnet and \oscar are much more similar.\footnote{We provide the results of a model trained on the whole \ccnet corpus in the Appendix. The conclusions are similar when comparing models trained on the full corpora: downstream results are similar when using \oscar or \ccnet.}


\subsection{How much data do you need?}
\label{subsec:sizeimpact}

An unexpected outcome of our experiments is that the model trained ``only'' on the 4GB sample of \oscar performs similarly  to the standard \camembert trained on the whole 138GB \oscar.
The only task with a large performance gap is NER, where  ``138GB'' models are better by 0.9 F1 points. This could be due to the higher number of named entities present in the larger corpora, which is beneficial for this task. On the contrary, other tasks don't seem to gain from the additional data.
%In settings where the language model is used as embeddings, the ``4GB'' model actually performs better than the standard ``138GB'' \camembert more often than the other way round, although differences in scores are rarely striking. For fine-tuning settings, the standard \camembert usually performs better than the 4GB-based one, but here again, differences are always small. 

In other words, when trained on corpora such as \oscar and \ccnet, which are heterogeneous in terms of genre and style, 4GB of uncompressed text is large enough as pretraining corpus to reach state-of-the-art results with the BASE architecure, better than those obtained with \mbert (pretrained on 60GB of text).\footnote{The OSCAR-4GB model gets slightly better XNLI accuracy than the full OSCAR-138GB model (81.88 vs. 81.55). This might be due to the random seed used for pretraining, as each model is pretrained only once.} This calls into question the need to use a very large corpus such as \oscar or \ccnet when training a monolingual Transformer-based language model such as BERT or \roberta.
Not only does this mean that the computational (and therefore environmental) cost of training a state-of-the-art language model can be reduced, but it also means that \camembert-like models can be trained for all languages for which a Common-Crawl-based corpus of 4GB or more can be created. \oscar is available in 166 languages, and provides such a corpus for 38 languages. Moreover, it is possible that slightly smaller corpora (e.g.~down to 1GB) could also prove sufficient to train high-performing language models.
We obtained our results with BASE architectures. Further research is needed to confirm the validity of our findings on larger architectures and other more complex natural language understanding tasks.
However, even with a BASE architecture and 4GB of training data, the validation loss is still decreasing beyond 100k steps (and 400 epochs). This suggests that we are still under-fitting the 4GB pretraining dataset, training longer might increase downstream performance.

\section{Discussion}

Since the pre-publication of this work \cite{martin-etal-2020-camembert},
many monolingual language models have appeared, e.g.
\cite{le-etal-2020-flaubert-unsupervised,virtanen-etal-2019-multilingual,delobelle-etal-2020-robbert}, for as much as 30
languages \cite{nozza-etal-2020-what}. In almost all tested
configurations they displayed better results than multilingual
language models such as \mbert \cite{pires-etal-2019-multilingual}.
Interestingly, \newcite{le-etal-2020-flaubert-unsupervised} showed that using their
FlauBert, a RoBERTa-based language model for French, which was trained on less
but more edited data, in conjunction to \camembert in an ensemble
system could improve the performance of a parsing model and  establish
a new state-of-the-art in constituency parsing of French, highlighting thus
the complementarity of both models.\footnote{We refer the reader to
    \cite{le-etal-2020-flaubert-unsupervised} for a comprehensive benchmark and details
    therein.}
As it was the case for English when \bert was first released, the
availability of similar scale language models for French enabled
interesting applications, such as large scale anonymization of legal
texts, where \camembert-based models established a new
state-of-the-art on this task \cite{benesty-2019-ner}, or the first
large question answering experiments on a French Squad data set that
was released very recently \cite{dhoffschmidt-etal-2020-fquad} where the authors matched human performance using \camembertlarge.
Being the first pre-trained language model that used the open-source Common Crawl Oscar
corpus and given its impact on the community,
\camembert paved the way for many works on monolingual language
models that followed. Furthermore, the availability of all its
training data favors reproducibility and is a step towards better
understanding such models.
In that spirit, we make the models used in our experiments available via our website and via the \texttt{huggingface} and \texttt{fairseq} APIs, in addition to the base \camembert model.

\section{Conclusion}
In this work, we investigated the feasibility of training a
Transformer-based language model for languages other than English.
Using French as an example, we trained \camembert, a language model
based on \roberta.
We evaluated \camembert on four downstream tasks
(part-of-speech tagging, dependency parsing, named entity recognition
and natural language inference) in which our best model reached or improved the state of the art in all tasks
considered, even when compared to strong multilingual models such as
\mbert, XLM and XLM-R, while also having fewer parameters.

Our experiments demonstrate that using web crawled
data with high variability is preferable to using Wikipedia-based data.  In addition we
showed that our models could reach surprisingly high performances with
as low as 4GB of pretraining data, questioning thus the need for large
scale pretraining corpora.  This shows that state-of-the-art
Transformer-based language models can be trained on languages with far
fewer resources than English, whenever a few gigabytes of data are
available. This paves the way for the rise of monolingual contextual
pre-trained language-models for under-resourced languages.  The
question of knowing whether pretraining on small domain specific
content will be a better option than transfer learning techniques such
as fine-tuning remains open and we leave it for future work.

%% ds: la formulation est un peu cheloue. C con qu'il y ait plus les \draftnote et tout...

Pretrained on pure open-source corpora, \camembert is freely available and distributed with the MIT license via popular NLP libraries (\href{https://github.com/pytorch/fairseq}{\texttt{fairseq}} and \href{https://github.com/huggingface/transformers}{\texttt{huggingface}}) as well as on our website \href{https://camembert-model.fr}{\texttt{camembert-model.fr}}.

%\ds{Say something like: Since the beginning of this work, more monolingual models have appeared such as FlauBERT, finish bert... - ds: I'm on it}
%We will publish an updated version in the near future where we will explore and release models trained for longer, with additional downstream tasks, baselines (e.g. XLM) and analysis, we will also train additional models with potentially cleaner corpora such as CCNet \cite{wenzek-etal-2020-ccnet} for more accurate performance evaluation and more complete ablation.


