% Encoding: UTF-8

@inproceedings{abadji-etal-2021-ungoliant,
  abstract  = {Since the introduction of large language models in Natural Language Processing, large raw corpora have played a crucial role in Computational Linguistics. However, most of these large raw corpora are either available only for English or not available to the general public due to copyright issues. Nevertheless, there are some examples of freely available multilingual corpora for training Deep Learning NLP models, such as the OSCAR and Paracrawl corpora. However, they have quality issues, especially for low-resource languages. Moreover, recreating or updating these corpora is very complex. In this work, we try to reproduce and improve the goclassy pipeline used to create the OSCAR corpus. We propose a new pipeline that is faster, modular, parameterizable, and well documented. We use it to create a corpus similar to OSCAR but larger and based on recent data. Also, unlike OSCAR, the metadata information is at the document level. We release our pipeline under an open source license and publish the corpus under a research-only license.},
  address   = {Mannheim},
  author    = {Julien Abadji and Pedro Javier Ortiz Su{\'a}rez and Laurent Romary and Beno{\^i}t Sagot},
  booktitle = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event)},
  doi       = {10.14618/ids-pub-10468},
  editor    = {Harald L{\"u}ngen and Marc Kupietz and Piotr Bański and Adrien Barbaresi and Simon Clematide and Ines Pisetta},
  language  = {en},
  pages     = {1 -- 9},
  publisher = {Leibniz-Institut f{\"u}r Deutsche Sprache},
  series    = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event)},
  title     = {Ungoliant: An optimized pipeline for the generation of a very large-scale multilingual web corpus},
  url       = {https://nbn-resolving.org/urn:nbn:de:bsz:mh39-104688},
  year      = {2021}
}

@inbook{abeille-etal-2003-building,
  abstract  = {We present a treebank project for French. We have annotated a newspaper corpus of 1 Million words with part of speech, inflection, compounds, lemmas and constituency. We describe the tagging and parsing phases of the project, and for each, the automatic tools, the guidelines and the validation process. We then present some uses of the corpus as well as some directions for future work.},
  address   = {Dordrecht},
  author    = {Abeill{\'e}, Anne
               and Cl{\'e}ment, Lionel
               and Toussenel, Fran{\c{c}}ois},
  booktitle = {Treebanks: Building and Using Parsed Corpora},
  doi       = {10.1007/978-94-010-0201-1_10},
  isbn      = {978-94-010-0201-1},
  pages     = {165--187},
  publisher = {Springer Netherlands},
  title     = {Building a Treebank for French},
  url       = {https://doi.org/10.1007/978-94-010-0201-1_10},
  year      = {2003}
}

@article{ando-zhang-2005-framework,
  author  = {Rie Kubota Ando and Tong Zhang},
  journal = {Journal of Machine Learning Research},
  number  = {61},
  pages   = {1817-1853},
  title   = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
  url     = {http://jmlr.org/papers/v6/ando05a.html},
  volume  = {6},
  year    = {2005}
}

@article{arivazhagan-etal-2019-massively,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190705019A},
  archiveprefix = {arXiv},
  author        = {{Arivazhagan}, Naveen and {Bapna}, Ankur and {Firat}, Orhan and {Lepikhin}, Dmitry and {Johnson}, Melvin and {Krikun}, Maxim and {Chen}, Mia Xu and {Cao}, Yuan and {Foster}, George and {Cherry}, Colin and {Macherey}, Wolfgang and {Chen}, Zhifeng and {Wu}, Yonghui},
  eid           = {arXiv:1907.05019},
  eprint        = {1907.05019},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  month         = jul,
  pages         = {arXiv:1907.05019},
  primaryclass  = {cs.CL},
  title         = {{Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges}},
  year          = 2019
}

@misc{barera-2020-mind,
  author       = {Barera, Michael},
  howpublished = {https://rc.library.uta.edu/uta-ir/handle/10106/29572},
  publisher    = {University of Texas at Arlington},
  title        = {Mind the Gap: Addressing Structural Equity and Inclusion on Wikipedia},
  year         = {2020}
}

@article{baroni-etal-2009-the,
  abstract      = {This article introduces ukWaC, deWaC and itWaC, three very large corpora of English, German, and Italian built by web crawling, and describes the methodology and tools used in their construction. The corpora contain more than a billion words each, and are thus among the largest resources for the respective languages. The paper also provides an evaluation of their suitability for linguistic research, focusing on ukWaC and itWaC. A comparison in terms of lexical coverage with existing resources for the languages of interest produces encouraging results. Qualitative evaluation of ukWaC versus the British National Corpus was also conducted, so as to highlight differences in corpus composition (text types and subject matters). The article concludes with practical information about format and availability of corpora and tools.},
  author        = {Baroni, Marco and Bernardini, Silvia and Ferraresi, Adriano and Zanchetta, Eros},
  bdsk-url-1    = {https://doi.org/10.1007/s10579-009-9081-4},
  da            = {2009/09/01},
  date-added    = {2021-09-21 2:13:54 PM +0200},
  date-modified = {2021-09-21 2:13:54 PM +0200},
  doi           = {10.1007/s10579-009-9081-4},
  id            = {Baroni2009},
  isbn          = {1574-0218},
  journal       = {Language Resources and Evaluation},
  number        = {3},
  pages         = {209--226},
  title         = {The WaCky wide web: a collection of very large linguistically processed web-crawled corpora},
  ty            = {JOUR},
  url           = {https://doi.org/10.1007/s10579-009-9081-4},
  volume        = {43},
  year          = {2009}
}

@inproceedings{bechet-charton-2010-unsupervised,
  author    = {Bechet, Frederic and Charton, Eric},
  booktitle = {2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
  doi       = {10.1109/ICASSP.2010.5494962},
  number    = {},
  pages     = {5338-5341},
  title     = {Unsupervised knowledge acquisition for Extracting Named Entities from speech},
  volume    = {},
  year      = {2010}
}

@inproceedings{bender-etal-2021-on,
  abstract  = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  address   = {New York, NY, USA},
  author    = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  doi       = {10.1145/3442188.3445922},
  isbn      = {9781450383097},
  location  = {Virtual Event, Canada},
  numpages  = {14},
  pages     = {610–623},
  publisher = {Association for Computing Machinery},
  series    = {FAccT '21},
  title     = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?{{\NotoEmoji \symbol{"1F99C}}}},
  url       = {https://doi.org/10.1145/3442188.3445922},
  year      = {2021}
}

@webpage{benesty-2019-ner,
  author      = {Benesty, Michaël},
  bdsk-url-1  = {https://towardsdatascience.com/benchmark-ner-algorithm-d4ab01b2d4c3?source=friends_link&sk=5bffa2cb19997d1658479f18ce8cf6bb},
  lastchecked = {March 14, 2020},
  month       = {December},
  title       = {NER algo benchmark: spaCy, Flair, m-BERT and camemBERT on anonymizing French commercial legal cases},
  url         = {https://towardsdatascience.com/benchmark-ner-algorithm-d4ab01b2d4c3?source=friends_link&sk=5bffa2cb19997d1658479f18ce8cf6bb},
  year        = {2019}
}

@article{biber-1993-representativeness,
  abstract = {{The present paper addresses a number of issues related to achieving ‘representativeness’ in linguistic corpus design, including: discussion of what it means to ‘represent’ a language, definition of the target population, stratified versus proportional sampling of a language, sampling within texts, and issues relating to the required sample size (number of texts) of a corpus. The paper distinguishes among various ways that linguistic features can be distributed within and across texts; it analyses the distributions of several particular features, and it discusses the implications of these distributions for corpus design.The paper argues that theoretical research should be prior in corpus design, to identify the situational parameters that distinguish among texts in a speech community, and to identify the types of linguistic features that will be analysed in the corpus. These theoretical considerations should be complemented by empirical investigations of linguistic variation in a pilot corpus of texts, as a basis for specific sampling decisions. The actual construction of a corpus would then proceed in cycles: the original design based on theoretical and pilot-study analyses, followed by collection of texts, followed by further empirical investigations of linguistic variation and revision of the design.}},
  author   = {Biber, Douglas},
  doi      = {10.1093/llc/8.4.243},
  eprint   = {https://academic.oup.com/dsh/article-pdf/8/4/243/10889454/243.pdf},
  issn     = {0268-1145},
  journal  = {Literary and Linguistic Computing},
  month    = {01},
  number   = {4},
  pages    = {243-257},
  title    = {{Representativeness in Corpus Design}},
  url      = {https://doi.org/10.1093/llc/8.4.243},
  volume   = {8},
  year     = {1993}
}

@article{biderman-etal-2020-pitfalls,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv201102832B},
  archiveprefix = {arXiv},
  author        = {{Biderman}, Stella and {Scheirer}, Walter J.},
  eid           = {arXiv:2011.02832},
  eprint        = {2011.02832},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Machine Learning, Statistics - Methodology, Statistics - Machine Learning},
  month         = nov,
  pages         = {arXiv:2011.02832},
  primaryclass  = {cs.LG},
  title         = {{Pitfalls in Machine Learning Research: Reexamining the Development Cycle}},
  year          = 2020
}

@inproceedings{birhane-etal-2021-large,
  abstract  = {In this paper we investigate problematic practices and consequences of large scale vision datasets (LSVDs). We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class- wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both the society at large and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique their pros and cons. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation.},
  author    = {Birhane, Abeba and Prabhu, Vinay Uday},
  booktitle = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  doi       = {10.1109/WACV48630.2021.00158},
  issn      = {2642-9381},
  keywords  = {},
  month     = {Jan},
  number    = {},
  pages     = {1536-1546},
  title     = {Large image datasets: A pyrrhic win for computer vision?},
  volume    = {},
  year      = {2021}
}

@misc{black-etal-2021-gpt,
  author    = {Black, Sid and
               Gao, Leo and
               Wang, Phil and
               Leahy, Connor and
               Biderman, Stella},
  doi       = {10.5281/zenodo.5297715},
  month     = mar,
  note      = {{If you use this software, please cite it using 
               these metadata.}},
  publisher = {Zenodo},
  title     = {{GPT-Neo: Large Scale Autoregressive Language 
               Modeling with Mesh-Tensorflow}},
  url       = {https://doi.org/10.5281/zenodo.5297715},
  version   = {1.0},
  year      = 2021
}

@article{blumofe-etal-1999-scheduling,
  abstract   = {This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured)
                multithreaded computations on parallel computers. A popular and practical method of
                scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which
                processors needing work steal computational threads from other processors. In this
                paper, we give the first provably good work-stealing scheduler for multithreaded computations
                with dependencies.Specifically, our analysis shows that the expected time to execute
                a fully strict computation on P processors using our work-stealing scheduler is T1/P
                + O(T ∞ , where T1 is the minimum serial execution time of the multithreaded computation
                and (T ∞ is the minimum execution time with an infinite number of processors. Moreover,
                the space required by the execution is at most S1P, where S1 is the minimum serial
                space requirement. We also show that the expected total communication of the algorithm
                is at most O(PT ∞( 1 + nd)Smax), where Smax is the size of the largest activation
                record of any thread and nd is the maximum number of times that any thread synchronizes
                with its parent. This communication bound justifies the folk wisdom that work-stealing
                schedulers are more communication efficient than their work-sharing counterparts.
                All three of these bounds are existentially optimal to within a constant factor.},
  address    = {New York, NY, USA},
  author     = {Blumofe, Robert D. and Leiserson, Charles E.},
  doi        = {10.1145/324133.324234},
  issn       = {0004-5411},
  issue_date = {Sept. 1999},
  journal    = {J. ACM},
  keywords   = {multithreading, work stealing, critical-path length, randomized algorithm, multiprocessor, thread scheduling},
  month      = sep,
  number     = {5},
  numpages   = {29},
  pages      = {720–748},
  publisher  = {Association for Computing Machinery},
  title      = {Scheduling Multithreaded Computations by Work Stealing},
  url        = {https://doi.org/10.1145/324133.324234},
  volume     = {46},
  year       = {1999}
}

@inproceedings{bnc-2007-the,
  author    = {BNC Consortium and others},
  booktitle = {The British National Corpus, version 3 - BNC XML Edition},
  title     = {520 million words, 1990-present},
  url       = {http://www.natcorp.ox.ac.uk/},
  year      = 2007
}

@inproceedings{bonami-etal-2015-implicative,
  author    = {Olivier Bonami and
               Sacha Beniamine},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/networds/BonamiB15.bib},
  booktitle = {Proceedings of the NetWordS Final Conference on Word Knowledge and
               Word Usage: Representations and Processes in the Mental Lexicon, Pisa,
               Italy, March 30 - April 1, 2015},
  editor    = {Vito Pirrelli and
               Claudia Marzi and
               Marcello Ferro},
  pages     = {4--9},
  publisher = {CEUR-WS.org},
  series    = {{CEUR} Workshop Proceedings},
  timestamp = {Tue, 28 Jul 2020 15:05:48 +0200},
  title     = {Implicative structure and joint predictiveness},
  url       = {http://ceur-ws.org/Vol-1347/paper01.pdf},
  volume    = {1347},
  year      = {2015}
}

@inproceedings{brown-etal-2020-language,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{buch-kromann-2003-the,
  author    = {Buch-Kromann, Matthias},
  booktitle = {2nd Workshop on Treebanks and Linguistic Theories (TLT), Sweden},
  pages     = {217--220},
  title     = {The Danish dependency treebank and the dtag treebank tool},
  year      = {2003}
}

@misc{callan-etal-2009-clueweb09,
  author = {Callan, Jamie and Hoy, Mark and Yoo, Changkuk and Zhao, Le},
  title  = {Clueweb09 data set},
  year   = {2009}
}

@misc{camps-etal-2019-geste,
  author    = {Jean-Baptiste-Camps and LAKME-ENC and AliceCochet and LucenceIng and Paulinelvq},
  doi       = {10.5281/zenodo.2630574},
  month     = apr,
  publisher = {Zenodo},
  title     = {{Geste: Geste: un corpus de chansons de geste}},
  url       = {https://doi.org/10.5281/zenodo.2630574},
  version   = {v02},
  year      = 2019
}

@article{caswell-etal-2021-quality,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2021arXiv210312028C},
  archiveprefix = {arXiv},
  author        = {{Caswell}, Isaac and {Kreutzer}, Julia and {Wang}, Lisa and {Wahab}, Ahsan and {van Esch}, Daan and {Ulzii-Orshikh}, Nasanbayar and {Tapo}, Allahsera and {Subramani}, Nishant and {Sokolov}, Artem and {Sikasote}, Claytone and {Setyawan}, Monang and {Sarin}, Supheakmungkol and {Samb}, Sokhar and {Sagot}, Beno{\^\i}t and {Rivera}, Clara and {Rios}, Annette and {Papadimitriou}, Isabel and {Osei}, Salomey and {Ortiz Su{\'a}rez}, Pedro Javier and {Orife}, Iroro and {Ogueji}, Kelechi and {Niyongabo}, Rubungo Andre and {Nguyen}, Toan Q. and {M{\"u}ller}, Mathias and {M{\"u}ller}, Andr{\'e} and {Hassan Muhammad}, Shamsuddeen and {Muhammad}, Nanda and {Mnyakeni}, Ayanda and {Mirzakhalov}, Jamshidbek and {Matangira}, Tapiwanashe and {Leong}, Colin and {Lawson}, Nze and {Kudugunta}, Sneha and {Jernite}, Yacine and {Jenny}, Mathias and {Firat}, Orhan and {Dossou}, Bonaventure F.~P. and {Dlamini}, Sakhile and {de Silva}, Nisansa and {{\c{C}}abuk Ball{\i}}, Sakine and {Biderman}, Stella and {Battisti}, Alessia and {Baruwa}, Ahmed and {Bapna}, Ankur and {Baljekar}, Pallavi and {Abebe Azime}, Israel and {Awokoya}, Ayodele and {Ataman}, Duygu and {Ahia}, Orevaoghene and {Ahia}, Oghenefego and {Agrawal}, Sweta and {Adeyemi}, Mofetoluwa},
  eid           = {arXiv:2103.12028},
  eprint        = {2103.12028},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  month         = mar,
  pages         = {arXiv:2103.12028},
  primaryclass  = {cs.CL},
  title         = {{Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets}},
  year          = 2021
}

@misc{chan-etal-2019-german,
  author       = {Branden Chan and Timo M{\"o}ller and Malte Pietsch and Tanay Soni and Chin Man Yeung},
  howpublished = {\url{https://deepset.ai/german-bert}},
  title        = {German BERT},
  year         = {2019}
}

@article{chanier-etal-2014-the,
  author      = {Chanier, Thierry and Poudat, C{\'e}line and Sagot, Beno{\^i}t and Antoniadis, Georges and Wigham, Ciara R. and Hriba, Linda and Longhi, Julien and Seddah, Djam{\'e}},
  hal_id      = {halshs-00953507},
  hal_version = {v2},
  journal     = {{Journal for language technology and computational linguistics}},
  keywords    = {Computer Mediated Communication ; CMC ; CoMeRe ; corpus},
  note        = {Final version to Special Issue of JLCL (Journal of Language Technology and Computational Linguistics (JLCL, http://jlcl.org/): BUILDING AND ANNOTATING CORPORA OF COMPUTER-MEDIATED DISCOURSE: Issues and Challenges at the Interface of Corpus and Computational Linguistics (ed. by Michael Bei{\ss}wenger, Nelleke Oostdijk, Angelika Storrer \& Henk van den Heuvel)},
  number      = {2},
  pages       = {1-30},
  pdf         = {https://halshs.archives-ouvertes.fr/halshs-00953507v2/file/cmr-article-jlcl-v140912-hal.pdf},
  publisher   = {{GSCL (Gesellschaft f{\"u}r Sprachtechnologie und Computerlinguistik) }},
  title       = {{The CoMeRe corpus for French: structuring and annotating heterogeneous CMC genres}},
  url         = {https://halshs.archives-ouvertes.fr/halshs-00953507},
  volume      = {29},
  year        = {2014}
}

@inproceedings{chelba-etal-2014-one,
  author    = {Ciprian Chelba and
               Tom{\'{a}}s Mikolov and
               Mike Schuster and
               Qi Ge and
               Thorsten Brants and
               Phillipp Koehn and
               Tony Robinson},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/interspeech/ChelbaMSGBKR14.bib},
  booktitle = {{INTERSPEECH} 2014, 15th Annual Conference of the International Speech
               Communication Association, Singapore, September 14-18, 2014},
  editor    = {Haizhou Li and
               Helen M. Meng and
               Bin Ma and
               Engsiong Chng and
               Lei Xie},
  pages     = {2635--2639},
  publisher = {{ISCA}},
  timestamp = {Tue, 16 Nov 2021 11:42:24 +0100},
  title     = {One billion word benchmark for measuring progress in statistical language
               modeling},
  url       = {http://www.isca-speech.org/archive/interspeech\_2014/i14\_2635.html},
  year      = {2014}
}

@article{chriqui-etal-2021-hebert,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2021arXiv210201909C},
  archiveprefix = {arXiv},
  author        = {{Chriqui}, Avihay and {Yahav}, Inbal},
  eid           = {arXiv:2102.01909},
  eprint        = {2102.01909},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  month         = feb,
  pages         = {arXiv:2102.01909},
  primaryclass  = {cs.CL},
  title         = {{HeBERT \& HebEMO: a Hebrew BERT Model and a Tool for Polarity Analysis and Emotion Recognition}},
  year          = 2021
}

@inproceedings{clark-etal-2020-electra,
  author    = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  booktitle = {International Conference on Learning Representations},
  title     = {ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  url       = {https://openreview.net/forum?id=r1xMH1BtvB},
  year      = {2020}
}

@inproceedings{conneau-lample-2019-cross,
  author    = {Conneau, Alexis and Lample, Guillaume},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Cross-lingual Language Model Pretraining},
  url       = {https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@inproceedings{dai-le-2015-semi,
  author    = {Dai, Andrew M and Le, Quoc V},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Semi-supervised Sequence Learning},
  url       = {https://proceedings.neurips.cc/paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf},
  volume    = {28},
  year      = {2015}
}

@book{dale-etal-2000-handbook,
  address   = {New York, Basel},
  author    = {Dale, Robert and Somers, Harold L. and Moisl, Hermann },
  isbn      = {0-585-36769-8},
  pages     = {XVIII-943 p.},
  publisher = {M. Dekker},
  title     = {Handbook of natural language processing},
  url       = {http://www.sudoc.fr/084369299},
  year      = {2000}
}

@article{davies-2009-the,
  abstract  = {The Corpus of Contemporary American English (COCA), which was released online in early 2008, is the first large and diverse corpus of American English. In this paper, we first discuss the design of the corpus — which contains more than 385 million words from 1990–2008 (20 million words each year), balanced between spoken, fiction, popular magazines, newspapers, and academic journals. We also discuss the unique relational databases architecture, which allows for a wide range of queries that are not available (or are quite difficult) with other architectures and interfaces. To conclude, we consider insights from the corpus on a number of cases of genre-based variation and recent linguistic variation, including an extended analysis of phrasal verbs in contemporary American English.},
  author    = {Davies, Mark},
  doi       = {https://doi.org/10.1075/ijcl.14.2.02dav},
  issn      = {1384-6655},
  journal   = {International Journal of Corpus Linguistics},
  keywords  = {corpus},
  keywords  = {genres},
  keywords  = {American English},
  keywords  = {phrasal verbs},
  keywords  = {diachronic},
  keywords  = {relational databases},
  number    = {2},
  pages     = {159-190},
  publisher = {John Benjamins},
  title     = {The 385+ million word Corpus of Contemporary American English (1990–2008+): Design, architecture, and linguistic insights},
  type      = {Journal Article},
  url       = {https://www.jbe-platform.com/content/journals/10.1075/ijcl.14.2.02dav},
  volume    = {14},
  year      = {2009}
}

@article{davies-2010-the,
  abstract = {{The Corpus of Contemporary American English is the first large, genre-balanced corpus of any language, which has been designed and constructed from the ground up as a ‘monitor corpus’, and which can be used to accurately track and study recent changes in the language. The 400 million words corpus is evenly divided between spoken, fiction, popular magazines, newspapers, and academic journals. Most importantly, the genre balance stays almost exactly the same from year to year, which allows it to accurately model changes in the ‘real world’. After discussing the corpus design, we provide a number of concrete examples of how the corpus can be used to look at recent changes in English, including morphology (new suffixes –friendly and –gate), syntax (including prescriptive rules, quotative like, so not ADJ, the get passive, resultatives, and verb complementation), semantics (such as changes in meaning with web, green, or gay), and lexis––including word and phrase frequency by year, and using the corpus architecture to produce lists of all words that have had large shifts in frequency between specific historical periods.}},
  author   = {Davies, Mark},
  doi      = {10.1093/llc/fqq018},
  eprint   = {https://academic.oup.com/dsh/article-pdf/25/4/447/6191556/fqq018.pdf},
  issn     = {0268-1145},
  journal  = {Literary and Linguistic Computing},
  month    = {10},
  number   = {4},
  pages    = {447-464},
  title    = {{The Corpus of Contemporary American English as the first reliable monitor corpus of English}},
  url      = {https://doi.org/10.1093/llc/fqq018},
  volume   = {25},
  year     = {2010}
}

@inproceedings{desrochers-etal-2016-a,
  abstract  = {Recent Intel processors support the Running Average Power Level (RAPL) interface, which among other things provides estimated energy measurements for the CPUs, integrated GPU, and DRAM. These measurements are easily accessible by the user, and can be gathered by a wide variety of tools, including the Linux perf_event interface. This allows unprecedented easy access to energy information when designing and optimizing energy-aware code.While greatly useful, on most systems these RAPL measurements are estimated values, generated on the fly by an on-chip energy model. The values are not documented well, and the results (especially the DRAM results) have undergone only limited validation.We validate the DRAM RAPL results on both desktop and server Haswell machines, with multiple types of DDR3 and DDR4 memory. We instrument the hardware to gather actual power measurements and compare them to the RAPL values returned via Linux perf_event. We describe the many challenges encountered when instrumenting systems for detailed power measurement.We find that the RAPL results match overall energy and power trends, usually by a constant power offset. The results match best when the DRAM is being heavily utilized, but do not match as well in cases where the system is idle, or when an integrated GPU is using the memory.We also verify that Haswell server machines produce more accurate results, as they include actual power measurements gathered through the integrated voltage regulator.},
  address   = {New York, NY, USA},
  author    = {Desrochers, Spencer and Paradis, Chad and Weaver, Vincent M.},
  booktitle = {Proceedings of the Second International Symposium on Memory Systems},
  doi       = {10.1145/2989081.2989088},
  isbn      = {9781450343053},
  keywords  = {DRAM Power, RAPL, DRAM Energy},
  location  = {Alexandria, VA, USA},
  numpages  = {16},
  pages     = {455–470},
  publisher = {Association for Computing Machinery},
  series    = {MEMSYS '16},
  title     = {A Validation of DRAM RAPL Power Measurements},
  url       = {https://doi.org/10.1145/2989081.2989088},
  year      = {2016}
}

@inproceedings{dozat-manning-2017-deep,
  author    = {Timothy Dozat and
               Christopher D. Manning},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/DozatM17.bib},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  timestamp = {Thu, 25 Jul 2019 14:25:56 +0200},
  title     = {Deep Biaffine Attention for Neural Dependency Parsing},
  url       = {https://openreview.net/forum?id=Hk95PK9le},
  year      = {2017}
}

@inproceedings{ehrmann-etal-2016-diachronic,
  abstract  = {In recent years, many cultural institutions have engaged  in large-scale newspaper digitization projects and large  amounts of historical texts are being acquired (via  transcription or OCRization). Beyond document preservation,  the next step consists in providing an enhanced access to  the content of these digital resources. In this regard, the  processing of units which act as referential anchors,  namely named entities (NE), is of particular importance.  Yet, the application of standard NE tools to historical  texts faces several challenges and performances are often  not as good as on contemporary documents. This paper  investigates the performances of different NE recognition  tools applied on old newspapers by conducting a diachronic  evaluation over 7 time-series taken from the archives of  Swiss newspaper Le Temps.},
  address   = {Bochum, Germany},
  author    = {Ehrmann, Maud and Colavizza, Giovanni and Rochat, Yannick  and Kaplan, Frédéric},
  booktitle = {Proceedings of the 13th Conference on Natural Language  Processing (KONVENS 2016)},
  journal   = {Proceedings of the 13th Conference on Natural Language  Processing (KONVENS 2016)},
  pages     = {97-107},
  publisher = {Bochumer Linguistische Arbeitsberichte},
  title     = {Diachronic Evaluation of NER Systems on Old Newspapers},
  url       = {http://infoscience.epfl.ch/record/221391},
  year      = {2016}
}

@inproceedings{ehrmann-etal-2020-extended,
  abstract  = {This paper presents an extended overview of the first  edition of HIPE (Identifying Historical People, Places and  other Entities), a pioneering shared task dedicated to the  evaluation of named entity processing on historical  newspapers in French, German and English. Since its  introduction some twenty years ago, named entity (NE)  processing has become an essential component of virtually  any text mining application and has undergone major  changes. Recently, two main trends characterise its  developments: the adoption of deep learning architectures  and the consideration of textual material originating from  historical and cultural heritage collections. While the  former opens up new opportunities, the latter introduces  new challenges with heterogeneous, historical and noisy  inputs. In this context, the objective of HIPE, run as part  of the CLEF 2020 conference, is threefold: strengthening  the robustness of existing approaches on non-standard  inputs, enabling performance comparison of NE processing on  historical texts, and, in the long run, fostering efficient  semantic indexing of historical documents. Tasks, corpora,  and results of 13 participating teams are presented.  Compared to the condensed overview [31], this paper  includes further details about data generation and  statistics, additional information on participating  systems, and the presentation of complementary results.},
  author    = {Ehrmann, Maud and Romanello, Matteo and Flückiger, Alex  and Clematide, Simon},
  booktitle = {CLEF 2020 Working Notes. Conference and Labs of the  Evaluation Forum},
  doi       = {10.5281/zenodo.4117566},
  journal   = {CLEF 2020 Working Notes. Conference and Labs of the  Evaluation Forum},
  pages     = {38},
  publisher = {CEUR-WS},
  series    = {CEUR Workshop Proceedings. 2696},
  title     = {Extended Overview of CLEF HIPE 2020: Named Entity  Processing on Historical Newspapers},
  url       = {http://infoscience.epfl.ch/record/281054},
  volume    = {2696},
  year      = {2020}
}

@misc{emerick-2018-list,
  author = {Emerick, Jacob},
  title  = {List of dirty naughty obscene and otherwise bad words},
  url    = {https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words},
  year   = {2018}
}

@inproceedings{evans-2003-a,
  author    = {Richard J. Evans},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/ranlp/Evans03.bib},
  booktitle = {Recent Advances in Natural Language Processing III, Selected Papers
               from {RANLP} 2003, Borovets, Bulgaria},
  editor    = {Nicolas Nicolov and
               Kalina Bontcheva and
               Galia Angelova and
               Ruslan Mitkov},
  pages     = {267--276},
  publisher = {John Benjamins, Amsterdam/Philadelphia},
  series    = {Current Issues in Linguistic Theory {(CILT)}},
  timestamp = {Tue, 22 Feb 2005 13:47:43 +0100},
  title     = {A framework for named entity recognition in the open domain},
  volume    = {260},
  year      = {2003}
}

@article{fan-etal-2020-beyond,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv201011125F},
  archiveprefix = {arXiv},
  author        = {{Fan}, Angela and {Bhosale}, Shruti and {Schwenk}, Holger and {Ma}, Zhiyi and {El-Kishky}, Ahmed and {Goyal}, Siddharth and {Baines}, Mandeep and {Celebi}, Onur and {Wenzek}, Guillaume and {Chaudhary}, Vishrav and {Goyal}, Naman and {Birch}, Tom and {Liptchinsky}, Vitaliy and {Edunov}, Sergey and {Grave}, Edouard and {Auli}, Michael and {Joulin}, Armand},
  eid           = {arXiv:2010.11125},
  eprint        = {2010.11125},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  month         = oct,
  pages         = {arXiv:2010.11125},
  primaryclass  = {cs.CL},
  title         = {{Beyond English-Centric Multilingual Machine Translation}},
  year          = 2020
}

 @inproceedings{galliano-etal-2005-the,
  author    = {Sylvain Galliano and Edouard Geoffrois and Djamel Mostefa and Khalid Choukri and Jean-François Bonastre and Guillaume Gravier},
  booktitle = {Proc. Interspeech 2005},
  doi       = {10.21437/Interspeech.2005-441},
  pages     = {1149--1152},
  title     = {{The ESTER phase II evaluation campaign for the rich transcription of French broadcast news}},
  year      = 2005
}

@inproceedings{galliano-etal-2009-the,
  author    = {Sylvain Galliano and Guillaume Gravier and Laura Chaubard},
  booktitle = {Proc. Interspeech 2009},
  doi       = {10.21437/Interspeech.2009-680},
  pages     = {2583--2586},
  title     = {{The ester 2 evaluation campaign for the rich transcription of French radio broadcasts}},
  year      = 2009
}

@article{gao-etal-2020-pile,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2021arXiv210100027G},
  archiveprefix = {arXiv},
  author        = {{Gao}, Leo and {Biderman}, Stella and {Black}, Sid and {Golding}, Laurence and {Hoppe}, Travis and {Foster}, Charles and {Phang}, Jason and {He}, Horace and {Thite}, Anish and {Nabeshima}, Noa and {Presser}, Shawn and {Leahy}, Connor},
  eid           = {arXiv:2101.00027},
  eprint        = {2101.00027},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  month         = dec,
  pages         = {arXiv:2101.00027},
  primaryclass  = {cs.CL},
  title         = {{The Pile: An 800GB Dataset of Diverse Text for Language Modeling}},
  year          = 2020
}

@article{gao-etal-2020-the,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2021arXiv210100027G},
  archiveprefix = {arXiv},
  author        = {{Gao}, Leo and {Biderman}, Stella and {Black}, Sid and {Golding}, Laurence and {Hoppe}, Travis and {Foster}, Charles and {Phang}, Jason and {He}, Horace and {Thite}, Anish and {Nabeshima}, Noa and {Presser}, Shawn and {Leahy}, Connor},
  eid           = {arXiv:2101.00027},
  eprint        = {2101.00027},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  month         = dec,
  pages         = {arXiv:2101.00027},
  primaryclass  = {cs.CL},
  title         = {{The Pile: An 800GB Dataset of Diverse Text for Language Modeling}},
  year          = 2020
}

@article{gebru-etal-2018-datasheets,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180309010G},
  archiveprefix = {arXiv},
  author        = {{Gebru}, Timnit and {Morgenstern}, Jamie and {Vecchione}, Briana and {Wortman Vaughan}, Jennifer and {Wallach}, Hanna and {Daum{\'e}}, Hal, III and {Crawford}, Kate},
  eid           = {arXiv:1803.09010},
  eprint        = {1803.09010},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Databases, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  month         = mar,
  pages         = {arXiv:1803.09010},
  primaryclass  = {cs.DB},
  title         = {{Datasheets for Datasets}},
  year          = 2018
}

@inproceedings{ghannay-etal-2018-end,
  author    = {Ghannay, S. and Caubrière, A. and Estève, Y. and Camelin, N. and Simonnet, E. and Laurent, A. and Morin, E.},
  booktitle = {2018 IEEE Spoken Language Technology Workshop (SLT)},
  doi       = {10.1109/SLT.2018.8639513},
  number    = {},
  pages     = {692-699},
  title     = {End-To-End Named Entity And Semantic Concept Extraction From Speech},
  volume    = {},
  year      = {2018}
}

@article{glessen-2003-elaboration,
  author    = {Gleßgen, Martin-Dietrich},
  booktitle = {Mémoires et documents de l'École des chartes},
  journal   = {Mémoires et documents de l'École des chartes},
  pages     = {371--386},
  title     = {L'élaboration philologique et l'étude lexicologique des Plus anciens documents linguistiques de la {France} à l'aide de l'informatique},
  volume    = {71},
  year      = {2003}
}

@article{goyal-etal-2021-flores-101,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2021arXiv210603193G},
  archiveprefix = {arXiv},
  author        = {{Goyal}, Naman and {Gao}, Cynthia and {Chaudhary}, Vishrav and {Chen}, Peng-Jen and {Wenzek}, Guillaume and {Ju}, Da and {Krishnan}, Sanjana and {Ranzato}, Marc'Aurelio and {Guzman}, Francisco and {Fan}, Angela},
  eid           = {arXiv:2106.03193},
  eprint        = {2106.03193},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  month         = jun,
  pages         = {arXiv:2106.03193},
  primaryclass  = {cs.CL},
  title         = {{The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation}},
  year          = 2021
}

@inproceedings{guibon-etal-2014-parsing,
  address     = {T{\"u}bingen, Germany},
  author      = {Guibon, Ga{\"e}l and Tellier, Isabelle and Constant, Mathieu and Pr{\'e}vost, Sophie and Gerdes, Kim},
  booktitle   = {{Thirteenth International Workshop on Treebanks and Linguistic Theories (TLT13)}},
  editor      = {V. Henrich and E. Hinrichs and D.de Kok and P. Osenova \& A. Przepi{\'o}rkowski},
  hal_id      = {hal-01250959},
  hal_version = {v2},
  keywords    = {POS labelling ; dependency parsing ; Old French ; machine learning ; corpus exploration},
  month       = Dec,
  pages       = {51-61},
  pdf         = {https://hal.archives-ouvertes.fr/hal-01250959v2/file/guibon_al_TLT2014.pdf},
  series      = {Proceedings of the Thirteenth International Workshop on Treebanks and Linguistic Theories (TLT13)},
  title       = {{Parsing Poorly Standardized Language Dependency on Old French}},
  url         = {https://hal.archives-ouvertes.fr/hal-01250959},
  year        = {2014}
}

@article{guillot-etal-2018-base,
  author      = {Guillot, C{\'e}line and Heiden, Serge and Lavrentiev, Alexei},
  hal_id      = {halshs-01809581},
  hal_version = {v1},
  journal     = {{Diachroniques. Revue de Linguistique fran{\c c}aise diachronique}},
  pages       = {168-184},
  pdf         = {https://halshs.archives-ouvertes.fr/halshs-01809581/file/Guillot-et-al_Base-de-francais-medieval-pour%20HAL.pdf},
  publisher   = {{Presses de l'Universit{\'e} Paris-Sorbonne (PUPS)}},
  series      = {Les {\'e}tats anciens des langues {\`a} l'heure du num{\'e}rique},
  title       = {{Base de fran{\c c}ais m{\'e}di{\'e}val~: une base de r{\'e}f{\'e}rence de sources m{\'e}di{\'e}vales ouverte et libre au service de la communaut{\'e} scientifique}},
  url         = {https://halshs.archives-ouvertes.fr/halshs-01809581},
  volume      = {7},
  year        = {2018}
}

@inproceedings{hill-etal-2016-the,
  author    = {Felix Hill and
               Antoine Bordes and
               Sumit Chopra and
               Jason Weston},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/HillBCW15.bib},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  title     = {The Goldilocks Principle: Reading Children's Books with Explicit Memory
               Representations},
  url       = {http://arxiv.org/abs/1511.02301},
  year      = {2016}
}

@article{hochreiter-schmidhuber-1997-long,
  abstract = {{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}},
  author   = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  doi      = {10.1162/neco.1997.9.8.1735},
  eprint   = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
  issn     = {0899-7667},
  journal  = {Neural Computation},
  month    = {11},
  number   = {8},
  pages    = {1735-1780},
  title    = {{Long Short-Term Memory}},
  url      = {https://doi.org/10.1162/neco.1997.9.8.1735},
  volume   = {9},
  year     = {1997}
}

@article{holland-etal-2018-the,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180503677H},
  archiveprefix = {arXiv},
  author        = {{Holland}, Sarah and {Hosny}, Ahmed and {Newman}, Sarah and {Joseph}, Joshua and {Chmielinski}, Kasia},
  eid           = {arXiv:1805.03677},
  eprint        = {1805.03677},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Databases, Computer Science - Computers and Society},
  month         = may,
  pages         = {arXiv:1805.03677},
  primaryclass  = {cs.DB},
  title         = {{The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards}},
  year          = 2018
}

@inproceedings{hu-etal-2020-xtreme,
  abstract  = {Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We will release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.},
  author    = {Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  editor    = {III, Hal Daumé and Singh, Aarti},
  month     = {13--18 Jul},
  pages     = {4411--4421},
  pdf       = {http://proceedings.mlr.press/v119/hu20b/hu20b.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {{XTREME}: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation},
  url       = {https://proceedings.mlr.press/v119/hu20b.html},
  volume    = {119},
  year      = {2020}
}

@article{huang-etal-2015-bidirectional,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2015arXiv150801991H},
  archiveprefix = {arXiv},
  author        = {{Huang}, Zhiheng and {Xu}, Wei and {Yu}, Kai},
  eid           = {arXiv:1508.01991},
  eprint        = {1508.01991},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  month         = aug,
  pages         = {arXiv:1508.01991},
  primaryclass  = {cs.CL},
  title         = {{Bidirectional LSTM-CRF Models for Sequence Tagging}},
  year          = 2015
}

@article{joulin-etal-2016-fasttext,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv161203651J},
  archiveprefix = {arXiv},
  author        = {{Joulin}, Armand and {Grave}, Edouard and {Bojanowski}, Piotr and {Douze}, Matthijs and {J{\'e}gou}, H{\'e}rve and {Mikolov}, Tomas},
  eid           = {arXiv:1612.03651},
  eprint        = {1612.03651},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  month         = dec,
  pages         = {arXiv:1612.03651},
  primaryclass  = {cs.CL},
  title         = {{FastText.zip: Compressing text classification models}},
  year          = 2016
}

@inproceedings{kingma-ba-2015-adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  title     = {Adam: {A} Method for Stochastic Optimization},
  url       = {http://arxiv.org/abs/1412.6980},
  year      = {2015}
}

@inproceedings{koutsikakis-etal-2020-greek,
  address   = {New York, NY, USA},
  author    = {Koutsikakis, John and Chalkidis, Ilias and Malakasiotis, Prodromos and Androutsopoulos, Ion},
  booktitle = {11th Hellenic Conference on Artificial Intelligence},
  doi       = {10.1145/3411408.3411440},
  isbn      = {9781450388788},
  keywords  = {Deep Neural Networks, Transformers, Greek NLP Resources, Natural Language Processing, Pre-trained Language Models},
  location  = {Athens, Greece},
  numpages  = {8},
  pages     = {110–117},
  publisher = {Association for Computing Machinery},
  series    = {SETN 2020},
  title     = {GREEK-BERT: The Greeks Visiting Sesame Street},
  url       = {https://doi.org/10.1145/3411408.3411440},
  year      = {2020}
}

@article{kroch-etal-2000-the,
  author  = {Anthony Kroch and Ann Taylor and Beatrice Santorini},
  editor  = {Department of Linguistics, University of Pennsylvania},
  journal = {CD-ROM, second edition, release 4},
  title   = {The {Penn-Helsinki Parsed Corpus} of {Middle English} ({PPCME2})},
  url     = {http://www.ling.upenn.edu/ppche/ppche-release-2016/PPCME2-RELEASE-4},
  year    = {2000}
}

@book{kunstmann-stein-2007-le,
  address   = {Stuttgart},
  author    = {Kunstmann, Pierre and Stein, Achim},
  isbn      = {978-3-515-08997-5},
  pages     = {1 vol. (200 p.)},
  publisher = {F. Steiner},
  series    = {Zeitschrift für französische Sprache und Literatur Neue Folge 34},
  title     = {Le nouveau corpus d'Amsterdam actes de l'atelier de Lauterbad, 23-26 février 2006},
  url       = {http://www.sudoc.fr/121808459},
  year      = {2007}
}

@inproceedings{lafferty-etal-2001-conditional,
  address   = {San Francisco, CA, USA},
  author    = {Lafferty, John D. and McCallum, Andrew and Pereira, Fernando C. N.},
  booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
  isbn      = {1558607781},
  numpages  = {8},
  pages     = {282–289},
  publisher = {Morgan Kaufmann Publishers Inc.},
  series    = {ICML '01},
  title     = {Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data},
  year      = {2001}
}

@inproceedings{lan-etal-2020-albert,
  author    = {Zhenzhong Lan and
               Mingda Chen and
               Sebastian Goodman and
               Kevin Gimpel and
               Piyush Sharma and
               Radu Soricut},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/LanCGGSS20.bib},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  timestamp = {Thu, 07 May 2020 17:11:48 +0200},
  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
               Representations},
  url       = {https://openreview.net/forum?id=H1eA7AEtvS},
  year      = {2020}
}

@article{leaman-etal-2016-taggerone,
  author  = {Leaman, Robert and Lu, Zhiyong},
  eprint  = {https://academic.oup.com/bioinformatics/article-pdf/32/18/2839/24406872/btw343.pdf},
  issn    = {1367-4803},
  journal = {Bioinformatics},
  month   = {06},
  number  = {18},
  pages   = {2839-2846},
  title   = {{TaggerOne: joint named entity recognition and normalization with semi-Markov Models}},
  volume  = {32},
  year    = {2016}
}

@article{lee-etal-2019-BioBERT,
  abstract = {{Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\\% F1 score improvement), biomedical relation extraction (2.80\\% F1 score improvement) and biomedical question answering (12.24\\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.}},
  author   = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  doi      = {10.1093/bioinformatics/btz682},
  eprint   = {https://academic.oup.com/bioinformatics/article-pdf/36/4/1234/32527770/btz682.pdf},
  issn     = {1367-4803},
  journal  = {Bioinformatics},
  month    = {09},
  number   = {4},
  pages    = {1234-1240},
  title    = {{BioBERT: a pre-trained biomedical language representation model for biomedical text mining}},
  url      = {https://doi.org/10.1093/bioinformatics/btz682},
  volume   = {36},
  year     = {2019}
}

@article{lee-etal-2021-deduplicating,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2021arXiv210706499L},
  archiveprefix = {arXiv},
  author        = {{Lee}, Katherine and {Ippolito}, Daphne and {Nystrom}, Andrew and {Zhang}, Chiyuan and {Eck}, Douglas and {Callison-Burch}, Chris and {Carlini}, Nicholas},
  eid           = {arXiv:2107.06499},
  eprint        = {2107.06499},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  month         = jul,
  pages         = {arXiv:2107.06499},
  primaryclass  = {cs.CL},
  title         = {{Deduplicating Training Data Makes Language Models Better}},
  year          = 2021
}

@article{lee-kong-2014-a,
  abstract = {{We present a dependency treebank of Buddhist Chinese texts, containing more than 50K characters drawn from four sutras in the Chinese Buddhist Canon. With dates of composition that span almost five centuries, these sutras bear witness to the evolution of the Chinese language. The treebank has been annotated using the part-of-speech tagset of the Penn Chinese Treebank, and the Stanford Dependencies for Chinese with slight modifications. The article first discusses the texts and the annotation framework of this treebank, and reports on inter-annotator agreement. It then describes the search platform, to which the treebank has been imported, and applies the treebank to an open question in Chinese historical linguistics—the emergence of the Chinese copula.}},
  author   = {Lee, John and Kong, Yin Hei},
  doi      = {10.1093/llc/fqu048},
  eprint   = {https://academic.oup.com/dsh/article-pdf/31/1/140/21517990/fqu048.pdf},
  issn     = {2055-7671},
  journal  = {Digital Scholarship in the Humanities},
  month    = {09},
  number   = {1},
  pages    = {140-151},
  title    = {{A dependency treebank of Chinese Buddhist texts}},
  url      = {https://doi.org/10.1093/llc/fqu048},
  volume   = {31},
  year     = {2014}
}

@article{liu-etal-2019-roberta,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190711692L},
  archiveprefix = {arXiv},
  author        = {{Liu}, Yinhan and {Ott}, Myle and {Goyal}, Naman and {Du}, Jingfei and {Joshi}, Mandar and {Chen}, Danqi and {Levy}, Omer and {Lewis}, Mike and {Zettlemoyer}, Luke and {Stoyanov}, Veselin},
  eid           = {arXiv:1907.11692},
  eprint        = {1907.11692},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  month         = jul,
  pages         = {arXiv:1907.11692},
  primaryclass  = {cs.CL},
  title         = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
  year          = 2019
}

@book{marchello-nizia-etal-1979-histoire,
  address   = {Paris},
  author    = {Marchello-Nizia, Christiane },
  isbn      = {2-04-001160-9},
  pages     = {1 vol. (378 p.)},
  publisher = {\#0, Bordas},
  series    = {Collection Études},
  title     = {Histoire de la langue française aux XIVe et XVe siècles},
  url       = {http://www.sudoc.fr/000381160},
  year      = {1979}
}

@book{marchello-Nizia-etal-2020-grande,
  author    = {Christiane Marchello-Nizia and Bernard Combettes and Sophie Prévost and Tobias Scheer},
  doi       = {doi:10.1515/9783110348194},
  isbn      = {9783110348194},
  publisher = {De Gruyter Mouton},
  title     = {Grande Grammaire Historique du Français (GGHF)},
  url       = {https://doi.org/10.1515/9783110348194},
  year      = {2020}
}

@article{martineau-2008-un,
  author    = {Martineau, France},
  booktitle = {Corpus},
  doi       = {https://doi.org/10.4000/corpus.1508},
  eprint    = {http://journals.openedition.org/corpus/1508},
  journal   = {Corpus},
  month     = jul,
  title     = {Un corpus pour l’analyse de la variation et du changement linguistique},
  volume    = {7},
  year      = {2008}
}

@misc{may-2019-german,
  author = {Philip May},
  title  = {{German ELMo Model}},
  url    = {https://github.com/t-systems-on-site-services-gmbh/german-elmo-model},
  year   = {2019}
}

@inproceedings{mikolov-etal-2013-distributed,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  url       = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
  volume    = {26},
  year      = {2013}
}

@inproceedings{neudecker-etal-2014-large,
  author    = {Neudecker, Clemens and Wilms, Lotte and Faber, Willem Jan and van Veen, Theo},
  booktitle = {Proc. of IFLA 2014},
  title     = {Large-scale Refinement of Digital Historic Newspapers with Named Entity Recognition},
  year      = {2014}
}

@misc{nivre-etal-2018-universal,
  author    = {Nivre, Joakim and Abrams, Mitchell and Agi{\'c}, {\v Z}eljko and Ahrenberg, Lars and Antonsen, Lene and Aranzabe, Maria Jesus and Arutie, Gashaw and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Augustinus, Liesbeth and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Barbu Mititelu, Verginica and Bauer, John and Bellato, Sandra and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Biagetti, Erica and Bick, Eckhard and Blokland, Rogier and Bobicev, Victoria and B{\"o}rstell, Carl and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Boyd, Adriane and Burchardt, Aljoscha and Candito, Marie and Caron, Bernard and Caron, Gauthier and Cebiro{\u g}lu Eryi{\u g}it, G{\"u}l{\c s}en and Celano, Giuseppe G. A. and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and Chun, Jayeol and Cinkov{\'a}, Silvie and Collomb, Aur{\'e}lie and {\c C}{\"o}ltekin, {\c C}a{\u g}r{\i} and Connor, Miriam and Courtin, Marine and Davidson, Elizabeth and de Marneffe, Marie-Catherine and de Paiva, Valeria and Diaz de Ilarraza, Arantza and Dickerson, Carly and Dirix, Peter and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Dwivedi, Puneet and Eli, Marhaba and Elkahky, Ali and Ephrem, Binyam and Erjavec, Toma{\v z} and Etienne, Aline and Farkas, Rich{\'a}rd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\'a}udia and Gajdo{\v s}ov{\'a}, Katar{\'{\i}}na and Galbraith, Daniel and Garcia, Marcos and G{\"a}rdenfors, Moa and Gerdes, Kim and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\"o}k{\i}rmak, Memduh and Goldberg, Yoav and G{\'o}mez Guinovart, Xavier and Gonz{\'a}les Saavedra, Berta and Grioni,
               Matias and Gr{\=
               u}z{\={\i}}tis, Normunds and Guillaume, Bruno and Guillot-Barbance, C{\'e}line and Habash, Nizar and Haji{\v c}, Jan and Haji{\v c} jr., Jan and H{\`a} M{\~y}, Linh and Han, Na-Rae and Harris, Kim and Haug, Dag and Hladk{\'a}, Barbora and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Hociung, Florinel and Hohle, Petter and Hwang, Jena and Ion, Radu and Irimia, Elena and Jel{\'{\i}}nek, Tom{\'a}{\v s} and Johannsen, Anders and J{\o}rgensen, Fredrik and Ka{\c s}{\i}kara, H{\"u}ner and Kahane, Sylvain and Kanayama, Hiroshi and Kanerva, Jenna and Kayadelen, Tolga and Kettnerov{\'a}, V{\'a}clava and Kirchner, Jesse and Kotsyba, Natalia and Krek, Simon and Kwak, Sookyoung and Laippala, Veronika and Lambertino, Lorenzo and Lando, Tatiana and Larasati, Septina Dian and Lavrentiev, Alexei and Lee, John and L{\^e} H{\`{\^o}}ng, Phương and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Li, Keying and Lim, {KyungTae} and Ljube{\v s}i{\'c}, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and Mare{\v c}ek, David and Marheinecke, Katrin and Mart{\'{\i}}nez Alonso, H{\'e}ctor and Martins, Andr{\'e} and Ma{\v s}ek, Jan and Matsumoto, Yuji and {McDonald}, Ryan and Mendon{\c c}a, Gustavo and Miekka, Niko and Missil{\"a}, Anna and Mititelu, C{\u a}t{\u a}lin and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Shinsuke and Mortensen, Bjartur and Moskalevskyi, Bohdan and Muischnek, Kadri and Murawaki, Yugo and M{\"u}{\"u}risep, Kaili and Nainwani, Pinkey and Navarro Hor{\~n}iacek, Juan Ignacio and Nedoluzhko,
               Anna and Ne{\v s}pore-B{\=e}rzkalne, Gunta and Nguy{\~{\^e}}n Th{\d i}, Lương and Nguy{\~{\^e}}n Th{\d i} Minh, Huy{\`{\^e}}n and Nikolaev, Vitaly and Nitisaroj, Rattima and Nurmi, Hanna and Ojala, Stina and Ol{\'u}{\`o}kun, Ad{\'e}day{\d o}̀ and Omura, Mai and Osenova, Petya and {\"O}stling, Robert and {\O}vrelid, Lilja and Partanen, Niko and Pascual, Elena and Passarotti, Marco and Patejuk, Agnieszka and Peng, Siyao and Perez, Cenel-Augusto and Perrier, Guy and Petrov, Slav and Piitulainen, Jussi and Pitler, Emily and Plank, Barbara and Poibeau, Thierry and Popel, Martin and Pretkalni{\c n}a, Lauma and Pr{\'e}vost, Sophie and Prokopidis, Prokopis and Przepi{\'o}rkowski, Adam and Puolakainen, Tiina and Pyysalo, Sampo and R{\"a}{\"a}bis, Andriela and Rademaker, Alexandre and Ramasamy, Loganathan and Rama, Taraka and Ramisch, Carlos and Ravishankar, Vinit and Real, Livy and Reddy, Siva and Rehm, Georg and Rie{\ss}ler, Michael and Rinaldi, Larissa and Rituma, Laura and Rocha, Luisa and Romanenko, Mykhailo and Rosa, Rudolf and Rovati, Davide and Roșca, Valentin and Rudina, Olga and Sadde, Shoval and Saleh, Shadi and Samard{\v z}i{\'c}, Tanja and Samson, Stephanie and Sanguinetti,
               Manuela and Saul{\={\i}}te, Baiba and Sawanakunanon, Yanin and Schneider, Nathan and Schuster, Sebastian and Seddah, Djam{\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shen, Mo and Shimada, Atsuko and Shohibussirri, Muh and Sichinava, Dmitry and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\'o}, Katalin and {\v S}imkov{\'a}, M{\'a}ria and Simov, Kiril and Smith, Aaron and Soares-Bastos, Isabela and Stella, Antonio and Straka, Milan and Strnadov{\'a}, Jana and Suhr, Alane and Sulubacak, Umut and Sz{\'a}nt{\'o}, Zsolt and Taji, Dima and Takahashi, Yuta and Tanaka, Takaaki and Tellier, Isabelle and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Ure{\v s}ov{\'a}, Zde{\v n}ka and Uria, Larraitz and Uszkoreit, Hans and Vajjala, Sowmya and van Niekerk, Daniel and van Noord, Gertjan and Varga, Viktor and Vincze, Veronika and Wallin, Lars and Washington, Jonathan North and Williams, Seyi and Wir{\'e}n, Mats and Woldemariam, Tsegay and Wong, Tak-sum and Yan, Chunxiao and Yavrumyan, Marat M. and Yu, Zhuoran and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Zeldes, Amir and Zeman, Daniel and Zhang, Manying and Zhu, Hanzhi},
  copyright = {Licence Universal Dependencies v2.2},
  note      = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
  title     = {Universal Dependencies 2.2},
  url       = {http://hdl.handle.net/11234/1-2837},
  year      = {2018}
}

 @inproceedings{nouvel-etal-2014-pattern,
  abstract  = {Many evaluation campaigns have shown that knowledge-based and data-driven approaches remain equally competitive for Named Entity Recognition. Our research team has developed CasEN, a symbolic system based on finite state transducers, which achieved promising results during the Ester2 French-speaking evaluation campaign. Despite these encouraging results, manually extending the coverage of such a hand-crafted system is a difficult task. In this paper, we present a novel approach based on pattern mining for NER and to supplement our system's knowledge base. The system, mXS, exhaustively searches for hierarchical sequential patterns, that aim at detecting Named Entity boundaries. We assess their efficiency by using such patterns in a standalone mode and in combination with our existing system.},
  address   = {Cham},
  author    = {Nouvel, Damien
               and Antoine, Jean-Yves
               and Friburger, Nathalie},
  booktitle = {Human Language Technology Challenges for Computer Science and Linguistics},
  editor    = {Vetulani, Zygmunt
               and Mariani, Joseph},
  isbn      = {978-3-319-08958-4},
  pages     = {226--237},
  publisher = {Springer International Publishing},
  title     = {Pattern Mining for Named Entity Recognition},
  year      = {2014}
}

@article{nozza-etal-2020-what,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200302912N},
  archiveprefix = {arXiv},
  author        = {{Nozza}, Debora and {Bianchi}, Federico and {Hovy}, Dirk},
  eid           = {arXiv:2003.02912},
  eprint        = {2003.02912},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  month         = mar,
  pages         = {arXiv:2003.02912},
  primaryclass  = {cs.CL},
  title         = {{What the [MASK]? Making Sense of Language-Specific BERT Models}},
  year          = 2020
}

@inproceedings{ortiz-suarez-etal-2019-asynchronous,
  abstract  = {Common Crawl is a considerably large, heterogeneous multilingual corpus comprised of crawled documents from the internet, surpassing 20TB of data and distributed as a set of more than 50 thousand plain text files where each contains many documents written in a wide variety of languages. Even though each document has a metadata block associated to it, this data lacks any information about the language in which each document is written, making it extremely difficult to use Common Crawl for monolingual applications. We propose a general, highly parallel, multithreaded pipeline to clean and classify Common Crawl by language; we specifically design it so that it runs efficiently on medium to low resource infrastructures where I/O speeds are the main constraint. We develop the pipeline so that it can be easily reapplied to any kind of heterogeneous corpus and so that it can be parameterised to a wide range of infrastructures. We also distribute a 6.3TB version of Common Crawl, filtered, classified by language, shuffled at line level in order to avoid copyright issues, and ready to be used for NLP applications.},
  address   = {Mannheim},
  author    = {Pedro Javier {Ortiz Su{\'a}rez} and Beno{\^i}t Sagot and Laurent Romary},
  booktitle = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019},
  doi       = {10.14618/ids-pub-9021},
  editor    = {Piotr Bański and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc Kupietz and Harald L{\"u}ngen and Caroline Iliadi},
  language  = {en},
  pages     = {9 -- 16},
  publisher = {Leibniz-Institut f{\"u}r Deutsche Sprache},
  title     = {Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures},
  url       = {http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215},
  year      = {2019}
}

@misc{oxford-2001-the,
  author    = {Oxford Text Archive},
  copyright = {Oxford Text Archive},
  note      = {Oxford Text Archive},
  title     = {The York-Helsinki parsed corpus of Old English poetry ({YCOEP})},
  url       = {http://hdl.handle.net/20.500.12024/2425},
  year      = {2001}
}

@article{parker-etal-2011-english,
  address = {Philadelphia, Tech. Rep.},
  author  = {Parker, Robert and Graff, David and Kong, Junbo and Chen, Ke and Maeda, Kazuaki},
  journal = {Technical report, Technical Report. Linguistic Data Consortium},
  title   = {English gigaword fifth edition, linguistic data consortium},
  year    = {2011}
}

@inbook{passarotti-2019-project,
  author    = {Marco Passarotti},
  booktitle = {Digital Classical Philology: Ancient Greek and Latin in the Digital Revolution},
  doi       = {doi:10.1515/9783110599572-017},
  pages     = {299--320},
  publisher = {De Gruyter Saur},
  title     = {The Project of the Index Thomisticus Treebank},
  url       = {https://doi.org/10.1515/9783110599572-017},
  year      = {2019}
}

@techreport{phillips-etal-2005-tags,
  abstract    = {This document describes the structure, content, construction, and semantics of language tags for use in cases where it is desirable to indicate the language used in an information object. It also describes how to register values for use in language tags and a construct for matching such language tags, including user defined extensions for private interchange. This document replaces RFC 3066 (which replaced RFC 1766).},
  author      = {Addison Phillips and Mark Davis},
  day         = 15,
  institution = {Internet Engineering Task Force},
  month       = feb,
  note        = {Work in Progress},
  number      = {draft-phillips-langtags-10},
  pagetotal   = 45,
  publisher   = {Internet Engineering Task Force},
  title       = {{Tags for Identifying Languages}},
  type        = {Internet-Draft},
  url         = {https://datatracker.ietf.org/doc/html/draft-phillips-langtags-10},
  year        = 2005
}

@book{prevost-stein-2013-syntactic,
  address   = {Lyon/Stuttgart},
  edition   = {UPDATE VERSION NUMBER},
  editor    = {Pr\'evost, Sophie and Stein, Achim},
  islrn     = {899-492-963-833-3},
  keywords  = {korp-x, srcmf, mypubs-elecedit},
  publisher = {ENS de Lyon; Lattice, Paris; ILR University of Stuttgart},
  title     = {Syntactic Reference Corpus of Medieval French (SRCMF)},
  url       = {http://srcmf.org},
  year      = {2013}
}

@article{radford-etal-2018-improving,
  author  = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal = {OpenAI Blog},
  title   = {Improving language understanding by generative pre-training},
  year    = {2018}
}

@article{radford-etal-2019-language,
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal = {OpenAI Blog},
  pages   = {8},
  title   = {Language models are unsupervised multitask learners},
  volume  = {1},
  year    = {2019}
}

@article{raffel-etal-2020-exploring,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal = {Journal of Machine Learning Research},
  number  = {140},
  pages   = {1-67},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  url     = {http://jmlr.org/papers/v21/20-074.html},
  volume  = {21},
  year    = {2020}
}

@book{reenen-etal-2007-chartes,
  address   = {Orléans},
  author    = {Reenen, Pieter van and Mulken, Margot van and Wattel, Evert},
  isbn      = {978-2-86878-257-1},
  pages     = {1 vol. (XII-283 p.)},
  publisher = {Paradigme, Impr. Sagim-Canale},
  series    = {Medievalia 61},
  title     = {Chartes de Champagne en français conservées aux Archives de l'Aube, 1270-1300},
  url       = {http://www.sudoc.fr/113016220},
  year      = {2007}
}

@inbook{rocio-etal-2003-automated,
  abstract  = {The growing trend towards corpus-based linguistics has led researchers to manually annotate large quantities of text. The human effort involved in this task is often enormous, and requires highly specialised linguistically trained manpower. According to our point of view, another approach should be followed, using this highly trained manpower in other activities, more rewarding and creative, in a constructive dialogue among the various kinds of expertise needed for overcoming our ignorance about languages. As an experiment, we used tools and linguistic resources previously built for Contemporary Portuguese for partially automating the process of partial annotation of a Medieval Portuguese corpus. In this paper, we describe the tools used (POS tagger, lexical analyser and partial parser) and demonstrate that the similarities between a language at two different time periods is sufficient for bootstrapping and acquiring lexical knowledge from the partially parsed, automatically annotated corpus.},
  address   = {Dordrecht},
  author    = {Rocio, Vitor
               and Alves, M{\'a}rio Amado
               and Lopes, J. Gabriel
               and Xavier, Maria Francisca
               and Vicente, Gra{\c{c}}a},
  booktitle = {Treebanks: Building and Using Parsed Corpora},
  doi       = {10.1007/978-94-010-0201-1_12},
  isbn      = {978-94-010-0201-1},
  pages     = {211--227},
  publisher = {Springer Netherlands},
  title     = {Automated Creation of a Medieval Portuguese Partial Treebank},
  url       = {https://doi.org/10.1007/978-94-010-0201-1_12},
  year      = {2003}
}

@article{rosset-etal-2005-interaction,
  author  = {Rosset, Sophie and Illouz, Gabriel and Max, Aur\'{e}lien},
  journal = {Traitement Automatique des Langues},
  number  = {3},
  pages   = {155--179},
  title   = {{I}nteraction et recherche d'information~: le projet {Ritel}},
  volume  = {46},
  year    = {2005}
}

@techreport{rosset-etal-2011-entites,
  author                   = {Rosset, Sophie and Grouin, Cyril and Zweigenbaum, Pierre},
  edition                  = {Notes et Documents LIMSI N° : 2011-04},
  groups                   = {Communications sans acte},
  institution              = {LIMSI-CNRS},
  keywords                 = {named entity definition},
  note                     = {autres},
  organization             = {LIMSI-CNRS},
  owner                    = {rosset},
  timestamp                = {2011.12.02},
  title                    = {Entit\'es Nomm\'ees Structur\'ees : guide d'annotation Quaero},
  x-international-audience = {#no#},
  year                     = {2011}
}

@book{rothwell-etal-2005-anglo,
  address   = {London},
  author    = {Rothwell, William and Gregory, Stewart and Trotter, David},
  edition   = {2nd edition revised and enlarged},
  isbn      = {1-904350-39-9},
  pages     = {2 vol. (XLIX -1107 p.)},
  publisher = {Maney Publishing for the modern humanities research Association},
  series    = {Publications of the modern humanities research Association vol. 17},
  title     = {Anglo-Norman dictionary},
  url       = {http://www.sudoc.fr/108951375},
  year      = {2005}
}

 @inbook{sanguinetti-Bosco-2015-parttut,
  abstract  = {In this paper, we introduce an ongoing project for the development of a parallel treebank for Italian, English and French. The treebank is annotated in a dependency format, namely the one designed in the Turin University Treebank (TUT), hence the choice to call such new resource Par(allel)TUT. The project aims at creating a resource which can be useful in particular for translation research. Therefore, beyond constantly enriching the treebank with new and heterogeneous data, so as to build a dynamic and balanced multilingual treebank, the current stage of the project is devoted to the design of a tool for the alignment of data, which takes into account syntactic knowledge as annotated in this kind of resource. The paper focuses in particular on the study of translational divergences and their implications for the development of the alignment tool. The paper provides an overview of the treebank, with its current content and the peculiarities of the annotation format, the description of the classes of translational divergences which could be encountered in the treebank, together with a proposal for their alignment.},
  address   = {Cham},
  author    = {Sanguinetti, Manuela
               and Bosco, Cristina},
  booktitle = {Harmonization and Development of Resources and Tools for Italian Natural Language Processing within the PARLI Project},
  doi       = {10.1007/978-3-319-14206-7_3},
  isbn      = {978-3-319-14206-7},
  pages     = {51--69},
  publisher = {Springer International Publishing},
  title     = {PartTUT: The Turin University Parallel Treebank},
  url       = {https://doi.org/10.1007/978-3-319-14206-7_3},
  year      = {2015}
}

@inproceedings{schuster-nakajima-2012-japanese,
  author    = {Schuster, Mike and Nakajima, Kaisuke},
  booktitle = {2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  doi       = {10.1109/ICASSP.2012.6289079},
  number    = {},
  pages     = {5149-5152},
  title     = {Japanese and Korean voice search},
  volume    = {},
  year      = {2012}
}

@article{schwartz-etal-2020-green,
  abstract   = {Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.},
  address    = {New York, NY, USA},
  author     = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
  doi        = {10.1145/3381831},
  issn       = {0001-0782},
  issue_date = {December 2020},
  journal    = {Commun. ACM},
  month      = {nov},
  number     = {12},
  numpages   = {10},
  pages      = {54–63},
  publisher  = {Association for Computing Machinery},
  title      = {Green AI},
  url        = {https://doi.org/10.1145/3381831},
  volume     = {63},
  year       = {2020}
}

@article{seker-etal-2021-alephbert,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2021arXiv210404052S},
  archiveprefix = {arXiv},
  author        = {{Seker}, Amit and {Bandel}, Elron and {Bareket}, Dan and {Brusilovsky}, Idan and {Shaked Greenfeld}, Refael and {Tsarfaty}, Reut},
  eid           = {arXiv:2104.04052},
  eprint        = {2104.04052},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  month         = apr,
  pages         = {arXiv:2104.04052},
  primaryclass  = {cs.CL},
  title         = {{AlephBERT:A Hebrew Large Pre-Trained Language Model to Start-off your Hebrew NLP Application With}},
  year          = 2021
}

@article{skitka-etal-1999-does,
  abstract = {Computerized system monitors and decision aids are increasingly common additions to critical decision-making contexts such as intensive care units, nuclear power plants and aircraft cockpits. These aids are introduced with the ubiquitous goal of “reducing human error”. The present study compared error rates in a simulated flight task with and without a computer that monitored system states and made decision recommendations. Participants in non-automated settings out-performed their counterparts with a very but not perfectly reliable automated aid on a monitoring task. Participants with an aid made errors of omission (missed events when not explicitly prompted about them by the aid) and commission (did what an automated aid recommended, even when it contradicted their training and other 100% valid and available indicators). Possible causes and consequences of automation bias are discussed},
  author   = {Linda J. Skitka And Kathleen L. Mosier And Mark Burdick},
  doi      = {https://doi.org/10.1006/ijhc.1999.0252},
  issn     = {1071-5819},
  journal  = {International Journal of Human-Computer Studies},
  number   = {5},
  pages    = {991-1006},
  title    = {Does automation bias decision-making?},
  url      = {https://www.sciencedirect.com/science/article/pii/S1071581999902525},
  volume   = {51},
  year     = {1999}
}

@phdthesis{stern-2013-identification,
  author      = {Stern, Rosa},
  hal_id      = {tel-00939420},
  hal_version = {v1},
  keywords    = {information extraction ; semantic web ; semantic annotation ; entities ; named entitiy recognition ; linking ; extraction d'information ; web s{\'e}mantique ; annotation s{\'e}mantique ; linked data ; entit{\'e}s ; reconnaissance d'entit{\'e}s nomm{\'e}es ; liage},
  month       = Jun,
  pdf         = {https://tel.archives-ouvertes.fr/tel-00939420/file/these.pdf},
  school      = {{Universit{\'e} Paris-Diderot - Paris VII}},
  title       = {{Identification automatique d'entit{\'e}s pour l'enrichissement de contenus textuels}},
  type        = {Theses},
  url         = {https://tel.archives-ouvertes.fr/tel-00939420},
  year        = {2013}
}

@inproceedings{stern-sagot-2010-resources,
  address     = {Valletta, Malta},
  author      = {Stern, Rosa and Sagot, Beno{\^i}t},
  booktitle   = {{Entity 2010 Workshop at LREC 2010}},
  hal_id      = {inria-00521240},
  hal_version = {v1},
  month       = May,
  pdf         = {https://hal.inria.fr/inria-00521240/file/entity10np.pdf},
  title       = {{Resources for Named Entity Recognition and Resolution in News Wires}},
  url         = {https://hal.inria.fr/inria-00521240},
  year        = {2010}
}

@article{straka-strakova-2019-evaluating,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190807448S},
  archiveprefix = {arXiv},
  author        = {{Straka}, Milan and {Strakov{\'a}}, Jana and {Haji{\v{c}}}, Jan},
  eid           = {arXiv:1908.07448},
  eprint        = {1908.07448},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  month         = aug,
  pages         = {arXiv:1908.07448},
  primaryclass  = {cs.CL},
  title         = {{Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing}},
  year          = 2019
}

@inproceedings{sun-etal-2019-mithralabel,
  abstract  = {Using inappropriate datasets for data science tasks can be harmful, especially for
               applications that impact humans. Targeting data ethics, we demonstrate MithraLabel,
               a system for generating task-specific information about a dataset, in the form of
               a set of visual widgets, as a flexible "nutritional label" that provides a user with
               information to determine the fitness of the dataset for the task at hand.},
  address   = {New York, NY, USA},
  author    = {Sun, Chenkai and Asudeh, Abolfazl and Jagadish, H. V. and Howe, Bill and Stoyanovich, Julia},
  booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  doi       = {10.1145/3357384.3357853},
  isbn      = {9781450369763},
  keywords  = {fairness, machine bias, data ethics, accountability, transparency},
  location  = {Beijing, China},
  numpages  = {4},
  pages     = {2893–2896},
  publisher = {Association for Computing Machinery},
  series    = {CIKM '19},
  title     = {MithraLabel: Flexible Dataset Nutritional Labels for Responsible Data Science},
  url       = {https://doi.org/10.1145/3357384.3357853},
  year      = {2019}
}

@article{taylor-1953-cloze,
  abstract = {Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which “cloze procedure” results are compared with those of two readability formulas.},
  author   = {Wilson L. Taylor},
  doi      = {10.1177/107769905303000401},
  eprint   = {https://doi.org/10.1177/107769905303000401},
  journal  = {Journalism Quarterly},
  number   = {4},
  pages    = {415-433},
  title    = {“Cloze Procedure”: A New Tool for Measuring Readability},
  url      = {https://doi.org/10.1177/107769905303000401},
  volume   = {30},
  year     = {1953}
}

  @inbook{traugott-pintzuk-2008-coding,
  author    = {Elizabeth Closs Traugott and Susan Pintzuk},
  booktitle = {Studies in the History of the English Language IV: Empirical and Analytical Advances in the Study of English Language Change},
  doi       = {doi:10.1515/9783110211801.61},
  pages     = {61--80},
  publisher = {De Gruyter Mouton},
  title     = {Coding the York-Toronto-Helsinki Parsed Corpus of
               Old English Prose to investigate the syntaxpragmatics interface},
  url       = {https://doi.org/10.1515/9783110211801.61},
  year      = {2008}
}

@article{trinh-le-2018-a,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180602847T},
  archiveprefix = {arXiv},
  author        = {{Trinh}, Trieu H. and {Le}, Quoc V.},
  eid           = {arXiv:1806.02847},
  eprint        = {1806.02847},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  month         = jun,
  pages         = {arXiv:1806.02847},
  primaryclass  = {cs.AI},
  title         = {{A Simple Method for Commonsense Reasoning}},
  year          = 2018
}

@article{turc-etal-2019-well,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190808962T},
  archiveprefix = {arXiv},
  author        = {{Turc}, Iulia and {Chang}, Ming-Wei and {Lee}, Kenton and {Toutanova}, Kristina},
  eid           = {arXiv:1908.08962},
  eprint        = {1908.08962},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  month         = aug,
  pages         = {arXiv:1908.08962},
  primaryclass  = {cs.CL},
  title         = {{Well-Read Students Learn Better: On the Importance of Pre-training Compact Models}},
  year          = 2019
}

@inproceedings{vaswani-etal-2017-attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@article{virtanen-etal-2019-multilingual,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv191207076V},
  archiveprefix = {arXiv},
  author        = {{Virtanen}, Antti and {Kanerva}, Jenna and {Ilo}, Rami and {Luoma}, Jouni and {Luotolahti}, Juhani and {Salakoski}, Tapio and {Ginter}, Filip and {Pyysalo}, Sampo},
  eid           = {arXiv:1912.07076},
  eprint        = {1912.07076},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  month         = dec,
  pages         = {arXiv:1912.07076},
  primaryclass  = {cs.CL},
  title         = {{Multilingual is not enough: BERT for Finnish}},
  year          = 2019
}

@article{wang-cho-etal-2020-neural,
  author  = {Wang, Changhan and Cho, Kyunghyun and Gu, Jiatao},
  doi     = {10.1609/aaai.v34i05.6451},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  month   = {Apr.},
  number  = {05},
  pages   = {9154-9160},
  title   = {Neural Machine Translation with Byte-Level Subwords},
  url     = {https://ojs.aaai.org/index.php/AAAI/article/view/6451},
  volume  = {34},
  year    = {2020}
}

@article{wolf-etal-2019-huggingface,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv191003771W},
  archiveprefix = {arXiv},
  author        = {{Wolf}, Thomas and {Debut}, Lysandre and {Sanh}, Victor and {Chaumond}, Julien and {Delangue}, Clement and {Moi}, Anthony and {Cistac}, Pierric and {Rault}, Tim and {Louf}, R{\'e}mi and {Funtowicz}, Morgan and {Davison}, Joe and {Shleifer}, Sam and {von Platen}, Patrick and {Ma}, Clara and {Jernite}, Yacine and {Plu}, Julien and {Xu}, Canwen and {Le Scao}, Teven and {Gugger}, Sylvain and {Drame}, Mariama and {Lhoest}, Quentin and {Rush}, Alexander M.},
  eid           = {arXiv:1910.03771},
  eprint        = {1910.03771},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  month         = oct,
  pages         = {arXiv:1910.03771},
  primaryclass  = {cs.CL},
  title         = {{HuggingFace's Transformers: State-of-the-art Natural Language Processing}},
  year          = 2019
}

@misc{wrisley-2018-the,
  author   = {David Wrisley},
  language = {English (US)},
  title    = {The Open Medieval French Initiative (OpenMedFr)},
  type     = {Other},
  year     = {2018}
}

@article{xiong-etal-2021-nystromformer,
  abstractnote = {Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences - a topic being actively studied in the community. To address this limitation, we propose Nyströmformer - a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nyström method to approximate standard self-attention with O(n) complexity. The scalability of Nyströmformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nyströmformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nyströmformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github.com/mlpen/Nystromformer.},
  author       = {Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  month        = {May},
  number       = {16},
  pages        = {14138-14148},
  title        = {Nyströmformer: A Nyström-based Algorithm for Approximating Self-Attention},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17664},
  volume       = {35},
  year         = {2021}
}

@inproceedings{yang-etal-2019-xlnet,
  author    = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  url       = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@inproceedings{zaheer-etal-2020-big,
  author    = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {17283--17297},
  publisher = {Curran Associates, Inc.},
  title     = {Big Bird: Transformers for Longer Sequences},
  url       = {https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@misc{zeman-etal-2020-universal,
  author    = {Zeman, Daniel and Nivre, Joakim and Abrams, Mitchell and Ackermann, Elia and Aepli, No{\"e}mi and Aghaei, Hamid and Agi{\'c}, {\v Z}eljko and Ahmadi, Amir and Ahrenberg, Lars and Ajede,
               Chika Kennedy and Aleksandravi{\v c}i{\=u}t{\.e}, Gabriel{\.e} and Alfina, Ika and Antonsen, Lene and Aplonova, Katya and Aquino, Angelina and Aragon, Carolina and Aranzabe, Maria Jesus and Arnard{\'o}ttir, {\TH}{\'o}runn and Arutie, Gashaw and Arwidarasti, Jessica Naraiswari and Asahara, Masayuki and Ateyah, Luma and Atmaca, Furkan and Attia, Mohammed and Atutxa, Aitziber and Augustinus, Liesbeth and Badmaeva, Elena and Balasubramani, Keerthana and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Barbu Mititelu, Verginica and Basmov, Victoria and Batchelor, Colin and Bauer, John and Bedir, Seyyit Talha and Bengoetxea, Kepa and Berk, G{\"o}zde and Berzak, Yevgeni and Bhat, Irshad Ahmad and Bhat, Riyaz Ahmad and Biagetti, Erica and Bick, Eckhard and Bielinskien{\.e}, Agn{\.e} and Bjarnad{\'o}ttir, Krist{\'{\i}}n and Blokland, Rogier and Bobicev, Victoria and Boizou, Lo{\"{\i}}c and Borges V{\"o}lker, Emanuel and B{\"o}rstell, Carl and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Boyd, Adriane and Brokait{\.e}, Kristina and Burchardt, Aljoscha and Candito, Marie and Caron, Bernard and Caron, Gauthier and Cavalcanti, Tatiana and Cebiro{\u g}lu Eryi{\u g}it, G{\"u}l{\c s}en and Cecchini, Flavio Massimiliano and Celano, Giuseppe G. A. and {\v C}{\'e}pl{\"o}, Slavom{\'{\i}}r and Cetin, Savas and {\c C}etino{\u g}lu, {\"O}zlem and Chalub, Fabricio and Chi, Ethan and Cho, Yongseok and Choi, Jinho and Chun, Jayeol and Cignarella, Alessandra T. and Cinkov{\'a}, Silvie and Collomb, Aur{\'e}lie and {\c C}{\"o}ltekin, {\c C}a{\u g}r{\i} and Connor, Miriam and Courtin, Marine and Davidson, Elizabeth and de Marneffe, Marie-Catherine and de Paiva, Valeria and Derin, Mehmet Oguz and de Souza, Elvis and Diaz de Ilarraza, Arantza and Dickerson, Carly and Dinakaramani, Arawinda and Dione, Bamba and Dirix, Peter and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Dwivedi, Puneet and Eckhoff, Hanne and Eli, Marhaba and Elkahky, Ali and Ephrem, Binyam and Erina, Olga and Erjavec, Toma{\v z} and Etienne, Aline and Evelyn, Wograine and Facundes, Sidney and Farkas, Rich{\'a}rd and Fernanda, Mar{\'{\i}}lia and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\'a}udia and Fujita, Kazunori and Gajdo{\v s}ov{\'a}, Katar{\'{\i}}na and Galbraith, Daniel and Garcia, Marcos and G{\"a}rdenfors, Moa and Garza, Sebastian and Gerardi, Fabr{\'{\i}}cio Ferraz and Gerdes, Kim and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\"o}k{\i}rmak, Memduh and Goldberg, Yoav and G{\'o}mez Guinovart, Xavier and Gonz{\'a}lez Saavedra,
               Berta and Grici{\=u}t{\.e}, Bernadeta and Grioni, Matias and Grobol,
               Lo{\"{\i}}c and Gr{\=
               u}z{\={\i}}tis, Normunds and Guillaume, Bruno and Guillot-Barbance, C{\'e}line and G{\"u}ng{\"o}r, Tunga and Habash, Nizar and Hafsteinsson, Hinrik and Haji{\v c}, Jan and Haji{\v c} jr., Jan and H{\"a}m{\"a}l{\"a}inen, Mika and H{\`a} M{\~y}, Linh and Han, Na-Rae and Hanifmuti, Muhammad Yudistira and Hardwick, Sam and Harris, Kim and Haug, Dag and Heinecke, Johannes and Hellwig, Oliver and Hennig, Felix and Hladk{\'a}, Barbora and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Hociung, Florinel and Hohle, Petter and Huber, Eva and Hwang, Jena and Ikeda, Takumi and Ingason, Anton Karl and Ion, Radu and Irimia, Elena and Ishola, {\d O}l{\'a}j{\'{\i}}d{\'e} and Jel{\'{\i}}nek, Tom{\'a}{\v s} and Johannsen, Anders and J{\'o}nsd{\'o}ttir, Hildur and J{\o}rgensen, Fredrik and Juutinen, Markus and K, Sarveswaran and Ka{\c s}{\i}kara, H{\"u}ner and Kaasen, Andre and Kabaeva, Nadezhda and Kahane, Sylvain and Kanayama, Hiroshi and Kanerva, Jenna and Katz, Boris and Kayadelen, Tolga and Kenney, Jessica and Kettnerov{\'a}, V{\'a}clava and Kirchner, Jesse and Klementieva, Elena and K{\"o}hn, Arne and K{\"o}ksal, Abdullatif and Kopacewicz, Kamil and Korkiakangas, Timo and Kotsyba, Natalia and Kovalevskait{\.e}, Jolanta and Krek, Simon and Krishnamurthy, Parameswari and Kwak, Sookyoung and Laippala, Veronika and Lam, Lucia and Lambertino, Lorenzo and Lando, Tatiana and Larasati, Septina Dian and Lavrentiev, Alexei and Lee, John and L{\^e} H{\`{\^o}}ng, Phương and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Levina, Maria and Li, Cheuk Ying and Li, Josie and Li, Keying and Li, Yuan and Lim, {KyungTae} and Lind{\'e}n, Krister and Ljube{\v s}i{\'c}, Nikola and Loginova, Olga and Luthfi, Andry and Luukko, Mikko and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and Mare{\v c}ek, David and Marheinecke, Katrin and Mart{\'{\i}}nez Alonso, H{\'e}ctor and Martins, Andr{\'e} and Ma{\v s}ek, Jan and Matsuda, Hiroshi and Matsumoto, Yuji and {McDonald}, Ryan and {McGuinness}, Sarah and Mendon{\c c}a, Gustavo and Miekka, Niko and Mischenkova, Karina and Misirpashayeva, Margarita and Missil{\"a}, Anna and Mititelu, C{\u a}t{\u a}lin and Mitrofan, Maria and Miyao, Yusuke and Mojiri Foroushani, {AmirHossein} and Moloodi, Amirsaeid and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Keiko Sophie and Mori, Shinsuke and Morioka, Tomohiko and Moro, Shigeki and Mortensen, Bjartur and Moskalevskyi, Bohdan and Muischnek, Kadri and Munro, Robert and Murawaki, Yugo and M{\"u}{\"u}risep, Kaili and Nainwani, Pinkey and Nakhl{\'e}, Mariam and Navarro Hor{\~n}iacek, Juan Ignacio and Nedoluzhko,
               Anna and Ne{\v s}pore-B{\=e}rzkalne, Gunta and Nguy{\~{\^e}}n Th{\d i}, Lương and Nguy{\~{\^e}}n Th{\d i} Minh, Huy{\`{\^e}}n and Nikaido, Yoshihiro and Nikolaev, Vitaly and Nitisaroj, Rattima and Nourian, Alireza and Nurmi, Hanna and Ojala, Stina and Ojha, Atul Kr. and Ol{\'u}{\`o}kun, Ad{\'e}day{\d o}̀ and Omura, Mai and Onwuegbuzia, Emeka and Osenova, Petya and {\"O}stling, Robert and {\O}vrelid, Lilja and {\"O}zate{\c s}, {\c S}aziye Bet{\"u}l and {\"O}zg{\"u}r, Arzucan and {\"O}zt{\"u}rk Ba{\c s}aran, Balk{\i}z and Partanen, Niko and Pascual, Elena and Passarotti, Marco and Patejuk, Agnieszka and Paulino-Passos, Guilherme and Peljak-{\L}api{\'n}ska, Angelika and Peng, Siyao and Perez, Cenel-Augusto and Perkova, Natalia and Perrier, Guy and Petrov, Slav and Petrova, Daria and Phelan, Jason and Piitulainen, Jussi and Pirinen, Tommi A and Pitler, Emily and Plank, Barbara and Poibeau, Thierry and Ponomareva, Larisa and Popel, Martin and Pretkalni{\c n}a, Lauma and Pr{\'e}vost, Sophie and Prokopidis, Prokopis and Przepi{\'o}rkowski, Adam and Puolakainen, Tiina and Pyysalo, Sampo and Qi, Peng and R{\"a}{\"a}bis, Andriela and Rademaker, Alexandre and Rama, Taraka and Ramasamy, Loganathan and Ramisch, Carlos and Rashel, Fam and Rasooli, Mohammad Sadegh and Ravishankar, Vinit and Real, Livy and Rebeja, Petru and Reddy, Siva and Rehm, Georg and Riabov, Ivan and Rie{\ss}ler, Michael and Rimkut{\.e}, Erika and Rinaldi, Larissa and Rituma, Laura and Rocha, Luisa and R{\"o}gnvaldsson, Eir{\'{\i}}kur and Romanenko, Mykhailo and Rosa, Rudolf and Roșca, Valentin and Rovati, Davide and Rudina, Olga and Rueter, Jack and R{\'u}narsson, Kristj{\'a}n and Sadde, Shoval and Safari, Pegah and Sagot, Beno{\^{\i}}t and Sahala, Aleksi and Saleh, Shadi and Salomoni, Alessio and Samard{\v z}i{\'c}, Tanja and Samson, Stephanie and Sanguinetti, Manuela and S{\"a}rg,
               Dage and Saul{\={\i}}te, Baiba and Sawanakunanon, Yanin and Scannell, Kevin and Scarlata, Salvatore and Schneider, Nathan and Schuster, Sebastian and Seddah, Djam{\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shen, Mo and Shimada, Atsuko and Shirasu, Hiroyuki and Shohibussirri, Muh and Sichinava, Dmitry and Sigurðsson, Einar Freyr and Silveira, Aline and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\'o}, Katalin and {\v S}imkov{\'a}, M{\'a}ria and Simov, Kiril and Skachedubova, Maria and Smith, Aaron and Soares-Bastos, Isabela and Spadine, Carolyn and Steingr{\'{\i}}msson, Stein{\th}{\'o}r and Stella, Antonio and Straka, Milan and Strickland, Emmett and Strnadov{\'a}, Jana and Suhr, Alane and Sulestio, Yogi Lesmana and Sulubacak, Umut and Suzuki, Shingo and Sz{\'a}nt{\'o}, Zsolt and Taji, Dima and Takahashi, Yuta and Tamburini, Fabio and Tan, Mary Ann C. and Tanaka, Takaaki and Tella, Samson and Tellier, Isabelle and Thomas, Guillaume and Torga, Liisi and Toska, Marsida and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and T{\"u}rk, Utku and Tyers, Francis and Uematsu, Sumire and Untilov, Roman and Ure{\v s}ov{\'a}, Zde{\v n}ka and Uria, Larraitz and Uszkoreit, Hans and Utka, Andrius and Vajjala, Sowmya and van Niekerk, Daniel and van Noord, Gertjan and Varga, Viktor and Villemonte de la Clergerie, Eric and Vincze, Veronika and Wakasa, Aya and Wallenberg, Joel C. and Wallin, Lars and Walsh, Abigail and Wang, Jing Xian and Washington, Jonathan North and Wendt, Maximilan and Widmer, Paul and Williams, Seyi and Wir{\'e}n, Mats and Wittern, Christian and Woldemariam, Tsegay and Wong, Tak-sum and Wr{\'o}blewska, Alina and Yako, Mary and Yamashita, Kayo and Yamazaki, Naoki and Yan, Chunxiao and Yasuoka, Koichi and Yavrumyan, Marat M. and Yu, Zhuoran and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Zahra, Shorouq and Zeldes, Amir and Zhu, Hanzhi and Zhuravleva, Anna},
  copyright = {Licence Universal Dependencies v2.7},
  note      = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
  title     = {Universal Dependencies 2.7},
  url       = {http://hdl.handle.net/11234/1-3424},
  year      = {2020}
}

@inproceedings{zhu-etal-2015-aligning,
  abstract  = {Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.},
  author    = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  doi       = {10.1109/ICCV.2015.11},
  issn      = {2380-7504},
  keywords  = {},
  month     = {Dec},
  number    = {},
  pages     = {19-27},
  title     = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
  volume    = {},
  year      = {2015}
}
