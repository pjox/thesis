\section{Introduction}\label{sect:Intro}

The question of quality versus size of training corpora is increasingly gaining attention and interest in the context of the latest developments in neural language models' performance.
The longstanding issue of corpora "representativeness" is here addressed, in order to grasp to what extent a linguistically balanced cross-genre language sample is sufficient for a language model to gain in accuracy for contextualized word-embeddings on different NLP tasks.

Several increasingly larger corpora are nowadays compiled from the web, i.e. frWAC \citep{baroni-etal-2009-the}, CCNet \citep{wenzek-etal-2020-ccnet} and OSCAR-fr \citep{ortiz-suarez-etal-2019-asynchronous}. However, does large size necessarily go along with better performance for language model training? Their alleged lack of representativeness has called for inventive ways of building a French balanced corpus offering new insights into language variation and NLP.

Following Biber's definition, “representativeness refers to the extent to which a sample includes the full range of variability in a population” \citep{biber-1993-representativeness}. We adopt a balanced approach by sampling a wide spectrum of language use and its cross-genre variability, be it situational (e.g.\ format, author, addressee, purposes, settings or topics) or linguistic, e.g.\ linked to distributional parameters like frequencies of word classes and genres.
In this way, we developed two newly built corpora. The French Balanced Reference Corpus - \textit{CaBeRnet} - includes a wide-ranging and balanced coverage of cross-genre language use to be maximally representative of French language and therefore yield good generalizations from. The second corpus, the \textit{French Children Book Test} (CBT-fr), includes both narrative material and oral language use as present in youth literature, and will be used for domain-specific language model training. Both are inspired by existing American and English corpora, respectively  COCA, the balanced Corpus of Contemporary American English \citep{davies-2009-the, davies-2010-the}, and the Children Book Test \citep[CBT]{hill-etal-2016-the}.

%better performing word-embeddings
%evaluate the quality of word embedding generated by pre-training 
The second main contribution of this paper lies in the evaluation of the quality of the word-embeddings obtained by pre-training and fine-tuning on different corpora, that are made here publicly available.
Based on the underlying assumption that a linguistically representative corpus would possibly generate better word-embeddings. %being more representative of real language use, would tentatively preform better in downstream tasks. 
We provide an evaluation-based investigation of how a balanced cross-genre corpus can yield improvements in the performance of neural language models like ELMo \citep{peters-etal-2018-deep} on various downstream tasks.
The two corpora, \Cabernet and CBT-fr, and the ELMos will be distributed freely under Creative Commons License.

Specifically, we want to investigate the contribution of oral language use as present in different corpora. Through a series of comparisons, we contrast a more domain-specific and written corpus like Wikipedia-fr with the newly built domain-specific CBT-fr corpus which additionally features oral style dialogues, like the ones one can find in youth literature. To test for the effect of corpus size, we further compare a wide ranging corpora characterized by a variety of linguistic phenomena crawled from internet, like OSCAR \citep{ortiz-suarez-etal-2019-asynchronous}, with our newly built French Balanced Reference Corpus \Cabernet.
Our aim is assess the benefits that can be gained from a balanced, multi-domain corpus such as \Cabernet, despite its being 34 times smaller than the web-based OSCAR.

%Methodologically, our approach constitutes an original proof-of-concept that fine-tuning with resources that are up to 35 times smaller than pre-training corpora has a observable impact on classical NLP tasks scores. Secondly, we show that pre-training a language model on a very small sample like the French Children Book Test corpus yields unexpected positive results.
%\iffalse
%Resources associated to this paper encompass\footnote{The link to the corpus and FrElMos will be available upon acceptance of the paper. Following the link the reader will have access to a dedicated website \textit{cabernet-corpus.fr} where raw text version and metadata for each sub-part of the corpus are also be available.} five version of FrELMo trained on the four corpora presented in this paper and two newly brewed corpora - The French Balanced Reference Corpus CaBeRnet and the French Children Book Test.
%\fi

%In sum, this paper offers three main contributions: (1) two newly built corpora one French Balanced Reference Corpus and a second domain-specific corpus having both oral and written style, (2) five versions of FrELMo, and (3) a whole array of computational results that deepen our understanding on the effects of balance and register in NLP evaluation. 


%The data for corpus creation has been extracted from selected sections of open data including dumped data from the web and section of already published corpora that specialize on a given register of source.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{The present Paper}\label{ssect:Paper_struct}

%The paper presents two substantially different training corpora, proposing a set of evaluations to enquire on the accuracy of Language Models

%We report a few experiments designed to better understand the computational impact of the quality and size of training corpora with the method and tasks described so far. All the experiments are performed on xxx.

%Crucially, manipulating the presence of oral transcriptions and oral based text will be interesting to understand the impact on accuracy of our language model and their impact on several language tasks after fine-tuning. 

%\paragraph{Structure of the paper}
The paper is organized as follows. Sections \ref{sec:DescribeCorpora} and \ref{sec:CompareCorpora} are dedicated to a descriptive overlook of the building of our two newly brewed corpora \Cabernet and CBT-fr, including quantitative measures like type-token ratio and morphological richness. %corpora building and data collection. The construction process of  is here presented.% through
%in this section that summarizes information details that can be found in corpora metadata. The achievement of linguistic balance in \Cabernet is detailed in section \ref{subsec:DescribeCaBeRnet}. 
%statistics on the distribution of lexical, syntactic and morphological features of the different sub-parts of the corpus are also presented. 
%Section \ref{sec:CompareCorpora} focuses on several quantitative measures characterizing the corpora under comparison : number of sentences, type-token ratio and morphological richness. %The characteristics of CBT-fr and \Cabernet are compared to the other corpora under analysis (OSCAR-fr, Wiki-fr). 
Section \ref{sect:EvalMethod} presents the evaluation methods for POS-tagging, NER and dependency Parsing tasks, while results are introduced in §\ref{sect:ResultsCorpora}
Finally, we conclude in §\ref{sec:Concl} on the computational relevance of word-embeddings obtained through a balanced and representative corpus, and broaden the discussion on the benefits of smaller and noiseless corpora in neural NLP.% and some future developments of \Cabernet.

%\notemumu{@ALL should we say here why each corpus ? and summarize the table here under \ref{Table_copus_feature} commented here under}

%\begin{table}[htbp]
%\centering
%\begin{tabular}{lccccc}\\\toprule
%Corpora     & noise     & Range     & O/W    & Ling. \\\midrule
%OSCAR-fr    &  -        &   +      &  -      &   -    \\%\midrule 
%\Cabernet   &  +        &   +      &  both   &   +     \\%\midrule 
%Wiki-fr     &  +        &   -      &  -      &   -    \\%\midrule 
%CBT-fr      &  +        &   -      &  both   &   -     \\\bottomrule  
%\end{tabular}
%\caption{\label{Table_copus_feature} Comparison of corpora. Full-fledged linguistic representativity.}
%\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%% A-  corpus Description.    %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Corpora Building}%- Quantitative Description
\label{sec:DescribeCorpora}
%Two main criteria guided corpus building, the first was balance and representativeness, and the second was maximizing the usage of open resources to build our corpora.

\subsection{\Cabernet} \label{subsec:DescribeCaBeRnet}

\Cabernet corpus was inspired by the genre partition of the American balanced corpus COCA, %\footnote{\url{https://www.corpusdata.org}}
which currently contains over 618 million words of text (20 million words each year 1990-2019) and is equally divided among spoken, fiction, popular magazines, newspapers, and academic texts \citep{davies-2009-the, davies-2010-the}. A second reference, guiding our approach and sampling method, is one of the earliest precursors of balanced reference corpora: the BNC \citep{bnc-2007-the}, first covered a wide variety of genres, with the intention to be a representative sample of spoken and written language.

\Cabernet was obtained by compiling existing data-sets and web-text extracted from different sources as detailed in this section. As shown in Table \ref{Table_Morpho_CabernetSub}, genres sources are evenly divided ($\sim$120 million words each) into spoken, fiction, magazine, newspaper, academic to achieve genre-balanced between oral and written modality in newspapers or popular written style, technical reports and Wikipedia entries, fiction, literature or academic production).
%(cf. Metadata)
%Encompassing five different genres and registers : xxx

\paragraph{\Cabernet Oral} \label{subsec:DescribeCaBeRnetOral}
The oral sub-portion gathers both oral transcriptions (\textsc{ORFEO} and Rhapsodie\footnote{\textsc{ORFEO} corpus available at \url{www.cocoon.huma-num.fr/exist/crdo/} ; Rhapsodie corpus at \url{www.projet-rhapsodie.fr}.}) and Films subtitles (Open Subtitles.org), pruned from diacritics, interlocutors tagging and time stamps. To these transcriptions, the French European Parliament Proceedings (1996-2011), as presented in \citet{koehn-2005-europarl}, contributed a sample of more complex oral style with longer sentences and richer vocabulary.%\url{www.opensubtitles.org/fr}

\paragraph{\Cabernet Popular Press} \label{subsec:DescribeCaBeRnetPop}
The whole sub-portion of Popular Press is gathered from an open data-set from the \textit{Est  Républicain} (1999, 2002 and 2003), a regional press format\footnote{Corpus available at \url{www.cnrtl.fr/corpus/estrepublicain/}.}. %Ce corpus est constitué des données textuelles correspondant à deux années de toutes les éditions intégrales du quotidien régional.
It was selected to match popular style as it is characterized by easy-to-read press style and a wide range of every-day topics characterizing local regional press.

\paragraph{\Cabernet Fiction \& Literature} \label{subsec:DescribeCaBeRnetFic}
The Fiction \& Literature sub-portion was compiled from march 2019's Wiki Source and WikiBooks dump and extracted using WikiExtractor.py, a script that extracts and cleans text from a WikiMedia database dumps, by performing template expansion and preprocessing of template definitions.\footnote{Script available at \url{https://github.com/attardi/wikiextractor}.}

\paragraph{\Cabernet News} \label{subsec:DescribeCaBeRnetNews}
The News sub-portion builds upon web crawled elements, including Wikimedia's NewsComments and WikiNews reports from may 2019 WikiMedia dump, collected with a custom version of WikiExtractor.py.
Newspaper's content gathered by the Chambers-Rostand Corpus (i.e. Le Monde 2002-2003, La Dépèche 2002-2003, L'Humanité 2002-2003) and \textit{Le Monde diplomatique} open-source corpus were assembled to represent a higher register of written news style from different political and thematic horizons.
Several months of French Press Agency reports (AFP, 2007-2011-2012) competed with more simple and telegraphic style the newspaper written sample of the corpus.\footnote{At the time being, this part of \Cabernet corpus is still subject to Licence restrictions. This restricted amount of AFP news reports can reasonably fall in the public domain.}

\paragraph{\Cabernet Academic} \label{subsec:DescribeCaBeRnetAcad}
The academic genre was also built from different sources including technical and educational texts from WikiBooks and Wikipedia dump (prior to 2016) for their thematic variety of highly specialized written production. \textsc{ORFEO} Corpus offered a small sample of academic writings like PHD dissertations and scientific articles encompassing a wide choice of disciplinary topics, and TALN Corpus\footnote{TALN proceedings corpus (about 2 million) builds on a subset of 586 scientific articles (from 2007 to 2013), namely TALN and RECITAL. Available at \url{redac.univ-tlse2.fr/corpus/taln_en.html}.} was included to represent more concise written style characterizing scientific abstracts and proceedings.

\begin{table}[ht]
    \centering\small
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lrrr}                                                                                      \\\toprule
            {\textsc{\Cabernet Sub-set}} & {\textsc{Tokens}} & {\textsc{Unique Forms}} & {\textsc{TTR}} \\\midrule
            Oral                         & 122 864 888       & 291 744                 & 0.0024         \\
            Popular                      & 131 444 017       & 458 521                 & 0.0035         \\
            News                         & 132 708 943       & 462 971                 & 0.0035         \\
            Fiction                      & 198 343 802       & 983 195                 & 0.0050         \\
            Academic                     & 126 431 211       & 1 433 663               & 0.0113         \\
            \textit{Total}               & 711 792 861       & 2 558 513               & 0.0036         \\ \bottomrule
        \end{tabular}}
    \caption{\label{Table_Morpho_CabernetSub} Comparison of number of unique forms in the different genres represented by \Cabernet partition. TTR: Type-Token Ration. Lemmatization and tokenization was performed as described in §\ref{sec:CompareCorpora}.}
\end{table}
%%%check !! 
%\begin{table}[htbp]
%\centering
%\scalebox{0.83}{
%\begin{tabular}{lrrr}\\
%\toprule
%\multicolumn{2}{}{French Balanced Reference Corpus - \textbf{\Cabernet}}\\\midrule
%{\sc \textbf{\Cabernet}}   & nb{\sc Tokens}    & nb {\sc Unique Forms}  & Mo \\\midrule
%Oral        &  122,864,888      & 291,744  & 735,4 Mo   \\ 
%Popular     &  131,444,017      & 458,521  & 758,5 Mo   \\  
%News        &  132,708,943      & 462,971  & 797,2 Mo   \\ 
%Fiction     &  198,343,802      & 983,195  & 1 080 Mo    \\
%Academic    &  126,431,211      & 1433663  & 810,8 Mo   \\
%Tot.        &  711,792,861     & 2,558,513 & 4 190 Mo   \\ \bottomrule  
%\end{tabular}}
%\caption{\label{Table_TTR_CabernetSub} Comparison of number of unique forms in the different genres represent by \Cabernet partition into Oral, Popular, News, Fiction and Academic. Mo: Mega Octet. lemmatisation and tokenisation was achieved as described in section \ref{sec:CompareCorpora}}
%\end{table}
%voir si remplaçabl

For all sub-portions of \Cabernet, visual inspection was performed to remove section titles, redundant meta-information linked to publishing schemes of each of the six news editor includes. This was manually achieved by compiling a rich set of regular expressions specific of each textual source to obtain clean plain text as an outcome.


\subsection{French Children Book Test (CBT-fr)}
\label{subsec:DescribeCBT}

The French Children Book Test (CBT-fr) was built upon its original English version, the Children Book Test (CBT) \citet{hill-etal-2016-the}\footnote{This data-set can be found at \url{www.fb.ai/babi/}.}, which consists of books freely available on \url{www.gutenberg.org}{Project Gutenberg}.

Using youth literature and children books guarantees a clear narrative structure, and a large amount of dialogues, which enrich with oral register the literary style of this corpus.
The English version of this corpus was originally built as benchmark data-set to test how well language models capture meaning in context. It contains 108 books, and a vocabulary size of 53,628.

French version of CBT, named CBT-fr, was constructed to guarantee enough linguistic similarities between the collected books in the two languages. 104 freely available books were included. One third of the books were purposely chosen because they were classical translations of English literary classics. % (see Metadata). %www.cabernet-corpus.fr 
Chapter heads, titles, notes and all types of editorial information were removed to obtain a plain narrative text. The effort of keeping proportion, genre, domain, and time as equal as possible yields a multilingual set of comparable corpora with a similar balance and representativeness.

\begin{table}[ht]
    \centering\small
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lr}                                                             \\\toprule
            {\textsc{Children Book Test - fr}}           & { \textsc{words}} \\\midrule
            number of different lemmas                   & 25 139            \\
            total number of forms                        & 95 058            \\
            mean number of forms per lemma               & 3.78              \\
            Number of lemmas having more than one form : & 14 128            \\
            Percentage of lemmas with multiple forms     & 56.20             \\
            \bottomrule
        \end{tabular}}
    \caption{\label{Table_DescribeCBTfr} Lexical statistics of French CBT, performed as described in §\ref{sec:CompareCorpora}}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%      Section 3    %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Corpora Descriptive Comparison} \label{sec:CompareCorpora}

We used two different tokenizers: SEM, Segmenteur-Étiqueteur Markovien standalone \citet{dupont-2017-exploration} and TreeTagger. Both are based on cascades of regular expressions, and both perform tokenization and sentence splitting.
The first was used for descriptive purposes because it technically allowed to segment and tokenize all corpora including OSCAR (23 billion words). Hence, all corpora were entirely segmented into sentences and tokenized using SEM.

The second tokenization method was run only on 3 million words samples to automatically tag them with TreeTagger into part-of-speech and lemmatize them.\footnote{Based on the tag-set available at \url{https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/french-tagset.html}.} All corpora were randomly shuffled by sentence to then select samples of 3 million words, to be able to compare them in terms of lexical composition (Type-Token Ratio, see Table \ref{Table_MorphoRich}).
%All corpora were POS-Tagged for descriptive reasons using MElt POS-tagging (\citep{denis2012coupling} and \citep{sagot2016external}). Vocabulary size was evaluated after Lemmatization of each Corpus with MElt tool.

\subsection{Corpora Size and Composition}
%\notemumu{@Eric est-ce que je raporte la moyenne ou la mediane ou les deux ? je me rappelle que dans tes scripts tu as les deux.}

Length of sentences is a simple measure to quantify both sentence syntactic complexity and genre. Hence, the number of sentences reported in Table \ref{Table_nb_Words} shows interesting patterns of distributions across genres, consider the comparison between \Cabernet an Wiki-fr.
In our effort to evaluate the impact of corpora pre-training on ELMo-based contextualized word-embedding, we introduce here our two terms of comparison, namely the crawled corpus OSCAR-fr and the Wikipedia-fr one.
%an average length per sentence of 
%xxx for OSCAR-fr, xxx for frWac, xxx for Wiki-fr, xxx for \Cabernet and xx for CBT-fr.
%TODO

\subsubsection{OSCAR fr}
As it has been shown that pre-trained language models can be significantly improved by using more data \citep{liu-etal-2019-roberta,raffel-etal-2020-exploring}, we decided to include in our comparison a corpus of French text extracted from Common Crawl\footnote{More information available  at \url{https://commoncrawl.org/about/}.}. We leverage on a recently published corpus, OSCAR \citep{ortiz-suarez-etal-2019-asynchronous}, which offers a pre-classified and pre-filtered version of the November 2018 Common Craw snapshot.

OSCAR gathers a set of monolingual text extracted from Common Crawl - in plain text \emph{WET} format - where all HTML tags are removed and all text encodings are converted to UTF-8. It follows a similar approach to \citep{grave-etal-2018-learning} by using a language classification model based on the fastText linear classifier \citep{joulin-etal-2016-fasttext,joulin-etal-2017-bag} pre-trained on Wikipedia, Tatoeba and SETimes, supporting 176 different languages.

After language classification, a deduplication step is performed without introducing a specialized filtering scheme: paragraphs containing 100 or more UTF-8 encoded characters are kept. This makes OSCAR an example of unfiltered data that is nearly as noisy as to the original Crawled data.%\footnote{We did not use CCNet because of its difficult availability and download.}

%\subsubsection{frWac} %on l'enlève !
%The frWaC corpus is a French text corpus collected from the .fr domain with using medium-frequency words from the Le Monde Diplomatique corpus and basic French vocabulary lists as seeds. The corpus consists of French websites with total size 1.3 billion words (xxx).%add to biblio

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{FrWIKI}
This corpus collects a selection of pages from Wikipedia-fr from a dump executed in April 2019, where HTML tags and tables were removed, together with template expansion using Attardi's tool (WikiExtractor, §\ref{subsec:DescribeCaBeRnetFic}). As reported on Table \ref{Table_nb_Words}, in this data-set (660 million words) sentences are relatively longer compared to other corpora. It has the advantage of having a comparable size to \Cabernet, but its homogeneity in terms of written genre is set to Wikipedia entries descriptive style.

\begin{table}[ht]
    \centering
    \scalebox{0.85}{
        \begin{tabular}{lrrr}                                                                                 \\\toprule
            {\textsc{corpus}} & { \textsc{wordforms}} & { \textsc{tokens}} & { \textsc{sentences}} \\\midrule
            OSCAR-fr          & 23 212 459 287        & 27 439 082 933     & 1 003 261 066         \\
            Wiki-fr           & 665 599 545           & 802 283 130        & 21 775 351            \\
            \Cabernet         & 697 119 013           & 830 894 133        & 54 216 010            \\
            CBT-fr            & 5 697 584             & 6 910 201          & 317 239               \\\bottomrule
            %frWac       &  1,357,598,417  & 1,622,619,337  &  57,236,199  \\  
        \end{tabular}}
    \caption{\label{Table_nb_Words} Comparing the corpora under study.}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Corpora Lexical Variety}

Focusing on a useful measure of complexity that documents lexical richness or variety in vocabulary, we present the type-token ration (TTR) of the corpora under analysis. Generally used to asses language use aspects like the variety of different words used to communicate by learners or children, it represents the total number of unique words (types/forms) divided by the total number of tokens in a given sample of language production. Hence, the closer the TTR ratio is to 1, the greater the lexical richness of the corpus. Table \ref{Table_Morpho_CabernetSub} summarizes the lexical variety of the five sub-portions of \Cabernet, respectively taken as representative of Oral, Popular, Fiction, News, and Academic genres. Domain diversity of texts can be observed in the lexical statistics showing a gradual increase in the number of distinct lexical forms (cf. TTR). This pattern  reflects a generally acknowledged distributional pattern of vocabulary-size across genres. Oral style shows a poorer lexical variety compared to newspapers/magazines’ textual typology. The lexically rich fictional/classic literature is outreached by academic writing-style with its wide-ranging specialized vocabulary. All in all, Table \ref{Table_Morpho_CabernetSub} quantitatively demonstrates that the selected textual and oral materials are indeed representative of the five types of genres of CaBeRnet.


%\textbf{DEFINITION :} The Type Token Ratio (TTR). The TTR is the number of Types divided by the number of Tokens. The closer the TTR is to 1 the more lexical variety there is. Enter Henry's TTR for his written sample in Table 1 below.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Corpora Morphological richness}

To select a measure that would help quantifying the different corpora morphological richness, we follow \citep{bonami-etal-2015-implicative}. Hence, the proportion of lemmas with multiple forms in a given vocabulary size was evaluated on randomly selected samples of 3-million-words from each corpus under analysis (see Table \ref{Table_MorphoRich}).
%Distribution of the lemmas in function of the percentage of their full flexion . 

%\notemumu{@Benoit : je suis pas sûre de bien ex-primer ici le contenu de nos discussions sur le sujet}

\begin{table}[ht]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lrrrr}
            \toprule
            \textsc{3 M samples}  & \textsc{CBT-fr} & \textsc{\Cabernet} & \textsc{Fr-Wiki} & \textsc{OSCAR} \\
            \midrule
            nb of diff. lemmas    & 25 139          & 30 488             & 31 385           & 31 204         \\
            tot. nb forms         & 95 058          & 180 089            & 238 121          & 190 078        \\
            mean nb forms/lemma   & 3.78            & 6.19               & 7.85             & 6.40           \\
            nb lemmas $>$ 1 form  & 14 128          & 15 927             & 15 182           & 16 480         \\
            \% lemmas  $>$ 1 form & 56.20           & 52.24              & 48.37            & 52.81          \\
            \bottomrule
        \end{tabular}}
    \caption{Lexical statistics on morphological richness over randomly selected samples of 3 million words from each corpus. nb : number}
    \label{Table_MorphoRich}
\end{table}

Table 4 reports some more in-depth lexical and morphological statistics across corpora. Although OSCAR is 34 times bigger than CaBeRnet, their total number of forms and the proportion of lemmas having more than one form in a 3-million-word sample are comparable. FrWiki shows a radically different lexical distribution with numerous hapaxes but a lower morphological richness. Although its total number of forms is more than one third higher than in OSCAR and CaBeRnet samples, the proportion of lemmas having more than one distinct form is around four points below CaBeRnet and OSCAR. Comparatively, youth literature in CBT-fr shows the greatest morphological richness, around 56\% of lemmas have more than one form.

%\begin{table}[ht]
%\centering\small
%\scalebox{0.95}{
%\begin{tabular}{lr}\\\toprule

%{\sc CBT-fr 3 m sample}   &   \\\midrule

%%%number of different lemmas    & 25.139   \\ 
%%total number of forms       &  95.058  \\  
%%mean number of forms per lemma     &  3,78   \\ 
%%Number of lemmas having more than one form :   &  14.128  \\
%%Percentage of lemmas with multiple forms  &  56,20 %\\\midrule

% CBT-fr
%number of forms: 70230
%number of lemmas: 22123
%mean number of forms per lemma: 74819 3.381955430999412
%proportion of lemmas with multiple forms: 11808 0.5337431632237942

%%{\sc CaBeRnetFRanc 3 m sample}   &   \\\midrule
%%number of different lemmas          & 30 488   \\ 
%%total number of forms               &  180.089  \\  
%%mean number of forms per lemma      &  6,19   \\ %188.675
%%Number of lemmas having more than one form :   &  15.927  \\
%%Percentage of lemmas with multiple forms  & 52,24 %\\\midrule

%CaBeRnetFRanc.txt
%number of forms: 
%number of lemmas: 30488
%mean number of forms per lemma: 188675 6.188500393597481
%proportion of lemmas with multiple forms: 15927 0.5224 022566255576

%{\sc frWAC 3 m sample}   &   \\\midrule
%number of different lemmas    & 30.892   \\ 
%total number of forms       &  194.562  \\  
%mean number of forms per lemma     &  6,62 \\ 
%Number of lemmas having more than one form :   &  16.197  \\
%Proportion of lemmas with multiple forms  &  52,43 \\\midrule
% frwac
%number of forms: 194562
%number of lemmas: 30892
%mean number of forms per lemma: 204400 6.61659976692995
%proportion of lemmas with multiple forms: 16197 0.5243 105011006086

%%{\sc frwiki 3 m sample}   &   \\\midrule
%%number of different lemmas    & 31.385   \\ 
%%total number of forms       &  238.121  \\  
%%mean number of forms per lemma     &  7,85   \\ 
%Number of lemmas having more than one form :   &  15.182  \\
%%Percentage of lemmas with multiple forms  &  48,37 %\\\midrule
% frwiki
%number of forms: 238121
%number of lemmas: 31385
%mean number of forms per lemma: 246418 7.851457702724232
%proportion of lemmas with multiple forms: 15182 0.4837 3426796240243

%{\sc OSCAR 3 m sample}   &   \\\midrule
%number of different lemmas    & 31.204   \\ 
%%total number of forms       &  190.078  \\  
%%mean number of forms per lemma     &  6,40   \\ 
%Number of lemmas having more than one form :   &  16.480  \\
%%Percentage of lemmas with multiple forms  &  52,81 %\\
% OSCAR
%number of forms: 190078
%number of lemmas: 31204
%mean number of forms per lemma: 199589 6.396263299576978
%proportion of lemmas with multiple forms: 16480 0.5281 374182797077
%\bottomrule  
%\end{tabular}}
%\caption{\label{Table_MorphoRich} Lexical Statistics comparing morphological richness of the corpora under study.}
%\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%% section 4    Evaluation.    %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Corpora Evaluation Tasks} \label{sect:EvalMethod}

This section reports the method of experiments designed to better understand the computational impact of the quality, size and linguistic balance of ELMo's \citep{peters-etal-2018-deep} pre-training (§\ref{MethodTRAIN}) and their evaluations tasks (§\ref{MethodEVAL}).

\paragraph{Embeddings from Language Models} ELMo is an LSTM-based language model. More precisely, it uses a bidirectional language model, which combines a both forward and a backward LSTM-based language models. ELMo also computes a context-independent token representation via a CNN over characters.
Methodologically, we selected ELMo which not only performs generally better on sequence tagging than other architectures, but which is also better suited to pre-train on small corpora because of its smaller number of parameters (93.6 million) compared to the RoBERTa-base architecture used for CamBERT (BERTbase, 12,110 million - Transformer) \citep{martin-etal-2020-camembert}.

\subsection{ELMo Pre-traing \& Fine-tuning Method}\label{MethodTRAIN}

Two protocols were carried out to evaluate the impact of corpora characteristics on the tasks under analysis. \textit{Method 1} implies a full pre-training ELMo-based language models for each of the corpora mentioned in Table \ref{Table_nb_Words}. While \textit{Method 2} is based on pre-training OSCAR + fine-tuning with our French Balanced Reference Corpus \Cabernet, yielding \ELMocoscar.
Hence, the pure pre-traing (i.e. Method 1) yields the following four language models which were pre-trained on the four corpora under comparison :  \ELMooscar, \ELMowiki, \ELMococa and \ELMocbt. %The fine-tuning method (i.e. Method 2) was applied only to \ELMooscar fine-tuned with \Cabernet.

%we seek to understand if fine-tuning with resources that are up to 30 times smaller than pre-training corpora has a observable impact on NLP tasks scores. It is namely for this reason, w


% \iffalse
% \paragraph{Embeddings from Language Models} (ELMo) \citep{peters-etal-2018-deep} is a neurla Language Model, that is, a model that given a sequence of $N$ input tokens, $(t_1, t_2, ..., t_N)$, computes the probability of the sequence by modeling the probability of token $t_k$ given the history $(t_1, ..., t_{k-1})$:
% \[
%     p(t_1, t_2, \ldots, t_N) = \prod_{k=1}^N p({t_k} \mid t_1, t_2, \ldots, t_{k-1}).
% \]
% ELMo in particular uses a biLM consisting of LSTM layers, that is, it concatenates both a forward and a backward language model generating a contextualized bi-directional representation of each token in a given sentence.

% All the training experiments are performed with a fully trained model for 10 epochs. As is was done for the original English ELMo \citep{peters-etal-2018-deep}.
% Hence, all our FRrELMo-based language models build on top of the UDPipe Future parser and tagger \citep{straka-2018-udpipe} as implemented in \citet{straka2019evaluating} which is open source and freely available.\footnote{https://github.com/CoNLL-UD-2018/UDPipe-Future}
% \fi

%\paragraph{The UDPipe Future architecture} is a multi-task model that predicts POS tags, lemmas and dependency trees jointly. It consists of an embedding step containing: character level word-embeddings that are trained along the rest of the network, pre-trained word-embeddings\footnote{We use the French fastText embeddings distributed by \citep{Grave:2018}.}, a randomly initialized word embeddings that are trained along the rest of the network, and contextualized word-embeddings for which we plug our customly trained ELMos.

%All these embeddings are then concatenated and are fed to two shared Bi-LSTMs that generate shared representations that are forwarded to two separate Bi-LSTMs; one that is followed by a softmax layer and predicts the POS tags, and another that is followed by a Deep Bi-Affine Attention Layer \citep{Dozat:2017b} that produces dependency trees.


%To this formal reason an additional ecological one is to be considered. A recent paper (\textbf{cite ACL paper}) shows the environmental impact of ... \notemumu{do you remember the ACL paper about ecological reasons for ELMo ? }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Base evaluation systems}

\textbf{UDPipe Future} \citep{straka-2018-udpipe} is an LSTM based model ranked 3\textsuperscript{rd} in dependency parsing and 6\textsuperscript{th} in POS tagging during the CoNLL~2018 shared task \citep{seker-etal-2018-universal}. We report the scores as they appear in \citet{kondratyuk-straka-2019-75}'s paper.
We add to UDPipe Future, five differently trained ELMo language model pre-trained on the qualitatively and quantitatively different corpora under comparison. Additionally, we also test the impact of the \Cabernet Corpus on ELMo fine-tuning.

\textbf{The LSTM-CRF} is a model originally concived by \citet{lample-etal-2016-neural} is just a Bi-LSTM pre-appended by both character level word embeddings and pre-trained word embeddings and pos-appended by a CRF decoder layer. For our experiments, we use the implementation of \citep{strakova-etal-2019-neural} which is readily available\footnote{Available at \url{https://github.com/ufal/acl2019_nested_ner}.} and it is designed to easily pre-append contextualized word-embeddings to the model.

\subsection{Evaluation Tasks}\label{MethodEVAL}

We distinguish three main evaluation tasks that were performed
to asses the lexical and syntactic quality of contextualized word-embeddings obtained from different pre-training corpora under comparison.% : ELMo pre-trained on OSCAR (\ELMooscar), frWIKI (\ELMowiki), \Cabernet (\ELMococa) and CBT-fr (\ELMocbt). 
Crucially, comparing them with and ELMo pre-trained on OSCAR and fine-tuned with \Cabernet, i.e. \ELMocoscar, will allow to control for  the presence of oral transcriptions and proceeding in order to understand its impact on the accuracy of our language model and on the development experiments after fine-tuning.% Our development experiments compare the corpora presented in Table \ref{Table_nb_Words}.
%by and comparing them with ELMo pre-trained on OSCAR and fine-tuned with \Cabernet, i.e. \ELMocoscar (see Results Table \ref{tab:fine-tuning_results}).
%justifier les différentes taches :

\paragraph{Syntactic tasks}
The evaluation tasks were selected to probe to what extent corpus "representativeness" and balance is impacting syntactic representations, in both (1) low-level syntactic relations in POS-tagging tasks, and (2) higher level syntactic relations at constituent- and sentence-level thanks to dependency-parsing evaluation task. Namely, POS-tagging is a low-level syntactic task, which consists in assigning to each word its corresponding grammatical category. Dependency-parsing consists of higher order syntactic task like predicting the labeled syntactic tree capturing the syntactic relations between words.
We evaluate the performance of our models using the standard UPOS accuracy for POS-tagging, and Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS) for dependency parsing. We assume gold tokenisation and gold word segmentation as provided in the UD treebanks.
%Additionally, we include a contrast for the two corpora that are comparable in size on Language model perplexities, namely FrWiki and \Cabernet.

\paragraph{Lexical tasks}
To test for word-level representation obtained through the different pre-training corpora and fine-tunings, Named Entity Recognition task (NER) was retained (\ref{ner-section}). As it involves a sequence labeling task that consists in predicting which words refer to real-world objects, such as people, locations, artifacts and organizations, it directly probes the quality and specificity of semantic representations issued by the more or less balanced corpora under comparison.

%\notemumu{@All : est-ce qu eje peux dire ça ? Cette interprétation est-elle correcte ?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{POS-tagging and dependency parsing}

%To build a state-of-the at baseline, we fist evaluate \camembert 

Experiments were run using the Universal Dependencies (UD) paradigm and its corresponding UD POS-tag set \citep{petrov-etal-2012-universal} and UD treebank collection version 2.2 \citep{nivre-etal-2018-universal}, which was used for the CoNLL 2018 shared task.

Different terms of comparisons were considered on the two downstream tasks of part-of-speech (POS) tagging and dependency parsing.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Treebanks test data-set}
We perform our work on the four freely available French UD treebanks in UD~v2.2: GSD, Sequoia, Spoken, and ParTUT, presented in Table \ref{treebanks-tab-cabernet}.

\textbf{GSD} treebank \citep{mcdonald-etal-2013-universal} is the second-largest tree-bank available for French after the FTB (described in subsection \ref{ner-section}), it contains data from blogs, news, reviews, and Wikipedia.

\textbf{Sequoia} tree-bank %\footnote{\url{https://deep-sequoia.inria.fr}} %candito2012le,
\citep{candito-etal-2014-deep} comprises more than 3000 sentences, from the French Europarl, the regional newspaper \emph{L’Est Républicain}, the French Wikipedia and documents from the European Medicines Agency.

\textbf{Spoken} was automatically converted from the Rhapsodie tree-bank  %\footnote{\url{https://www.projet-rhapsodie.fr}} 
\citep{lacheret-etal-2014-rhapsodie} with manual corrections. It consists of 57 sound samples of spoken French with phonetic transcription aligned with sound (word boundaries, syllables, and phonemes), syntactic and prosodic annotations.

Finally, \textbf{ParTUT} is a conversion of a multilingual parallel treebank developed at the University of Turin, and consisting of a variety of text genres, including talks, legal texts, and Wikipedia articles, among others; ParTUT data is derived from the already-existing parallel treebank, Par(allel)TUT \citep{sanguinetti-Bosco-2015-parttut}. Table~\ref{treebanks-tab-cabernet} contains a summary comparing the sizes of the treebanks.%\footnote{\url{https://universaldependencies.org}}.

\begin{table}
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lcccl}
            \toprule
            Treebank & Tokens  & Words   & Sentences & Genre                    \\
            \midrule
            GSD      & 389 363 & 400 387 & 16 342    & News Wiki. Blogs         \\
            Sequoia  & 68 615  & 70 567  & 3 099     & Pop. Wiki. Med. EuroParl \\
            Spoken   & 34 972  & 34 972  & 2 786     & Oral transcip.           \\
            ParTUT   & 27 658  & 28 594  & 1 020     & Oral Wiki. Legal         \\
            \bottomrule
        \end{tabular}}
    \caption{Sizes of the 4 treebanks used in the evaluations of POS-tagging and dependency parsing. \label{treebanks-tab-cabernet}}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{State-of-the-art}

For POS-tagging and Parsing we select as a baseline UDPipe Future (2.0), without any additional contextualized embeddings \citep{straka-2018-udpipe}. This model was ranked 3rd in dependency parsing and 6th in POS-tagging during the CoNLL~2018 shared task \citep{seker-etal-2018-universal}. Notably, UDPipe Future provides us a strong baseline that does not make use of any pre-trained contextual embedding.

We report on Table \ref{tab:fine-tuning_results} the published results on UDify by \cite{kondratyuk-straka-2019-75}, a multitask and multilingual model based on \mbert that is near state-of-the-art on all UD languages including French for both POS-tagging and dependency parsing.

%On the other hand, UDify, UDPipe Future + mBERT \citep{straka2019evaluating} and \camembert \citep{martin-etal-2020-camembert} represent different terms of comparison for state-of-the-art results on Parsing and POS-tagging.

%We compare our models to  \citep{kondratyuk-straka-2019-75}, 

Finally, it is also relevant to compare our results with \camembert on the selected tasks, because compared to UDify it is the work that pushed the furthest the performance in fine-tuning end-to-end a \bert-based model.

%Finally, we compare our models to UDPipe Future 

%To demonstrate the value of building a dedicated version of \bert for French, we first compare \camembert to the multilingual cased version of \bert (designated as \mbert).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% BIG results table %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}
    \small\centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{ l  c  c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c  c  @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c  c }
            \toprule
                                                        & \multicolumn{3}{c @{\hspace{0.5cm}}}{\textsc{GSD}} & \multicolumn{3}{c @{\hspace{0.7cm}}}{\textsc{Sequoia}} & \multicolumn{3}{c @{\hspace{0.7cm}}}{\textsc{Spoken}} & \multicolumn{3}{c @{\hspace{0.35cm}}}{\textsc{ParTUT}}                                                                                                                                                                                                                                                                                                                        \\
            \cmidrule(l{2pt}r{0.4cm}){2-4}\cmidrule(l{-0.2cm}r{0.4cm}){5-7}\cmidrule(l{-0.2cm}r{0.4cm}){8-10}\cmidrule(l{-0.2cm}r{2pt}){11-13}
            \multirow{-2}{*}[1pt]{\textsc{Model}}       & \textsc{UPOS}                                      & \textsc{UAS}                                           & \textsc{LAS}                                          & \textsc{UPOS}                                          & \textsc{UAS}                           & \textsc{LAS}                           & \textsc{UPOS}                              & \textsc{UAS}                           & \textsc{LAS}                           & \textsc{UPOS}     & \textsc{UAS}                           & \textsc{LAS}                           \\
            \midrule
            %\multicolumn{1}{c}{UDPipe Future + ELMo} & \multicolumn{12}{c}{}\\
            %\cmidrule(lr){1-1}

            %\multicolumn{13}{l}{\textit{Baseline}} \\
            \underline{\textit{Baseline} UDPipe Future} & 97.63                                              & 90.65                                                  & 88.06                                                 & 98.79                                                  & 92.37                                  & 90.73                                  & 95.91                                      & 82.90                                  & 77.53                                  & 96.93             & 92.17                                  & 89.63                                  \\

            \:+\ELMocbt                                 & 97.49                                              & 90.21                                                  & 87.37                                                 & 98.40                                                  & 92.18                                  & 90.56                                  & 96.60                                      & 85.05                                  & 79.82                                  & 97.27             & 92.55                                  & 90.44                                  \\

            \:+\ELMowiki                                & \underline{97.92}                                  & 92.13                                                  & 89.77                                                 & 99.22                                                  & 94.28                                  & 92.97                                  & \underline{97.28}                          & 85.61                                  & 80.79                                  & \textbf{97.62}    & 94.01                                  & 91.78                                  \\

            %-FrWak  & \underline{97.89} & 92.04 & 89.70 & 99.25 & 94.53 & 93.36 & 97.20 & \textbf{86.04} & \textbf{81.14} & 97.47 & \textbf{94.78} & 92.40\\ 

            %\midrule 
            %\:+\ELMococa  & 97.76 & 91.91 & 89.49 & \underline{99.27} & \underline{94.65} & \underline{93.40} & \cellcolor[gray]{0.7}\emph{\textbf{97.32}} & 85.63 & 80.61 & \underline{97.58} & 94.24 & 91.90\\ 

            %%%%%%% new results on clean cabernet %%%%%%%%%%%%%%%%
            \:+\ELMocaber                               & 97.87                                              & 92.02                                                  & 89.62                                                 & \underline{99.33}                                      & 94.42                                  & 93.14                                  & \cellcolor[gray]{0.7}\emph{\textbf{97.30}} & 85.39                                  & 80.63                                  & 97.43             & 94.02                                  & 91.86                                  \\
            %\midrule 
            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

            \:+\ELMooscar                               & 97.85                                              & \cellcolor[gray]{0.9}\underline{92.41}                 & \cellcolor[gray]{0.9}\underline{90.05}                & 99.30                                                  & \cellcolor[gray]{0.9}\underline{94.43} & \cellcolor[gray]{0.9}\underline{93.25} & 97.10                                      & \cellcolor[gray]{0.9}\underline{85.83} & \cellcolor[gray]{0.9}\textbf{80.94}    & 97.47             & \cellcolor[gray]{0.9}\textbf{94.74}    & \cellcolor[gray]{0.9}\textbf{92.55}    \\

            \midrule
            %\:+\ELMocoscar & \underline{97.88} & \cellcolor[gray]{0.9}\textbf{92.67} & \cellcolor[gray]{0.9} \textbf{90.34} & 99.26 & \cellcolor[gray]{0.9}\textbf{94.75} & \cellcolor[gray]{0.9}\textbf{93.54} & 97.22 & \cellcolor[gray]{0.9}\underline{85.77} & \cellcolor[gray]{0.9}\underline{80.80} & 97.50 & \cellcolor[gray]{0.9}\underline{94.66} & \cellcolor[gray]{0.9}\underline{92.43} \\ 

            %%%%%%%new results on clean cabernet oscar %%%%%%%%%%%%%%%%

            \:+\ELMocabercar                            & \textbf{97.98}                                     & \cellcolor[gray]{0.9}\textbf{92.57}                    & \cellcolor[gray]{0.9} \textbf{90.22}                  & \textbf{99.34}                                         & \cellcolor[gray]{0.9}\textbf{94.51}    & \cellcolor[gray]{0.9}\textbf{93.38}    & 97.24                                      & \cellcolor[gray]{0.9}\textbf{85.91}    & \cellcolor[gray]{0.9}\underline{80.93} & \underline{97.58} & \cellcolor[gray]{0.9}\underline{94.47} & \cellcolor[gray]{0.9}\underline{92.05} \\

            \midrule %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
            \multicolumn{13}{l}{\textit{State-of-the-art}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\

            \underline{UDify}                           & 97.83                                              & 93.60                                                  & 91.45                                                 & 97.89                                                  & 92.53                                  & 90.05                                  & 96.23                                      & 85.24                                  & 80.01                                  & 96.12             & 90.55                                  & 88.06                                  \\

            UDPipe Future + mBERT                       & 97.98                                              & 92.55                                                  & 90.31                                                 & \emph{99.32}                                           & 94.88                                  & 93.81                                  & 97.23                                      & \emph{86.27}                           & \emph{81.40}                           & \emph{97.64}      & 94.51                                  & 92.47                                  \\

            \camembert                                  & \emph{98.19}                                       & \emph{94.82}                                           & \emph{92.47}                                          & 99.21                                                  & \emph{95.56}                           & \emph{94.39}                           & 96.68                                      & 86.05                                  & 80.07                                  & 97.63             & 95.21                                  & \emph{92.90}                           \\

            \bottomrule
        \end{tabular}}
    \caption{Final POS and dependency parsing scores on 4 French treebanks (French GSD, Spoken, Sequoia and ParTUT), reported on test sets (4 averaged runs) assuming gold tokenisation. Best scores in bold, second to best underlined, state-of-the-art results in italics.}
    %gray : 
    %light gray :
    %frwac et orscar (10 fois plus grand que FRWAc)

    \label{tab:fine-tuning_results}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Named Entity Recognition}\label{ner-section}
\label{evalner}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Treebanks test data-set}
The benchmark data set from the French Treebank (FTB)  \citep{abeille-etal-2003-building} was selected in its 2008 version, as introduced by \citet{candito-crabbe-2009-improving} and complemented with NER annotations by \citet{sagot-etal-2012-annotation}\footnote{The NER-annotated FTB contains approximately than 12k sentences, and more than 350k tokens were extracted from articles of \emph{Le Monde} newspaper (1989 - 1995). As a whole, it encompasses 11,636 entity mentions distributed among 7 different types : 2025 mentions of ``Person'', 3761 of ``Location'', 2382 of ``Organisation'', 3357 of ``Company'', 67 of ``Product'', 15 of ``POI'' (Point of Interest) and 29 of ``Fictional Character''.}.
The tree-bank, shows a large proportion of the entity mentions that are multi-word entities. We therefore report the three metrics that are commonly used to evaluate models: precision, recall, and F1 score. %Specifically, (1) precision measures account for  the percentage of entities found by the system that are correctly tagged, (2) recall measures sand for the percentage of named entities present in the corpus that are found, and (3) F1 score measure combines both precision and recall measures giving a global measure of a model's performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{NER State-of-the-art} %baseline Dupont  + st ate of the art cambert 

% Most of the advances in NER haven been achieved in English, particularly focusing on the CoNLL 2003 \citep{tjong2003introduction} and the Ontonotes v5 \citep{pradhan2012conll,pradhan2013towards} English corpora. 

%Importantly, NER task was traditionally tackled using Conditional Random Fields (CRF) \citep{lafferty-etal-2001-conditional}, CRFs were later used as decoding layers for Bi-LSTM architectures \citep{huang2015bidirectional,lample-etal-2016-neural} showing considerable improvements over CRFs alone. Later, these Bi-LSTM-CRF architectures were enhanced with contextualised word-embeddings which yet again brought major improvements to the task \citep{peters-etal-2018-deep,akbik2018contextual}. Finally, large pre-trained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures \citep{devlin2019bert,baevski2019cloze}.

%\notemumu{@Pedro : voir si ce paragraphe est necessaire ou pas : je l'ai commenté pour l'instant, OUI ! Ça m'interesse } ok ! on le remet !
%In non-English NER the CoNLL 2002 shared task included NER corpora for Spanish and Dutch corpora \citep{tjong2002introduction} while the CoNLL 2003 included a German corpus \citep{tjong2003introduction}. Here the recent efforts of \citep{strakova-etal-2019-neural} settled the state of the art for Spanish and Dutch, while \citep{akbik2018contextual} did it for German.

English has received the most attention in NER in the past, with some recent developments in German, Dutch and Spanish by \citet{strakova-etal-2019-neural}. In French, no extensive work has been done due to the limited availability of NER corpora. We compare our model with the stable baselines settled by \citep{dupont-2017-exploration}, who trained both CRF and BiLSTM-CRF architectures on the FTB and enhanced them using heuristics and pre-trained word-embeddings.

And additional term of comparison was identified in a recently released state-of-the-art language model for French, CamemBERT \citep{martin-etal-2020-camembert}, based on the RoBERTa architecture pre-trained on the French sub-corpus of the newly available multilingual corpus OSCAR \citep{ortiz-suarez-etal-2019-asynchronous}.

%Mumu add summary camembert ? Peut êtr epas nécessaire  à discuter vaec Pedro

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%  NER results table %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lccc}
            \toprule
            %\multicolumn{4}{c}{\textsc{NER - Results}}  \\\midrule
            \textsc{NER - Results} on FTB                 & Precision                            & Recall                              & F1                                  \\
            \midrule
            \multicolumn{4}{l}{\textit{Baselines Models}}                                                                                                                    \\
            SEM (CRF) \citep{dupont-2017-exploration}     & 87.89                                & 82.34                               & 85.02                               \\ %baseline 
            LSTM-CRF \citep{dupont-2017-exploration}      & 87.23                                & 83.96                               & 85.57                               \\ \midrule %baseline 2
            LSTM-CRF  test models                         & 85.87                                & 81.35                               & 83.55                               \\
            \:+FastText                                   & 88.53                                & 84.63                               & 86.53                               \\
            \:+FastText+\ELMocbt                          & 79.77                                & 77.63                               & 78.69                               \\
            \:+FastText+\ELMowiki                         & 88.87                                & 87.56                               & 88.21                               \\
            % \:+FastText+\ELMococa                  & 88.82                 & 87.82                 & 88.32                 \\
            \:+FastText+\ELMocaber                        & \underline{88.91}                    & 87.22                               & 88.06                               \\
            \:+FastText+\ELMooscar                        & 88.89                                & \underline{88.43}                   & \underline{88.66}                   \\\midrule %
            %\:+FastText+\ELMocoscar                & \cellcolor[gray]{0.8} \emph{\textbf{88.93}} & \underline{88.08}     & \underline{88.50}     \\
            \:+FastText+\ELMocabercar                     & \cellcolor[gray]{0.8} \textbf{90.70} & \cellcolor[gray]{0.8}\textbf{89.12} & \cellcolor[gray]{0.8}\textbf{89.93} \\
            \midrule

            \multicolumn{4}{l}{\textit{State-of-the-art Models}}                                                                                                             \\
            \camembert \citep{martin-etal-2020-camembert} & 88.35                                & 87.46                               & 87.93                               \\ %baseline state of the art 
            \bottomrule
        \end{tabular}}
    \caption{NER Results on French Treebank (FTB): \textbf{best scores}, \underline{second to best}.}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%% results section  %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\section{Results \& Discussion} \label{sect:ResultsCorpora}

\subsection{Dependency Parsing and POS-tagging}\label{sect:ResultsParsePOS}

\paragraph{\ELMococa : a test for balance}
% balanced aspect of oral pays off 
The word-embeddings representations offered by \ELMococa are not only competitive but sometimes better than Wikipedia ones. One should keep in mind that almost all of the four treebanks we use in this section include Wikipedia data.
\ELMococa is reaching state-of-the-are results in POS-tagging on Spoken. Notably, it performs better than \camembert, the previous state of the art on this oral specialized tree-bank (cf. dark gray highlight on Table \ref{tab:fine-tuning_results}). We understand this results as a clear effect of balance when testing upon a purely spoken test-set. Importantly, this effect is difficultly explainable by the size of oral-style data in \Cabernet. The oral sub-part is only one fifth of the total, and in this one fifth, only an even smaller amount of data comes from purely oral transcripts comparable the ones in the Spoken tree-bank, namely 67,444 words from Rhapsodie corpus, and 575,894 words form \textsc{ORFEO}. Hence, \Cabernet's  balanced oral language use shows to pay off in POS-tagging. These results are extremely surprising especially given the fact that our evaluation method was aiming at comparing the quality of word-embedding representations and not beating the state-of-the-art.
%We observe that compared to OSCAR \Cabernet on in Sequoia  !

\paragraph{\ELMococa : a test for coverage}
From Table \ref{tab:fine-tuning_results}, we discover that not only balance, but also the broad and diverse genre converge of \Cabernet may play a role in its POS-tagging success is we compare its results with \ELMocbt that also features oral dialogues in youth literature. The fact that \ELMocbt does not show a comparable performance in POS-tagging, can be interpreted as linked to its size, but possibly also to its lack of variety in genres, thus, suggesting the advantage of a comprehensive coverage of language use. This suggests that a balanced sample may enhance the convergence of generalization about oral-style from distinct genre that still imply oral-like dialogues like in fiction. In sum, broad coverage may contribute to enhancing representations about oral language.

\paragraph{The effect of balance on Fine-tuning}
For POS-tagging in GSD the results of \ELMooscar are in second place position compared to \ELMocoscar that is extremely close to \ELMowiki. While in POS-tagging in ParTUT, \ELMowiki exhibits better results than \ELMooscar, and \ELMocoscar is in second position.

Further comparing GSD and Sequoia scores from \ELMooscar and \ELMocoscar, we observe that fine-tuning with \Cabernet the emdeddings that were pre-trained on OSCAR, yields better representations for the three tasks compared to both the original \ELMooscar and \ELMococa.
However, fine-tuning does not always yield better findings than \ELMooscar on Spoken and ParTUT, where \ELMocoscar places in second after \ELMooscar for parsing scores UAS/LAS (cf. Table \ref{tab:fine-tuning_results}).

A closer look on Parsing results reveals an interesting pattern of results across treebanks (see light gray highlights on Table \ref{tab:fine-tuning_results}). We see that for GSD and Sequoia the \Cabernet fine-tuned version \ELMocoscar compared to the pure OSCAR pre-trained \ELMooscar is achieving higher scores. While a reverse and less clear-cut pattern is observable for the other two treebanks, namely Spoken and ParTUT. This configuration can be explained if we understand this pattern as due to the reinforcement and unlearning of \ELMooscar representations during the process of fine-tuning. Specifically, we can observe that parsing scores are better on treebanks that share the kind of language use represented in \Cabernet, while they are worst on corpora that are closer in language sample to OSCAR corpus, like Spoken and ParTuT. This calls for further developments of \Cabernet (§\ref{sec:Concl}).
%stucutre sytnaxique with hesitations dans Spoken

\paragraph{\ELMocbt: small but relevant}
\ELMocbt shows an intriguing pattern of results. Even if its scores are under the baseline on GSD and Sequoia, it yields over the baseline results for Spoken and ParTUT.
%While the under the baseline results could be explained by over-fitting 
Given its reduced size, one would expect it to overfit, this would explain the under baseline performance. However, this was not the case on Spoken and ParTUT treebanks, thus showing \ELMocbt contribution in generating representations that are useful to UDPipe model to achieve better results in POS-tagging and parsing tasks on the ParTUT and Spoken tree-banks. The presence of oral dialogues is certainly playing a role in this results' pattern.
This unexpected result calls for further investigation on the impact of pre-training with reduced-size, noiseless, domain-specific corpora.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NER} \label{sect:ResultsNER}

For named entity recognition, LSTM-CRF +FastText +\ELMocabercar achieves a better precision, recall and F1 than the traditional CRF-based SEM architectures (§ \ref{evalner}) and \camembert, which is currently state-of-the-art.%(CRF and Bi-LSTM +CRF)
Importantly, LSTM-CRF +FastText +\ELMocaber reaches better results in finding entity mentions, than Wikipedia which is a highly specialized corpus in terms of vocabulary variety and size, as can be seen in the overwhelming total number of unique forms it contains (see Table \ref{Table_MorphoRich}). We can conclude that both pre-training and fine-tuning with \Cabernet on ELMo OSCAR generates better word-embedding representations than Wikipedia in this downstream task.
%Overall, NER scores shows improvements compared to \camembert. 

%Overall, Fine-tuning with \Cabernet shows better results that \ELMocaber and \ELMooscar. %we understand this slight drop as possibly due to unlearning of the wide spectrum of vocabulary that is in OSCAR and not in \Cabernet. For instance the whole french Wikipedia is included in OSCAR and not in \Cabernet. Nonetheless, it has to be noted that these scores are still better than previous state-of-the-art, \camembert.

%As for, fine-tuning with \Cabernet we observe a raise in Precision, but a negative impact on the recall. 

CBT-fr NER results are under the LSTM-CRF baseline. This can possibly be explained by the distance in terms of topics and domain from FTB tree-bank (i.e. newspaper articles), or by the reduced-size of the corpus to yield good-enough representation to perform entity mentions recognition.

All in all, our evaluations confirm the effectiveness of large ELMo-based language models fine-tuned or pre-trained with a balanced and linguistically representative corpus, like \Cabernet as opposed to domain-specific ones, or to an extra-large and noisy one like OSCAR.

%out :
%In sum, while the base model LSTM-CRF+Fastext is better than state-of-the-art \camembert, adding \Cabernet, OSCAR or both shows a dramatic improvement in finding entity mentions.

%All in all, we showed that \Cabernet corpus can reliably be used as a basis for training neural language models that perform in down-stream tasks, as well as suited for the creation of balanced lexical frequency-based dictionary entries, grammar studies, other language reference materials.


%%%%%%%%%%%%%%%%%%%%%%%%% STATS  %%%%%%%%%%%%%%%%%%%%%%% 

%\subsection{Results - Statistics}\label{ssect:ResultsMethod}
%We performed statistical comparison to test which method provided the better accuracy scores.
%\notemumu{@Pedro : do we finally have any stats ? }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%% CONCL %%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\section{Perspectives \& Conclusion} \label{sec:Concl}

%summarize in one sentence the aim/scope of the paper

The paper investigates the relevance of different types of corpora on ELMo's pre-training and fine-tuning. It confirms the effectiveness and quality of word-embeddings obtained through balanced and linguistically representative corpora. %while POS-tagging beats state-of-the art results.

%All our FRrELMo-based language models build on UDPipe Future parser and tagger, 
By adding to UDPipe Future 5 differently trained ELMo language models that were pre-trained on qualitatively and quantitatively different corpora, our French Balanced Reference Corpus \Cabernet unexpectedly establishes a new state-of-the-art for POS-tagging over previous monolingual \citep{straka-2018-udpipe} and multilingual approaches \citep{straka-strakova-2019-evaluating,kondratyuk-straka-2019-75}.

The proposed evaluation methods are showing that the two newly built corpora that are published here are not only relevant for neural NLP and language modeling in French, but that corpus balance shows to be a significant predictor of ELMo's accuracy on Spoken test data-set and for NER tasks.

Other perspective uses of \Cabernet involve it use as a corpus offering a reference point for lexical frequency measures, like association measures. Its comparability with English COCA further grants the cross-linguistic validity of measures like Point-wise Mutual Information or DICE's Coefficient. The representativeness probed through our experimental approach are key aspects that allow such measures to be tested against psycho-linguistic and neuro-linguistic data as shown in previous neuro-imaging studies \citep{bhattasali-etal-2018-processing}.

The results obtained for the parsing tasks on ParTUT open a new perspective for the development of the French Balanced Reference Corpus, involving the enhancement of the terminological coverage of \Cabernet. A sixth sub-part could be included to cover technical domains like legal and medical ones, and thereby enlarge the specialized lexical coverage of \Cabernet.
Further developments of this resource would involve an extension to cover user-generated content, ranging from well written blogs, tweets to more variable written productions like newspaper's comment or forums, as present in the CoMeRe corpus \citep{chanier-etal-2014-the}.%\footnote{More on CoMere corpus at \url{https://repository.ortolang.fr/api/content/comere/v2/comere.html}.}
The computational experiments conducted here also show that pre-training language models like ELMo on a very small sample like the French Children Book Test corpus or \Cabernet yields unexpected results. This opens a perspective for languages that have smaller training corpora. ELMo could be a better suited language model for those languages than it is for others having larger size resources.

Results on the NER task show that size - usually presented as the more important factor to enhance the precision of representation of word-embeddings - matters less than linguistic representativeness, as achieved through corpus linguistic balance. \ELMocoscar sets state-of-the art results in NER (i.e. Precision, Recall and F1) that are superior than those obtained with a 30 times larger corpus, like OSCAR.

%,Fabre:2019,Fabre:2020}

%\iffalse
%In the same line, an additional perspective to this work is to better understand why we observe better NER scores with ELMo architecture than we do with BERT-base language model.
%\fi

%Computational big buttom line

%The original methodology of fine-tuning neural language models with smaller, balanced and noiseless corpora presented in this paper paves the way for further computational work in evaluating corpora quality for parsing and other NLP tasks. 

%We found out that both method  that is increasing accuracy of the Language model in both pure pre-training . \notemumu{check the results !!!} 
%%%%%%%
To conclude, our current evaluations show that linguistic quality in terms of \textit{representativeness} and balance is yielding better performing contextualized word-embeddings.