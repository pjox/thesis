% Encoding: UTF-8

@inbook{abeille-etal-2003-building,
  author    = {Abeill{\'e}, Anne
               and Cl{\'e}ment, Lionel
               and Toussenel, Fran{\c{c}}ois},
  title     = {Building a Treebank for French},
  booktitle = {Treebanks: Building and Using Parsed Corpora},
  year      = {2003},
  publisher = {Springer Netherlands},
  address   = {Dordrecht},
  pages     = {165--187},
  abstract  = {We present a treebank project for French. We have annotated a newspaper corpus of 1 Million words with part of speech, inflection, compounds, lemmas and constituency. We describe the tagging and parsing phases of the project, and for each, the automatic tools, the guidelines and the validation process. We then present some uses of the corpus as well as some directions for future work.},
  isbn      = {978-94-010-0201-1},
  doi       = {10.1007/978-94-010-0201-1_10},
  url       = {https://doi.org/10.1007/978-94-010-0201-1_10}
}

@article{ando-zhang-2005-framework,
  author  = {Rie Kubota Ando and Tong Zhang},
  title   = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {61},
  pages   = {1817-1853},
  url     = {http://jmlr.org/papers/v6/ando05a.html}
}

@article{arivazhagan-etal-2019-massively,
  author        = {{Arivazhagan}, Naveen and {Bapna}, Ankur and {Firat}, Orhan and {Lepikhin}, Dmitry and {Johnson}, Melvin and {Krikun}, Maxim and {Chen}, Mia Xu and {Cao}, Yuan and {Foster}, George and {Cherry}, Colin and {Macherey}, Wolfgang and {Chen}, Zhifeng and {Wu}, Yonghui},
  title         = {{Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  year          = 2019,
  month         = jul,
  eid           = {arXiv:1907.05019},
  pages         = {arXiv:1907.05019},
  archiveprefix = {arXiv},
  eprint        = {1907.05019},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190705019A},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{baroni-etal-2009-the,
  abstract      = {This article introduces ukWaC, deWaC and itWaC, three very large corpora of English, German, and Italian built by web crawling, and describes the methodology and tools used in their construction. The corpora contain more than a billion words each, and are thus among the largest resources for the respective languages. The paper also provides an evaluation of their suitability for linguistic research, focusing on ukWaC and itWaC. A comparison in terms of lexical coverage with existing resources for the languages of interest produces encouraging results. Qualitative evaluation of ukWaC versus the British National Corpus was also conducted, so as to highlight differences in corpus composition (text types and subject matters). The article concludes with practical information about format and availability of corpora and tools.},
  author        = {Baroni, Marco and Bernardini, Silvia and Ferraresi, Adriano and Zanchetta, Eros},
  da            = {2009/09/01},
  date-added    = {2021-09-21 2:13:54 PM +0200},
  date-modified = {2021-09-21 2:13:54 PM +0200},
  doi           = {10.1007/s10579-009-9081-4},
  id            = {Baroni2009},
  isbn          = {1574-0218},
  journal       = {Language Resources and Evaluation},
  number        = {3},
  pages         = {209--226},
  title         = {The WaCky wide web: a collection of very large linguistically processed web-crawled corpora},
  ty            = {JOUR},
  url           = {https://doi.org/10.1007/s10579-009-9081-4},
  volume        = {43},
  year          = {2009},
  bdsk-url-1    = {https://doi.org/10.1007/s10579-009-9081-4}
}

@inproceedings{bechet-charton-2010-unsupervised,
  author    = {Bechet, Frederic and Charton, Eric},
  booktitle = {2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
  title     = {Unsupervised knowledge acquisition for Extracting Named Entities from speech},
  year      = {2010},
  volume    = {},
  number    = {},
  pages     = {5338-5341},
  doi       = {10.1109/ICASSP.2010.5494962}
}

@inproceedings{bender-etal-2021-on,
  author    = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  title     = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?{{\NotoEmoji \symbol{"1F99C}}}},
  year      = {2021},
  isbn      = {9781450383097},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3442188.3445922},
  doi       = {10.1145/3442188.3445922},
  abstract  = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages     = {610–623},
  numpages  = {14},
  location  = {Virtual Event, Canada},
  series    = {FAccT '21}
}

@webpage{benesty-2019-ner,
  lastchecked = {March 14, 2020},
  month       = {December},
  author      = {Benesty, Michaël},
  title       = {NER algo benchmark: spaCy, Flair, m-BERT and camemBERT on anonymizing French commercial legal cases},
  url         = {https://towardsdatascience.com/benchmark-ner-algorithm-d4ab01b2d4c3?source=friends_link&sk=5bffa2cb19997d1658479f18ce8cf6bb},
  year        = {2019},
  bdsk-url-1  = {https://towardsdatascience.com/benchmark-ner-algorithm-d4ab01b2d4c3?source=friends_link&sk=5bffa2cb19997d1658479f18ce8cf6bb}
}

@article{biber-1993-representativeness,
  author   = {Biber, Douglas},
  title    = {{Representativeness in Corpus Design}},
  journal  = {Literary and Linguistic Computing},
  volume   = {8},
  number   = {4},
  pages    = {243-257},
  year     = {1993},
  month    = {01},
  abstract = {{The present paper addresses a number of issues related to achieving ‘representativeness’ in linguistic corpus design, including: discussion of what it means to ‘represent’ a language, definition of the target population, stratified versus proportional sampling of a language, sampling within texts, and issues relating to the required sample size (number of texts) of a corpus. The paper distinguishes among various ways that linguistic features can be distributed within and across texts; it analyses the distributions of several particular features, and it discusses the implications of these distributions for corpus design.The paper argues that theoretical research should be prior in corpus design, to identify the situational parameters that distinguish among texts in a speech community, and to identify the types of linguistic features that will be analysed in the corpus. These theoretical considerations should be complemented by empirical investigations of linguistic variation in a pilot corpus of texts, as a basis for specific sampling decisions. The actual construction of a corpus would then proceed in cycles: the original design based on theoretical and pilot-study analyses, followed by collection of texts, followed by further empirical investigations of linguistic variation and revision of the design.}},
  issn     = {0268-1145},
  doi      = {10.1093/llc/8.4.243},
  url      = {https://doi.org/10.1093/llc/8.4.243},
  eprint   = {https://academic.oup.com/dsh/article-pdf/8/4/243/10889454/243.pdf}
}

@article{biderman-etal-2020-pitfalls,
  author        = {{Biderman}, Stella and {Scheirer}, Walter J.},
  title         = {{Pitfalls in Machine Learning Research: Reexamining the Development Cycle}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Machine Learning, Statistics - Methodology, Statistics - Machine Learning},
  year          = 2020,
  month         = nov,
  eid           = {arXiv:2011.02832},
  pages         = {arXiv:2011.02832},
  archiveprefix = {arXiv},
  eprint        = {2011.02832},
  primaryclass  = {cs.LG},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv201102832B},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{birhane-etal-2021-large,
  author    = {Birhane, Abeba and Prabhu, Vinay Uday},
  booktitle = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Large image datasets: A pyrrhic win for computer vision?},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {1536-1546},
  abstract  = {In this paper we investigate problematic practices and consequences of large scale vision datasets (LSVDs). We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class- wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both the society at large and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique their pros and cons. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation.},
  keywords  = {},
  doi       = {10.1109/WACV48630.2021.00158},
  issn      = {2642-9381},
  month     = {Jan}
}

@article{blumofe-etal-1999-scheduling,
  author     = {Blumofe, Robert D. and Leiserson, Charles E.},
  title      = {Scheduling Multithreaded Computations by Work Stealing},
  year       = {1999},
  issue_date = {Sept. 1999},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {46},
  number     = {5},
  issn       = {0004-5411},
  url        = {https://doi.org/10.1145/324133.324234},
  doi        = {10.1145/324133.324234},
  abstract   = {This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured)
                multithreaded computations on parallel computers. A popular and practical method of
                scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which
                processors needing work steal computational threads from other processors. In this
                paper, we give the first provably good work-stealing scheduler for multithreaded computations
                with dependencies.Specifically, our analysis shows that the expected time to execute
                a fully strict computation on P processors using our work-stealing scheduler is T1/P
                + O(T ∞ , where T1 is the minimum serial execution time of the multithreaded computation
                and (T ∞ is the minimum execution time with an infinite number of processors. Moreover,
                the space required by the execution is at most S1P, where S1 is the minimum serial
                space requirement. We also show that the expected total communication of the algorithm
                is at most O(PT ∞( 1 + nd)Smax), where Smax is the size of the largest activation
                record of any thread and nd is the maximum number of times that any thread synchronizes
                with its parent. This communication bound justifies the folk wisdom that work-stealing
                schedulers are more communication efficient than their work-sharing counterparts.
                All three of these bounds are existentially optimal to within a constant factor.},
  journal    = {J. ACM},
  month      = sep,
  pages      = {720–748},
  numpages   = {29},
  keywords   = {multithreading, work stealing, critical-path length, randomized algorithm, multiprocessor, thread scheduling}
}

@inproceedings{bnc-2007-the,
  author    = {BNC Consortium and others},
  booktitle = {The British National Corpus, version 3 - BNC XML Edition},
  title     = {520 million words, 1990-present},
  url       = {http://www.natcorp.ox.ac.uk/},
  year      = 2007
}

@inproceedings{bonami-etal-2015-implicative,
  author    = {Olivier Bonami and
               Sacha Beniamine},
  editor    = {Vito Pirrelli and
               Claudia Marzi and
               Marcello Ferro},
  title     = {Implicative structure and joint predictiveness},
  booktitle = {Proceedings of the NetWordS Final Conference on Word Knowledge and
               Word Usage: Representations and Processes in the Mental Lexicon, Pisa,
               Italy, March 30 - April 1, 2015},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {1347},
  pages     = {4--9},
  publisher = {CEUR-WS.org},
  year      = {2015},
  url       = {http://ceur-ws.org/Vol-1347/paper01.pdf},
  timestamp = {Tue, 28 Jul 2020 15:05:48 +0200},
  biburl    = {https://dblp.org/rec/conf/networds/BonamiB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{brown-etal-2020-language,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{buch-kromann-2003-the,
  title     = {The Danish dependency treebank and the dtag treebank tool},
  author    = {Buch-Kromann, Matthias},
  booktitle = {2nd Workshop on Treebanks and Linguistic Theories (TLT), Sweden},
  pages     = {217--220},
  year      = {2003}
}

@misc{callan-etal-2009-clueweb09,
  title  = {Clueweb09 data set},
  author = {Callan, Jamie and Hoy, Mark and Yoo, Changkuk and Zhao, Le},
  year   = {2009}
}

@article{caswell-etal-2021-quality,
  author        = {{Caswell}, Isaac and {Kreutzer}, Julia and {Wang}, Lisa and {Wahab}, Ahsan and {van Esch}, Daan and {Ulzii-Orshikh}, Nasanbayar and {Tapo}, Allahsera and {Subramani}, Nishant and {Sokolov}, Artem and {Sikasote}, Claytone and {Setyawan}, Monang and {Sarin}, Supheakmungkol and {Samb}, Sokhar and {Sagot}, Beno{\^\i}t and {Rivera}, Clara and {Rios}, Annette and {Papadimitriou}, Isabel and {Osei}, Salomey and {Ortiz Su{\'a}rez}, Pedro Javier and {Orife}, Iroro and {Ogueji}, Kelechi and {Niyongabo}, Rubungo Andre and {Nguyen}, Toan Q. and {M{\"u}ller}, Mathias and {M{\"u}ller}, Andr{\'e} and {Hassan Muhammad}, Shamsuddeen and {Muhammad}, Nanda and {Mnyakeni}, Ayanda and {Mirzakhalov}, Jamshidbek and {Matangira}, Tapiwanashe and {Leong}, Colin and {Lawson}, Nze and {Kudugunta}, Sneha and {Jernite}, Yacine and {Jenny}, Mathias and {Firat}, Orhan and {Dossou}, Bonaventure F.~P. and {Dlamini}, Sakhile and {de Silva}, Nisansa and {{\c{C}}abuk Ball{\i}}, Sakine and {Biderman}, Stella and {Battisti}, Alessia and {Baruwa}, Ahmed and {Bapna}, Ankur and {Baljekar}, Pallavi and {Abebe Azime}, Israel and {Awokoya}, Ayodele and {Ataman}, Duygu and {Ahia}, Orevaoghene and {Ahia}, Oghenefego and {Agrawal}, Sweta and {Adeyemi}, Mofetoluwa},
  title         = {{Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  year          = 2021,
  month         = mar,
  eid           = {arXiv:2103.12028},
  pages         = {arXiv:2103.12028},
  archiveprefix = {arXiv},
  eprint        = {2103.12028},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2021arXiv210312028C},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{chan-etal-2019-german,
  author       = {Branden Chan and Timo M{\"o}ller and Malte Pietsch and Tanay Soni and Chin Man Yeung},
  howpublished = {\url{https://deepset.ai/german-bert}},
  title        = {German BERT},
  year         = {2019}
}

@article{chanier-etal-2014-the,
  title       = {{The CoMeRe corpus for French: structuring and annotating heterogeneous CMC genres}},
  author      = {Chanier, Thierry and Poudat, C{\'e}line and Sagot, Beno{\^i}t and Antoniadis, Georges and Wigham, Ciara R. and Hriba, Linda and Longhi, Julien and Seddah, Djam{\'e}},
  url         = {https://halshs.archives-ouvertes.fr/halshs-00953507},
  note        = {Final version to Special Issue of JLCL (Journal of Language Technology and Computational Linguistics (JLCL, http://jlcl.org/): BUILDING AND ANNOTATING CORPORA OF COMPUTER-MEDIATED DISCOURSE: Issues and Challenges at the Interface of Corpus and Computational Linguistics (ed. by Michael Bei{\ss}wenger, Nelleke Oostdijk, Angelika Storrer \& Henk van den Heuvel)},
  journal     = {{Journal for language technology and computational linguistics}},
  publisher   = {{GSCL (Gesellschaft f{\"u}r Sprachtechnologie und Computerlinguistik) }},
  volume      = {29},
  number      = {2},
  pages       = {1-30},
  year        = {2014},
  keywords    = {Computer Mediated Communication ; CMC ; CoMeRe ; corpus},
  pdf         = {https://halshs.archives-ouvertes.fr/halshs-00953507v2/file/cmr-article-jlcl-v140912-hal.pdf},
  hal_id      = {halshs-00953507},
  hal_version = {v2}
}

@inproceedings{clark-etal-2020-electra,
  title     = {ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author    = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=r1xMH1BtvB}
}

@inproceedings{conneau-lample-2019-cross,
  author    = {Conneau, Alexis and Lample, Guillaume},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Cross-lingual Language Model Pretraining},
  url       = {https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@inproceedings{dai-le-2015-semi,
  author    = {Dai, Andrew M and Le, Quoc V},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Semi-supervised Sequence Learning},
  url       = {https://proceedings.neurips.cc/paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf},
  volume    = {28},
  year      = {2015}
}

@article{davies-2009-the,
  author    = {Davies, Mark},
  title     = {The 385+ million word Corpus of Contemporary American English (1990–2008+): Design, architecture, and linguistic insights},
  journal   = {International Journal of Corpus Linguistics},
  year      = {2009},
  volume    = {14},
  number    = {2},
  pages     = {159-190},
  doi       = {https://doi.org/10.1075/ijcl.14.2.02dav},
  url       = {https://www.jbe-platform.com/content/journals/10.1075/ijcl.14.2.02dav},
  publisher = {John Benjamins},
  issn      = {1384-6655},
  type      = {Journal Article},
  keywords  = {corpus},
  keywords  = {genres},
  keywords  = {American English},
  keywords  = {phrasal verbs},
  keywords  = {diachronic},
  keywords  = {relational databases},
  abstract  = {The Corpus of Contemporary American English (COCA), which was released online in early 2008, is the first large and diverse corpus of American English. In this paper, we first discuss the design of the corpus — which contains more than 385 million words from 1990–2008 (20 million words each year), balanced between spoken, fiction, popular magazines, newspapers, and academic journals. We also discuss the unique relational databases architecture, which allows for a wide range of queries that are not available (or are quite difficult) with other architectures and interfaces. To conclude, we consider insights from the corpus on a number of cases of genre-based variation and recent linguistic variation, including an extended analysis of phrasal verbs in contemporary American English.}
}

@article{davies-2010-the,
  author   = {Davies, Mark},
  title    = {{The Corpus of Contemporary American English as the first reliable monitor corpus of English}},
  journal  = {Literary and Linguistic Computing},
  volume   = {25},
  number   = {4},
  pages    = {447-464},
  year     = {2010},
  month    = {10},
  abstract = {{The Corpus of Contemporary American English is the first large, genre-balanced corpus of any language, which has been designed and constructed from the ground up as a ‘monitor corpus’, and which can be used to accurately track and study recent changes in the language. The 400 million words corpus is evenly divided between spoken, fiction, popular magazines, newspapers, and academic journals. Most importantly, the genre balance stays almost exactly the same from year to year, which allows it to accurately model changes in the ‘real world’. After discussing the corpus design, we provide a number of concrete examples of how the corpus can be used to look at recent changes in English, including morphology (new suffixes –friendly and –gate), syntax (including prescriptive rules, quotative like, so not ADJ, the get passive, resultatives, and verb complementation), semantics (such as changes in meaning with web, green, or gay), and lexis––including word and phrase frequency by year, and using the corpus architecture to produce lists of all words that have had large shifts in frequency between specific historical periods.}},
  issn     = {0268-1145},
  doi      = {10.1093/llc/fqq018},
  url      = {https://doi.org/10.1093/llc/fqq018},
  eprint   = {https://academic.oup.com/dsh/article-pdf/25/4/447/6191556/fqq018.pdf}
}

@inproceedings{desrochers-etal-2016-a,
  author    = {Desrochers, Spencer and Paradis, Chad and Weaver, Vincent M.},
  title     = {A Validation of DRAM RAPL Power Measurements},
  year      = {2016},
  isbn      = {9781450343053},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2989081.2989088},
  doi       = {10.1145/2989081.2989088},
  abstract  = {Recent Intel processors support the Running Average Power Level (RAPL) interface, which among other things provides estimated energy measurements for the CPUs, integrated GPU, and DRAM. These measurements are easily accessible by the user, and can be gathered by a wide variety of tools, including the Linux perf_event interface. This allows unprecedented easy access to energy information when designing and optimizing energy-aware code.While greatly useful, on most systems these RAPL measurements are estimated values, generated on the fly by an on-chip energy model. The values are not documented well, and the results (especially the DRAM results) have undergone only limited validation.We validate the DRAM RAPL results on both desktop and server Haswell machines, with multiple types of DDR3 and DDR4 memory. We instrument the hardware to gather actual power measurements and compare them to the RAPL values returned via Linux perf_event. We describe the many challenges encountered when instrumenting systems for detailed power measurement.We find that the RAPL results match overall energy and power trends, usually by a constant power offset. The results match best when the DRAM is being heavily utilized, but do not match as well in cases where the system is idle, or when an integrated GPU is using the memory.We also verify that Haswell server machines produce more accurate results, as they include actual power measurements gathered through the integrated voltage regulator.},
  booktitle = {Proceedings of the Second International Symposium on Memory Systems},
  pages     = {455–470},
  numpages  = {16},
  keywords  = {DRAM Power, RAPL, DRAM Energy},
  location  = {Alexandria, VA, USA},
  series    = {MEMSYS '16}
}

@inproceedings{dozat-manning-2017-deep,
  author    = {Timothy Dozat and
               Christopher D. Manning},
  title     = {Deep Biaffine Attention for Neural Dependency Parsing},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Hk95PK9le},
  timestamp = {Thu, 25 Jul 2019 14:25:56 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/DozatM17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{emerick-2018-list,
  title  = {List of dirty naughty obscene and otherwise bad words},
  author = {Emerick, Jacob},
  year   = {2018},
  url    = {https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words}
}

@article{fan-etal-2020-beyond,
  author        = {{Fan}, Angela and {Bhosale}, Shruti and {Schwenk}, Holger and {Ma}, Zhiyi and {El-Kishky}, Ahmed and {Goyal}, Siddharth and {Baines}, Mandeep and {Celebi}, Onur and {Wenzek}, Guillaume and {Chaudhary}, Vishrav and {Goyal}, Naman and {Birch}, Tom and {Liptchinsky}, Vitaliy and {Edunov}, Sergey and {Grave}, Edouard and {Auli}, Michael and {Joulin}, Armand},
  title         = {{Beyond English-Centric Multilingual Machine Translation}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  year          = 2020,
  month         = oct,
  eid           = {arXiv:2010.11125},
  pages         = {arXiv:2010.11125},
  archiveprefix = {arXiv},
  eprint        = {2010.11125},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv201011125F},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{galliano-etal-2005-the,
  author    = {Sylvain Galliano and Edouard Geoffrois and Djamel Mostefa and Khalid Choukri and Jean-François Bonastre and Guillaume Gravier},
  title     = {{The ESTER phase II evaluation campaign for the rich transcription of French broadcast news}},
  year      = 2005,
  booktitle = {Proc. Interspeech 2005},
  pages     = {1149--1152},
  doi       = {10.21437/Interspeech.2005-441}
}

@inproceedings{galliano-etal-2009-the,
  author    = {Sylvain Galliano and Guillaume Gravier and Laura Chaubard},
  title     = {{The ester 2 evaluation campaign for the rich transcription of French radio broadcasts}},
  year      = 2009,
  booktitle = {Proc. Interspeech 2009},
  pages     = {2583--2586},
  doi       = {10.21437/Interspeech.2009-680}
}

@article{gao-etal-2020-the,
  author        = {{Gao}, Leo and {Biderman}, Stella and {Black}, Sid and {Golding}, Laurence and {Hoppe}, Travis and {Foster}, Charles and {Phang}, Jason and {He}, Horace and {Thite}, Anish and {Nabeshima}, Noa and {Presser}, Shawn and {Leahy}, Connor},
  title         = {{The Pile: An 800GB Dataset of Diverse Text for Language Modeling}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  year          = 2020,
  month         = dec,
  eid           = {arXiv:2101.00027},
  pages         = {arXiv:2101.00027},
  archiveprefix = {arXiv},
  eprint        = {2101.00027},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2021arXiv210100027G},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{gebru-etal-2018-datasheets,
  author        = {{Gebru}, Timnit and {Morgenstern}, Jamie and {Vecchione}, Briana and {Wortman Vaughan}, Jennifer and {Wallach}, Hanna and {Daum{\'e}}, Hal, III and {Crawford}, Kate},
  title         = {{Datasheets for Datasets}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Databases, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  year          = 2018,
  month         = mar,
  eid           = {arXiv:1803.09010},
  pages         = {arXiv:1803.09010},
  archiveprefix = {arXiv},
  eprint        = {1803.09010},
  primaryclass  = {cs.DB},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180309010G},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{hill-etal-2016-the,
  author    = {Felix Hill and
               Antoine Bordes and
               Sumit Chopra and
               Jason Weston},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {The Goldilocks Principle: Reading Children's Books with Explicit Memory
               Representations},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.02301},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HillBCW15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hochreiter-schmidhuber-1997-long,
  author   = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  title    = {{Long Short-Term Memory}},
  journal  = {Neural Computation},
  volume   = {9},
  number   = {8},
  pages    = {1735-1780},
  year     = {1997},
  month    = {11},
  abstract = {{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}},
  issn     = {0899-7667},
  doi      = {10.1162/neco.1997.9.8.1735},
  url      = {https://doi.org/10.1162/neco.1997.9.8.1735},
  eprint   = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf}
}

@article{holland-etal-2018-the,
  author        = {{Holland}, Sarah and {Hosny}, Ahmed and {Newman}, Sarah and {Joseph}, Joshua and {Chmielinski}, Kasia},
  title         = {{The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Databases, Computer Science - Computers and Society},
  year          = 2018,
  month         = may,
  eid           = {arXiv:1805.03677},
  pages         = {arXiv:1805.03677},
  archiveprefix = {arXiv},
  eprint        = {1805.03677},
  primaryclass  = {cs.DB},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180503677H},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{hu-etal-2020-xtreme,
  title     = {{XTREME}: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation},
  author    = {Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {4411--4421},
  year      = {2020},
  editor    = {III, Hal Daumé and Singh, Aarti},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/hu20b/hu20b.pdf},
  url       = {https://proceedings.mlr.press/v119/hu20b.html},
  abstract  = {Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We will release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.}
}

@article{huang-etal-2015-bidirectional,
  author        = {{Huang}, Zhiheng and {Xu}, Wei and {Yu}, Kai},
  title         = {{Bidirectional LSTM-CRF Models for Sequence Tagging}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  year          = 2015,
  month         = aug,
  eid           = {arXiv:1508.01991},
  pages         = {arXiv:1508.01991},
  archiveprefix = {arXiv},
  eprint        = {1508.01991},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2015arXiv150801991H},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{joulin-etal-2016-fasttext,
  author        = {{Joulin}, Armand and {Grave}, Edouard and {Bojanowski}, Piotr and {Douze}, Matthijs and {J{\'e}gou}, H{\'e}rve and {Mikolov}, Tomas},
  title         = {{FastText.zip: Compressing text classification models}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  year          = 2016,
  month         = dec,
  eid           = {arXiv:1612.03651},
  pages         = {arXiv:1612.03651},
  archiveprefix = {arXiv},
  eprint        = {1612.03651},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2016arXiv161203651J},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{kingma-ba-2015-adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

 @inproceedings{lafferty-etal-2001-conditional,
  author    = {Lafferty, John D. and McCallum, Andrew and Pereira, Fernando C. N.},
  title     = {Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data},
  year      = {2001},
  isbn      = {1558607781},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address   = {San Francisco, CA, USA},
  booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
  pages     = {282–289},
  numpages  = {8},
  series    = {ICML '01}
}

@inproceedings{lan-etal-2020-albert,
  author    = {Zhenzhong Lan and
               Mingda Chen and
               Sebastian Goodman and
               Kevin Gimpel and
               Piyush Sharma and
               Radu Soricut},
  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
               Representations},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=H1eA7AEtvS},
  timestamp = {Thu, 07 May 2020 17:11:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LanCGGSS20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{liu-etal-2019-roberta,
  author        = {{Liu}, Yinhan and {Ott}, Myle and {Goyal}, Naman and {Du}, Jingfei and {Joshi}, Mandar and {Chen}, Danqi and {Levy}, Omer and {Lewis}, Mike and {Zettlemoyer}, Luke and {Stoyanov}, Veselin},
  title         = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  year          = 2019,
  month         = jul,
  eid           = {arXiv:1907.11692},
  pages         = {arXiv:1907.11692},
  archiveprefix = {arXiv},
  eprint        = {1907.11692},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190711692L},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{mikolov-etal-2013-distributed,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  url       = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
  volume    = {26},
  year      = {2013}
}

@misc{nivre-etal-2018-universal,
  title     = {Universal Dependencies 2.2},
  author    = {Nivre, Joakim and Abrams, Mitchell and Agi{\'c}, {\v Z}eljko and Ahrenberg, Lars and Antonsen, Lene and Aranzabe, Maria Jesus and Arutie, Gashaw and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Augustinus, Liesbeth and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Barbu Mititelu, Verginica and Bauer, John and Bellato, Sandra and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Biagetti, Erica and Bick, Eckhard and Blokland, Rogier and Bobicev, Victoria and B{\"o}rstell, Carl and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Boyd, Adriane and Burchardt, Aljoscha and Candito, Marie and Caron, Bernard and Caron, Gauthier and Cebiro{\u g}lu Eryi{\u g}it, G{\"u}l{\c s}en and Celano, Giuseppe G. A. and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and Chun, Jayeol and Cinkov{\'a}, Silvie and Collomb, Aur{\'e}lie and {\c C}{\"o}ltekin, {\c C}a{\u g}r{\i} and Connor, Miriam and Courtin, Marine and Davidson, Elizabeth and de Marneffe, Marie-Catherine and de Paiva, Valeria and Diaz de Ilarraza, Arantza and Dickerson, Carly and Dirix, Peter and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Dwivedi, Puneet and Eli, Marhaba and Elkahky, Ali and Ephrem, Binyam and Erjavec, Toma{\v z} and Etienne, Aline and Farkas, Rich{\'a}rd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\'a}udia and Gajdo{\v s}ov{\'a}, Katar{\'{\i}}na and Galbraith, Daniel and Garcia, Marcos and G{\"a}rdenfors, Moa and Gerdes, Kim and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\"o}k{\i}rmak, Memduh and Goldberg, Yoav and G{\'o}mez Guinovart, Xavier and Gonz{\'a}les Saavedra, Berta and Grioni,
               Matias and Gr{\=
               u}z{\={\i}}tis, Normunds and Guillaume, Bruno and Guillot-Barbance, C{\'e}line and Habash, Nizar and Haji{\v c}, Jan and Haji{\v c} jr., Jan and H{\`a} M{\~y}, Linh and Han, Na-Rae and Harris, Kim and Haug, Dag and Hladk{\'a}, Barbora and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Hociung, Florinel and Hohle, Petter and Hwang, Jena and Ion, Radu and Irimia, Elena and Jel{\'{\i}}nek, Tom{\'a}{\v s} and Johannsen, Anders and J{\o}rgensen, Fredrik and Ka{\c s}{\i}kara, H{\"u}ner and Kahane, Sylvain and Kanayama, Hiroshi and Kanerva, Jenna and Kayadelen, Tolga and Kettnerov{\'a}, V{\'a}clava and Kirchner, Jesse and Kotsyba, Natalia and Krek, Simon and Kwak, Sookyoung and Laippala, Veronika and Lambertino, Lorenzo and Lando, Tatiana and Larasati, Septina Dian and Lavrentiev, Alexei and Lee, John and L{\^e} H{\`{\^o}}ng, Phương and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Li, Keying and Lim, {KyungTae} and Ljube{\v s}i{\'c}, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and Mare{\v c}ek, David and Marheinecke, Katrin and Mart{\'{\i}}nez Alonso, H{\'e}ctor and Martins, Andr{\'e} and Ma{\v s}ek, Jan and Matsumoto, Yuji and {McDonald}, Ryan and Mendon{\c c}a, Gustavo and Miekka, Niko and Missil{\"a}, Anna and Mititelu, C{\u a}t{\u a}lin and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Shinsuke and Mortensen, Bjartur and Moskalevskyi, Bohdan and Muischnek, Kadri and Murawaki, Yugo and M{\"u}{\"u}risep, Kaili and Nainwani, Pinkey and Navarro Hor{\~n}iacek, Juan Ignacio and Nedoluzhko,
               Anna and Ne{\v s}pore-B{\=e}rzkalne, Gunta and Nguy{\~{\^e}}n Th{\d i}, Lương and Nguy{\~{\^e}}n Th{\d i} Minh, Huy{\`{\^e}}n and Nikolaev, Vitaly and Nitisaroj, Rattima and Nurmi, Hanna and Ojala, Stina and Ol{\'u}{\`o}kun, Ad{\'e}day{\d o}̀ and Omura, Mai and Osenova, Petya and {\"O}stling, Robert and {\O}vrelid, Lilja and Partanen, Niko and Pascual, Elena and Passarotti, Marco and Patejuk, Agnieszka and Peng, Siyao and Perez, Cenel-Augusto and Perrier, Guy and Petrov, Slav and Piitulainen, Jussi and Pitler, Emily and Plank, Barbara and Poibeau, Thierry and Popel, Martin and Pretkalni{\c n}a, Lauma and Pr{\'e}vost, Sophie and Prokopidis, Prokopis and Przepi{\'o}rkowski, Adam and Puolakainen, Tiina and Pyysalo, Sampo and R{\"a}{\"a}bis, Andriela and Rademaker, Alexandre and Ramasamy, Loganathan and Rama, Taraka and Ramisch, Carlos and Ravishankar, Vinit and Real, Livy and Reddy, Siva and Rehm, Georg and Rie{\ss}ler, Michael and Rinaldi, Larissa and Rituma, Laura and Rocha, Luisa and Romanenko, Mykhailo and Rosa, Rudolf and Rovati, Davide and Roșca, Valentin and Rudina, Olga and Sadde, Shoval and Saleh, Shadi and Samard{\v z}i{\'c}, Tanja and Samson, Stephanie and Sanguinetti,
               Manuela and Saul{\={\i}}te, Baiba and Sawanakunanon, Yanin and Schneider, Nathan and Schuster, Sebastian and Seddah, Djam{\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shen, Mo and Shimada, Atsuko and Shohibussirri, Muh and Sichinava, Dmitry and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\'o}, Katalin and {\v S}imkov{\'a}, M{\'a}ria and Simov, Kiril and Smith, Aaron and Soares-Bastos, Isabela and Stella, Antonio and Straka, Milan and Strnadov{\'a}, Jana and Suhr, Alane and Sulubacak, Umut and Sz{\'a}nt{\'o}, Zsolt and Taji, Dima and Takahashi, Yuta and Tanaka, Takaaki and Tellier, Isabelle and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Ure{\v s}ov{\'a}, Zde{\v n}ka and Uria, Larraitz and Uszkoreit, Hans and Vajjala, Sowmya and van Niekerk, Daniel and van Noord, Gertjan and Varga, Viktor and Vincze, Veronika and Wallin, Lars and Washington, Jonathan North and Williams, Seyi and Wir{\'e}n, Mats and Woldemariam, Tsegay and Wong, Tak-sum and Yan, Chunxiao and Yavrumyan, Marat M. and Yu, Zhuoran and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Zeldes, Amir and Zeman, Daniel and Zhang, Manying and Zhu, Hanzhi},
  url       = {http://hdl.handle.net/11234/1-2837},
  note      = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
  copyright = {Licence Universal Dependencies v2.2},
  year      = {2018}
}

@inproceedings{nouvel-etal-2014-pattern,
  author    = {Nouvel, Damien
               and Antoine, Jean-Yves
               and Friburger, Nathalie},
  editor    = {Vetulani, Zygmunt
               and Mariani, Joseph},
  title     = {Pattern Mining for Named Entity Recognition},
  booktitle = {Human Language Technology Challenges for Computer Science and Linguistics},
  year      = {2014},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {226--237},
  abstract  = {Many evaluation campaigns have shown that knowledge-based and data-driven approaches remain equally competitive for Named Entity Recognition. Our research team has developed CasEN, a symbolic system based on finite state transducers, which achieved promising results during the Ester2 French-speaking evaluation campaign. Despite these encouraging results, manually extending the coverage of such a hand-crafted system is a difficult task. In this paper, we present a novel approach based on pattern mining for NER and to supplement our system's knowledge base. The system, mXS, exhaustively searches for hierarchical sequential patterns, that aim at detecting Named Entity boundaries. We assess their efficiency by using such patterns in a standalone mode and in combination with our existing system.},
  isbn      = {978-3-319-08958-4}
}

@article{nozza-etal-2020-what,
  author        = {{Nozza}, Debora and {Bianchi}, Federico and {Hovy}, Dirk},
  title         = {{What the [MASK]? Making Sense of Language-Specific BERT Models}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  year          = 2020,
  month         = mar,
  eid           = {arXiv:2003.02912},
  pages         = {arXiv:2003.02912},
  archiveprefix = {arXiv},
  eprint        = {2003.02912},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2020arXiv200302912N},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{ortiz-suarez-etal-2019-asynchronous,
  author    = {Pedro Javier {Ortiz Su{\'a}rez} and Beno{\^i}t Sagot and Laurent Romary},
  booktitle = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019},
  title     = {Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures},
  editor    = {Piotr Bański and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc Kupietz and Harald L{\"u}ngen and Caroline Iliadi},
  publisher = {Leibniz-Institut f{\"u}r Deutsche Sprache},
  address   = {Mannheim},
  doi       = {10.14618/ids-pub-9021},
  url       = {http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215},
  pages     = {9 -- 16},
  year      = {2019},
  abstract  = {Common Crawl is a considerably large, heterogeneous multilingual corpus comprised of crawled documents from the internet, surpassing 20TB of data and distributed as a set of more than 50 thousand plain text files where each contains many documents written in a wide variety of languages. Even though each document has a metadata block associated to it, this data lacks any information about the language in which each document is written, making it extremely difficult to use Common Crawl for monolingual applications. We propose a general, highly parallel, multithreaded pipeline to clean and classify Common Crawl by language; we specifically design it so that it runs efficiently on medium to low resource infrastructures where I/O speeds are the main constraint. We develop the pipeline so that it can be easily reapplied to any kind of heterogeneous corpus and so that it can be parameterised to a wide range of infrastructures. We also distribute a 6.3TB version of Common Crawl, filtered, classified by language, shuffled at line level in order to avoid copyright issues, and ready to be used for NLP applications.},
  language  = {en}
}

@article{parker-etal-2011-english,
  title   = {English gigaword fifth edition, linguistic data consortium},
  author  = {Parker, Robert and Graff, David and Kong, Junbo and Chen, Ke and Maeda, Kazuaki},
  journal = {Technical report, Technical Report. Linguistic Data Consortium},
  address = {Philadelphia, Tech. Rep.},
  year    = {2011}
}

@techreport{phillips-etal-2005-tags,
  number      = {draft-phillips-langtags-10},
  type        = {Internet-Draft},
  institution = {Internet Engineering Task Force},
  publisher   = {Internet Engineering Task Force},
  note        = {Work in Progress},
  url         = {https://datatracker.ietf.org/doc/html/draft-phillips-langtags-10},
  author      = {Addison Phillips and Mark Davis},
  title       = {{Tags for Identifying Languages}},
  pagetotal   = 45,
  year        = 2005,
  month       = feb,
  day         = 15,
  abstract    = {This document describes the structure, content, construction, and semantics of language tags for use in cases where it is desirable to indicate the language used in an information object. It also describes how to register values for use in language tags and a construct for matching such language tags, including user defined extensions for private interchange. This document replaces RFC 3066 (which replaced RFC 1766).}
}

@article{radford-etal-2018-improving,
  title   = {Improving language understanding by generative pre-training},
  author  = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal = {OpenAI Blog},
  year    = {2018}
}

@article{radford-etal-2019-language,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal = {OpenAI Blog},
  volume  = {1},
  pages   = {8},
  year    = {2019}
}

@article{raffel-etal-2020-exploring,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@article{rosset-etal-2005-interaction,
  author  = {Rosset, Sophie and Illouz, Gabriel and Max, Aur\'{e}lien},
  journal = {Traitement Automatique des Langues},
  number  = {3},
  pages   = {155--179},
  title   = {{I}nteraction et recherche d'information~: le projet {Ritel}},
  volume  = {46},
  year    = {2005}
}

@techreport{rosset-etal-2011-entites,
  title                    = {Entit\'es Nomm\'ees Structur\'ees : guide d'annotation Quaero},
  author                   = {Rosset, Sophie and Grouin, Cyril and Zweigenbaum, Pierre},
  year                     = {2011},
  note                     = {autres},
  edition                  = {Notes et Documents LIMSI N° : 2011-04},
  groups                   = {Communications sans acte},
  keywords                 = {named entity definition},
  organization             = {LIMSI-CNRS},
  institution              = {LIMSI-CNRS},
  owner                    = {rosset},
  timestamp                = {2011.12.02},
  x-international-audience = {#no#}
}

@inbook{sanguinetti-Bosco-2015-parttut,
  author    = {Sanguinetti, Manuela
               and Bosco, Cristina},
  title     = {PartTUT: The Turin University Parallel Treebank},
  booktitle = {Harmonization and Development of Resources and Tools for Italian Natural Language Processing within the PARLI Project},
  year      = {2015},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {51--69},
  abstract  = {In this paper, we introduce an ongoing project for the development of a parallel treebank for Italian, English and French. The treebank is annotated in a dependency format, namely the one designed in the Turin University Treebank (TUT), hence the choice to call such new resource Par(allel)TUT. The project aims at creating a resource which can be useful in particular for translation research. Therefore, beyond constantly enriching the treebank with new and heterogeneous data, so as to build a dynamic and balanced multilingual treebank, the current stage of the project is devoted to the design of a tool for the alignment of data, which takes into account syntactic knowledge as annotated in this kind of resource. The paper focuses in particular on the study of translational divergences and their implications for the development of the alignment tool. The paper provides an overview of the treebank, with its current content and the peculiarities of the annotation format, the description of the classes of translational divergences which could be encountered in the treebank, together with a proposal for their alignment.},
  isbn      = {978-3-319-14206-7},
  doi       = {10.1007/978-3-319-14206-7_3},
  url       = {https://doi.org/10.1007/978-3-319-14206-7_3}
}

@inproceedings{schuster-nakajima-2012-japanese,
  author    = {Schuster, Mike and Nakajima, Kaisuke},
  booktitle = {2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Japanese and Korean voice search},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {5149-5152},
  doi       = {10.1109/ICASSP.2012.6289079}
}

@article{skitka-etal-1999-does,
  title    = {Does automation bias decision-making?},
  journal  = {International Journal of Human-Computer Studies},
  volume   = {51},
  number   = {5},
  pages    = {991-1006},
  year     = {1999},
  issn     = {1071-5819},
  doi      = {https://doi.org/10.1006/ijhc.1999.0252},
  url      = {https://www.sciencedirect.com/science/article/pii/S1071581999902525},
  author   = {Linda J. Skitka And Kathleen L. Mosier And Mark Burdick},
  abstract = {Computerized system monitors and decision aids are increasingly common additions to critical decision-making contexts such as intensive care units, nuclear power plants and aircraft cockpits. These aids are introduced with the ubiquitous goal of “reducing human error”. The present study compared error rates in a simulated flight task with and without a computer that monitored system states and made decision recommendations. Participants in non-automated settings out-performed their counterparts with a very but not perfectly reliable automated aid on a monitoring task. Participants with an aid made errors of omission (missed events when not explicitly prompted about them by the aid) and commission (did what an automated aid recommended, even when it contradicted their training and other 100% valid and available indicators). Possible causes and consequences of automation bias are discussed}
}

@inproceedings{stern-sagot-2010-resources,
  title       = {{Resources for Named Entity Recognition and Resolution in News Wires}},
  author      = {Stern, Rosa and Sagot, Beno{\^i}t},
  url         = {https://hal.inria.fr/inria-00521240},
  booktitle   = {{Entity 2010 Workshop at LREC 2010}},
  address     = {Valletta, Malta},
  year        = {2010},
  month       = May,
  pdf         = {https://hal.inria.fr/inria-00521240/file/entity10np.pdf},
  hal_id      = {inria-00521240},
  hal_version = {v1}
}

@article{straka-strakova-2019-evaluating,
  author        = {{Straka}, Milan and {Strakov{\'a}}, Jana and {Haji{\v{c}}}, Jan},
  title         = {{Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  year          = 2019,
  month         = aug,
  eid           = {arXiv:1908.07448},
  pages         = {arXiv:1908.07448},
  archiveprefix = {arXiv},
  eprint        = {1908.07448},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190807448S},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{sun-etal-2019-mithralabel,
  author    = {Sun, Chenkai and Asudeh, Abolfazl and Jagadish, H. V. and Howe, Bill and Stoyanovich, Julia},
  title     = {MithraLabel: Flexible Dataset Nutritional Labels for Responsible Data Science},
  year      = {2019},
  isbn      = {9781450369763},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3357384.3357853},
  doi       = {10.1145/3357384.3357853},
  abstract  = {Using inappropriate datasets for data science tasks can be harmful, especially for
               applications that impact humans. Targeting data ethics, we demonstrate MithraLabel,
               a system for generating task-specific information about a dataset, in the form of
               a set of visual widgets, as a flexible "nutritional label" that provides a user with
               information to determine the fitness of the dataset for the task at hand.},
  booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  pages     = {2893–2896},
  numpages  = {4},
  keywords  = {fairness, machine bias, data ethics, accountability, transparency},
  location  = {Beijing, China},
  series    = {CIKM '19}
}

@article{taylor-1953-cloze,
  author   = {Wilson L. Taylor},
  title    = {“Cloze Procedure”: A New Tool for Measuring Readability},
  journal  = {Journalism Quarterly},
  volume   = {30},
  number   = {4},
  pages    = {415-433},
  year     = {1953},
  doi      = {10.1177/107769905303000401},
  url      = {https://doi.org/10.1177/107769905303000401},
  eprint   = {https://doi.org/10.1177/107769905303000401},
  abstract = {Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which “cloze procedure” results are compared with those of two readability formulas.}
}

@article{trinh-le-2018-a,
  author        = {{Trinh}, Trieu H. and {Le}, Quoc V.},
  title         = {{A Simple Method for Commonsense Reasoning}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
  year          = 2018,
  month         = jun,
  eid           = {arXiv:1806.02847},
  pages         = {arXiv:1806.02847},
  archiveprefix = {arXiv},
  eprint        = {1806.02847},
  primaryclass  = {cs.AI},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180602847T},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{vaswani-etal-2017-attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@article{virtanen-etal-2019-multilingual,
  author        = {{Virtanen}, Antti and {Kanerva}, Jenna and {Ilo}, Rami and {Luoma}, Jouni and {Luotolahti}, Juhani and {Salakoski}, Tapio and {Ginter}, Filip and {Pyysalo}, Sampo},
  title         = {{Multilingual is not enough: BERT for Finnish}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  year          = 2019,
  month         = dec,
  eid           = {arXiv:1912.07076},
  pages         = {arXiv:1912.07076},
  archiveprefix = {arXiv},
  eprint        = {1912.07076},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv191207076V},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{wolf-etal-2019-huggingface,
  author        = {{Wolf}, Thomas and {Debut}, Lysandre and {Sanh}, Victor and {Chaumond}, Julien and {Delangue}, Clement and {Moi}, Anthony and {Cistac}, Pierric and {Rault}, Tim and {Louf}, R{\'e}mi and {Funtowicz}, Morgan and {Davison}, Joe and {Shleifer}, Sam and {von Platen}, Patrick and {Ma}, Clara and {Jernite}, Yacine and {Plu}, Julien and {Xu}, Canwen and {Le Scao}, Teven and {Gugger}, Sylvain and {Drame}, Mariama and {Lhoest}, Quentin and {Rush}, Alexander M.},
  title         = {{HuggingFace's Transformers: State-of-the-art Natural Language Processing}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Computation and Language},
  year          = 2019,
  month         = oct,
  eid           = {arXiv:1910.03771},
  pages         = {arXiv:1910.03771},
  archiveprefix = {arXiv},
  eprint        = {1910.03771},
  primaryclass  = {cs.CL},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv191003771W},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{xiong-etal-2021-nystromformer,
  title        = {Nyströmformer: A Nyström-based Algorithm for Approximating Self-Attention},
  volume       = {35},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17664},
  abstractnote = {Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences - a topic being actively studied in the community. To address this limitation, we propose Nyströmformer - a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nyström method to approximate standard self-attention with O(n) complexity. The scalability of Nyströmformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nyströmformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nyströmformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github.com/mlpen/Nystromformer.},
  number       = {16},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  year         = {2021},
  month        = {May},
  pages        = {14138-14148}
}

@inproceedings{yang-etal-2019-xlnet,
  author    = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  url       = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@inproceedings{zaheer-etal-2020-big,
  author    = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {17283--17297},
  publisher = {Curran Associates, Inc.},
  title     = {Big Bird: Transformers for Longer Sequences},
  url       = {https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{zhu-etal-2015-aligning,
  author    = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {19-27},
  abstract  = {Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.},
  keywords  = {},
  doi       = {10.1109/ICCV.2015.11},
  issn      = {2380-7504},
  month     = {Dec}
}

@phdthesis{stern-2013-identification,
  TITLE = {{Identification automatique d'entit{\'e}s pour l'enrichissement de contenus textuels}},
  AUTHOR = {Stern, Rosa},
  URL = {https://tel.archives-ouvertes.fr/tel-00939420},
  SCHOOL = {{Universit{\'e} Paris-Diderot - Paris VII}},
  YEAR = {2013},
  MONTH = Jun,
  KEYWORDS = {information extraction ; semantic web ; semantic annotation ; entities ; named entitiy recognition ; linking ; extraction d'information ; web s{\'e}mantique ; annotation s{\'e}mantique ; linked data ; entit{\'e}s ; reconnaissance d'entit{\'e}s nomm{\'e}es ; liage},
  TYPE = {Theses},
  PDF = {https://tel.archives-ouvertes.fr/tel-00939420/file/these.pdf},
  HAL_ID = {tel-00939420},
  HAL_VERSION = {v1},
}

@Comment{jabref-meta: databaseType:bibtex;}
