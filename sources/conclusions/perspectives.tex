%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions and Perspectives}\label{chap:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
    \begin{minipage}{0.66\textwidth}
        \begin{small}
            In which we present the conclusions of this Ph.D. thesis and outline future perspectives and research directions that might develop in the coming years.
        \end{small}
    \end{minipage}
    \vspace{0.5cm}
\end{center}

During the course of this Ph.D. thesis we developed 3 versions of a large multilingual corpus, we participated in the development of one balanced corpus for Contemporary French, the curation of a corpus for Medieval French and another for Early Modern French. With these corpora, we developed contextualized ELMo representations \citep{peters-etal-2018-deep} for six languages: Bulgarian, Catalan, Danish, Finnish, French and Indonesian. We also participated in the development of three \roberta-based \citep{liu-etal-2019-roberta} models for Contemporary, Medieval and Early Modern French. 

We chose to focus on the development of data for the pre-training of language models rather than on the architectures themselves. This approach proved to be extremely effective as we were able to establish a new states of the art for a wide range of tasks in natural language processing for five of the six aforementioned languages as well as for Contemporary, Medieval and Early Modern French. Furthermore, we were able to determine, not only that these contextualized language models are extremely sensitive to pre-training data quality, heterogeneity and balance, but we also showed that these three features were better predictors of the pre-trained models' performance in downstream tasks than the pre-training data size itself. In fact, we were able to determine that the importance of pre-training dataset size had been largely overestimated \citep{martin-etal-2020-camembert} as we were able to repeatedly show that one can pre-train these architectures with quite modestly size corpora.

We consider that we have reached and far exceeded all the initial objectives set by the BASNUM project for this thesis, by producing models and automatic annotation systems that will make it possible to enrich not only the \emph{Dictionnaire Universel} of Basnage but also any other type of document in French from any possible time period. Moreover, due to recent developments by the \emph{GROBID} \citep{lopez-etal-2018-grobid} and \emph{DeLFT} \citep{lopez-etal-2018-delft} authors, it is now possible to easily plug all of the contextualized language models that we have developed to their document structuring pipeline, we hope that these will quickly allow the members of the BASNUM project to use our French models and in particular, our \dalembert model with \emph{GROBID-dictionaries} \citep{khemakhem-etal-2017-automatic,khemakhem-etal-2018-enhancing}, hoping that they can largely improve the initial annotations of the macrostructure already performed by \citet{khemakhem-2020-standard} in just a few lines of code.

We hope that the resources we have produced here will be of great use to researchers in both natural language processing and digital humanities. In particular, we hope that we will be able to continue working in improving our OSCAR project, which grew from a side-project intended to gather resources for pre-training language models in French, to a fully fledge Open Source project with a thriving community. \footnote{\url{https://oscar-corpus.com}.}\textsuperscript{,}\footnote{\url{https://discord.com/invite/4JNg9FTar4}.}\textsuperscript{,}\footnote{\url{https://twitter.com/oscarnlp}.} The first version of the OSCAR corpus, OSCAR 2019, now has more than 14 thousands downloads\footnote{\url{https://huggingface.co/datasets/oscar}.} and has been used independent studies to train monolingual and multilingual language models in more than 18 different languages \citep{antoun-etal-2021-araelectra, kakwani-etal-2020-indicnlpsuite, wilie-etal-2020-indonlu, chan-etal-2020-germans, koutsikakis-etal-2020-greek, martin-etal-2020-camembert, chriqui-etal-2021-hebert, seker-etal-2021-alephbert, delobelle-etal-2020-robbert, dumitrescu-etal-2020-birth, masala-etal-2020-robert} making it possible to NLP researchers with all different backgrounds and interests, to access some of the latest developments in state-of-the-art NLP.

However, we do acknowledge that OSCAR is far from a perfect corpus, and many of the concerns expressed by \citet{caswell-etal-2020-language,kreutzer-etal-2021-quality} remain to be addressed. This is a direction of research that we would like to explore in the future, so that we can provide the OSCAR community with an ever-evolving, up-to-date, properly classified, high-quality and even annotated corpus.

Finally, we would like to continue our research in NLP for digital humanities, by trying to apply our methods to other historical languages, we would additionally like to study and research the latest developments in tokenization agnostic architectures \citep{clark-etal-2021-canine,xue-etal-2021-byt5} that would be of great use when one has to model historical language that exhibit free word order and non-standardized orthography. In direct relation to the BASNUM project, we would also like to study the new sparse attention architectures \cite{xiong-etal-2021-nystromformer,beltagy-etal-2020-longformer}, that are capable of handling large contextual windows and that might be of great use for tasks like document structuring were one has to deal with long sequences of text, allowing us to develop end-to-end architectures for document structuring without the need to go through dedicated document processing pipelines such as \emph{GROBID} \citep{lopez-etal-2018-grobid} or \emph{DeLFT} \citep{lopez-etal-2018-delft}.
