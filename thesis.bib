% Encoding: UTF-8

@inproceedings{brown-etal-2020-language,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@misc{callan-etal-2009-clueweb09,
  title  = {Clueweb09 data set},
  author = {Callan, Jamie and Hoy, Mark and Yoo, Changkuk and Zhao, Le},
  year   = {2009}
}

@inproceedings{clark-etal-2020-electra,
  title     = {ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author    = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=r1xMH1BtvB}
}

@article{caswell-etal-2021-quality,
  author     = {Isaac Caswell and
               Julia Kreutzer and
               Lisa Wang and
               Ahsan Wahab and
               Daan van Esch and
               Nasanbayar Ulzii{-}Orshikh and
               Allahsera Tapo and
               Nishant Subramani and
               Artem Sokolov and
               Claytone Sikasote and
               Monang Setyawan and
               Supheakmungkol Sarin and
               Sokhar Samb and
               Beno{\^{\i}}t Sagot and
               Clara Rivera and
               Annette Rios and
               Isabel Papadimitriou and
               Salomey Osei and
               Pedro Javier Ortiz Su{\'{a}}rez and
               Iroro Orife and
               Kelechi Ogueji and
               Rubungo Andre Niyongabo and
               Toan Q. Nguyen and
               Mathias M{\"{u}}ller and
               Andr{\'{e}} M{\"{u}}ller and
               Shamsuddeen Hassan Muhammad and
               Nanda Muhammad and
               Ayanda Mnyakeni and
               Jamshidbek Mirzakhalov and
               Tapiwanashe Matangira and
               Colin Leong and
               Nze Lawson and
               Sneha Kudugunta and
               Yacine Jernite and
               Mathias Jenny and
               Orhan Firat and
               Bonaventure F. P. Dossou and
               Sakhile Dlamini and
               Nisansa de Silva and
               Sakine {\c{C}}abuk Balli and
               Stella Biderman and
               Alessia Battisti and
               Ahmed Baruwa and
               Ankur Bapna and
               Pallavi Baljekar and
               Israel Abebe Azime and
               Ayodele Awokoya and
               Duygu Ataman and
               Orevaoghene Ahia and
               Oghenefego Ahia and
               Sweta Agrawal and
               Mofetoluwa Adeyemi},
  title      = {Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets},
  journal    = {CoRR},
  volume     = {abs/2103.12028},
  year       = {2021},
  url        = {https://arxiv.org/abs/2103.12028},
  eprinttype = {arXiv},
  eprint     = {2103.12028},
  timestamp  = {Wed, 24 Mar 2021 15:50:40 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2103-12028.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{joulin-etal-2016-fasttext,
  author     = {Armand Joulin and
               Edouard Grave and
               Piotr Bojanowski and
               Matthijs Douze and
               Herv{\'{e}} J{\'{e}}gou and
               Tom{\'{a}}s Mikolov},
  title      = {FastText.zip: Compressing text classification models},
  journal    = {CoRR},
  volume     = {abs/1612.03651},
  year       = {2016},
  url        = {http://arxiv.org/abs/1612.03651},
  eprinttype = {arXiv},
  eprint     = {1612.03651},
  timestamp  = {Mon, 28 Dec 2020 11:31:02 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/JoulinGBDJM16.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mikolov-etal-2013-distributed,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  url       = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
  volume    = {26},
  year      = {2013}
}

@inproceedings{ortiz-suarez-etal-2019-asynchronous,
  author    = {Pedro Javier {Ortiz Su{\'a}rez} and Beno{\^i}t Sagot and Laurent Romary},
  booktitle = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019},
  title     = {Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures},
  editor    = {Piotr Bański and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc Kupietz and Harald L{\"u}ngen and Caroline Iliadi},
  publisher = {Leibniz-Institut f{\"u}r Deutsche Sprache},
  address   = {Mannheim},
  doi       = {10.14618/ids-pub-9021},
  url       = {http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215},
  pages     = {9 -- 16},
  year      = {2019},
  abstract  = {Common Crawl is a considerably large, heterogeneous multilingual corpus comprised of crawled documents from the internet, surpassing 20TB of data and distributed as a set of more than 50 thousand plain text files where each contains many documents written in a wide variety of languages. Even though each document has a metadata block associated to it, this data lacks any information about the language in which each document is written, making it extremely difficult to use Common Crawl for monolingual applications. We propose a general, highly parallel, multithreaded pipeline to clean and classify Common Crawl by language; we specifically design it so that it runs efficiently on medium to low resource infrastructures where I/O speeds are the main constraint. We develop the pipeline so that it can be easily reapplied to any kind of heterogeneous corpus and so that it can be parameterised to a wide range of infrastructures. We also distribute a 6.3TB version of Common Crawl, filtered, classified by language, shuffled at line level in order to avoid copyright issues, and ready to be used for NLP applications.},
  language  = {en}
}

@article{parker-etal-2011-english,
  title   = {English gigaword fifth edition, linguistic data consortium},
  author  = {Parker, Robert and Graff, David and Kong, Junbo and Chen, Ke and Maeda, Kazuaki},
  journal = {Technical report, Technical Report. Linguistic Data Consortium},
  address = {Philadelphia, Tech. Rep.},
  year    = {2011}
}

@article{radford-etal-2018-improving,
  title   = {Improving language understanding by generative pre-training},
  author  = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal = {OpenAI Blog},
  year    = {2018}
}

@article{radford-etal-2019-language,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal = {OpenAI Blog},
  volume  = {1},
  pages   = {8},
  year    = {2019}
}

@article{raffel-etal-2020-exploring,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{vaswani-etal-2017-attention,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@article{xiong-etal-2021-nystromformer,
  title        = {Nyströmformer: A Nyström-based Algorithm for Approximating Self-Attention},
  volume       = {35},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17664},
  abstractnote = {Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences - a topic being actively studied in the community. To address this limitation, we propose Nyströmformer - a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nyström method to approximate standard self-attention with O(n) complexity. The scalability of Nyströmformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nyströmformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nyströmformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github.com/mlpen/Nystromformer.},
  number       = {16},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  year         = {2021},
  month        = {May},
  pages        = {14138-14148}
}

@inproceedings{yang-etal-2019-xlnet,
  author    = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  url       = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@inproceedings{zaheer-etal-2020-big,
  author    = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {17283--17297},
  publisher = {Curran Associates, Inc.},
  title     = {Big Bird: Transformers for Longer Sequences},
  url       = {https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{zhu-etal-2015-aligning,
  author    = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {19-27},
  abstract  = {Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.},
  keywords  = {},
  doi       = {10.1109/ICCV.2015.11},
  issn      = {2380-7504},
  month     = {Dec}
}

@Comment{jabref-meta: databaseType:bibtex;}
