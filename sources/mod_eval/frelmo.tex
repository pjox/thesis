%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{FrELMo}\label{chap:frelmo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
    \begin{minipage}{0.66\textwidth}
        \begin{small}
            In which we present a part of the work of \citet{ortiz-suarez-etal-2020-establishing} who pre-train an ELMo model for Contemporary French and then evaluate its performance in the NER annotated FTB against all the available versions of CamemBERT. From these experiments \citet{ortiz-suarez-etal-2020-establishing} set a new state of the art for this corpus. We also present part of the work of \citet{popa-fabre-etal-2020-french} who further train pre-train ELMo models with the previously presented \Cabernet and CBT-fr and then evaluate them in multiple downstream tasks in order to assess the importance of representative and balanced corpora as pre-training datasets.\footnotemark
        \end{small}
    \end{minipage}
    \vspace{0.5cm}
\end{center}

\footnotetext{Contributions: for the part of \citep{ortiz-suarez-etal-2020-establishing} presented here I pre-trained FrELMo  and conducted all the experiments involving FrELMo and \camembert. For the part of {popa-fabre-etal-2020-french} presented here, I pre-trained all the ELMo models and conducted all the evaluations in downstream tasks.}

Having trained the \roberta \citep{liu-etal-2019-roberta} based \camembert \citep{martin-etal-2020-camembert} models in the previous chapter, we wanted to fairly compare the Transformer-based architecture with ELMo \citep{peters-etal-2018-deep}, the BiLSTM-based contextualized word representations that predated the BERT model \citep{devlin-etal-2019-bert}. Such a comparison had already been done to an extent in English by \citet{peters-etal-2019-tune}, but in that case, ELMo and BERT where pre-trained with different datasets, which as we saw in previous chapters, can have an enormous impact on the performance of these types of models. We thus decided to train an ELMo model with the French subcorpus of OSCAR 2019 to fairly compare with CamemBERT. We first compare these two models in a benchmarking experiment in named entity recognition that we do in order to find the best possible combination of embeddings and architectures for NER or at least for the NER annotated version of the FTB that we presented in subsection \ref{subsec:alignment}. We then expand our experiments by actually repeating most of the CamemBERT experiments but this for comparing the OSCAR pre-trained ELMo with \Cabernet and CBT-fr ELMos.

\section{FrELMo}

We train an ELMo model for contemporary French using the French subcorpus of OSCAR 2019. Furthermore, we train each model for 10 epochs, as was done for the original English ELMo \citep{peters-etal-2018-deep}. We also use the same hyper-parameters and the same pre-processing as the originals ELMo authors, i.e., we shuffle the French subcorpus of OSCAR 2019 at a line level. In this case we do not bother to save checkpoints as we previously saw that training for longer produced better models (see \ref{chap:monolingual}), so we train for the full 10 epochs as the original authors suggested \citep{peters-etal-2018-deep}.

\subsection{Benchmarking NER Models}\label{subsec:benchmarking-ner-models}

\subsubsection{Experiments}
For our benchmark of NER models for French, we used SEM \citep{dupont-2017-exploration} as our strong baseline because, to the best of our knowledge, it was the previous state-of-the-art for named entity recognition on the FTB-NE corpus. Other French NER systems are available, such as the one given by SpaCy. However, it was trained on another corpus called WikiNER, making the results non-comparable. We can also cite the system of \citep{stern-etal-2012-joint}. This system was trained on another newswire (AFP) using the same annotation guidelines, so the results given in this article are not directly comparable. This model was trained on FTB-NE in \citet{stern-2013-identification} (table C.7, page 303), but the article is written in French. The model yielded an F1-score of 0.7564, which makes it a weaker baseline than SEM. We can cite yet another NER system, namely grobid-ner.\footnote{\url{https://github.com/kermitt2/grobid-ner\#corpus-lemonde-ftb-french}} It was trained on the FTB-NE and yields an F1-score of 0.8739, but two things are to be taken into consideration in grobid-ner's score: the tagset was slightly modified and scores were averaged over a 10-fold cross validation. To see why this is important for FTB-NE, see section \ref{subsubsec:shuffling}.

In this section, we will compare our strong baseline with a series of neural models. We will use the two current state-of-the-art neural architectures for NER, namely seq2seq and LSTM-CRFs models. We will use various pre-trained embeddings in said architectures: fastText, \camembert and FrELMo embeddings.


\subsubsection{SEM}
SEM \citep{dupont-2017-exploration} is a tool that relies on linear-chain CRFs \citep{lafferty-etal-2001-conditional} to perform tagging. SEM uses Wapiti \citep{lavergne-etal-2010-practical} v1.5.0 as linear-chain CRFs implementation. SEM uses the following features for NER:
\begin{itemize}
    \item token, prefix/suffix from 1 to 5 and a Boolean isDigit features in a [-2, 2] window;
    \item previous/next common noun in sentence;
    \item 10 gazetteers (including NE lists and trigger words for NEs) applied with some priority rules in a [-2, 2] window;
    \item a "fill-in-the-gaps" gazetteers feature where tokens not found in any gazetteer are replaced by their POS, as described in \citep{raymond-fayolle-2010-reconnaissance}. These features used token unigrams and token bigrams in a [-2, 2] a window.
    \item tag unigrams and bigrams.
\end{itemize}

We trained our own SEM model by using SEM features on gold tokenization and optimized L1 and L2 penalties on the development set. The metric used to estimate convergence of the model is the error on the development set ($1 - accuracy$). Our best result on the development set was obtained using the rprop algorithm, a 0.1 L1 penalty and a 0.1 L2 penalty.

SEM also uses an NE mention broadcasting post-processing (mentions found at least once are used as a gazetteer to tag unlabeled mentions), but we did not observe any improvement using this post-processing on the best hyperparameters on the development set.


\subsubsection{Neural models}

In order to study the relative impact of different word vector representations and different architectures, we trained a number of NER neural models that differ in multiple ways. They use zero to three of the following vector representations: FastText non-contextual embeddings \citep{bojanowski-etal-2017-enriching}, the FrELMo contextual language model, and one of multiple \camembert language models \citep{martin-etal-2020-camembert} (see Appendix \ref{appendix:camembert}). The \camembert models we use in our experiments differ in multiple ways:
\begin{itemize}
    \item Training corpus: OSCAR 2019 or CCNet \citep{wenzek-etal-2020-ccnet}. For comparison purposes, we also display the results of an experiment using the mBERT multilingual BERT model trained on the Wikpiedias for over 100 languages.
    \item Model size: following \citet{devlin-etal-2019-bert}, we use both ``BASE'' and ``LARGE'' models; these models differ by their number of layers (12 vs.~24), hidden dimensions (768 vs.~1024), attention heads (12 vs.~16) and, as a result, their number of parameters (110M vs.~340M).
    \item Masking strategy: the objective function used to train a \camembert model is a masked language model objective. However, BERT-like architectures like \camembert rely on a fixed vocabulary of explicitly predefined size obtained by an algorithm that splits rarer words into subwords, which are part of the vocabulary together with more frequent words. As a result, it is possible to use a whole-word masked language objective (the model is trained to guess missing words, which might be made of more than one subword) or a subword masked language objective (the model is trained to guess missing subwords). Our models use the acronyms WWM and SWM respectively to indicate the type of masking they used.
\end{itemize}

We use these word vector representations in three types of architectures:
\begin{itemize}
    \item Fine-tuning architectures: in this case, we add a dedicated linear layer to the first subword token of each word, and the whole architecture is then fine-tuned to the NER task on the training data.
    \item Embedding architectures: word vectors produced by language models are used as word embeddings. We use such embeddings in two types of LSTM-based architectures: an LSTM fed to a seq2seq layer and an LSTM fed to a CRF layer. In such configurations, the use of several word representations at the same time is possible, using concatenation as a combination operator. For instance, in Table~\ref{tab:results_ordered}, the model FastText + CamemBERT\textsubscript{OSCAR-BASE-WWM} under the header ``\emph{LSTM-CRF + embeddings} corresponds to a model using the LSTM-CRF architecture and, as embeddings, the concatenation of FastText embeddings, the output of the \camembert ``BASE'' model trained on OSCAR with a whole-word masking objective, and the output of the FrELMo language model. For all LSTM-based architectures we use the implementation of \citet{strakova-etal-2019-neural}.
\end{itemize}

For our neural models, we optimized hyperparameters using F1-score on development set as our convergence metric.

We train each model three times with three different seeds, select the best seed on the development set, and report the results of this seed on the test set in Table~\ref{tab:results_ordered}.

\subsection{Results}

\begin{table}[htp!]
    \centering\small
    \begin{tabular}{lrrr}
        \toprule
        \textsc{Model}                                                 & \textsc{Precision} & \textsc{Recall}   & \textsc{F1-Score} \\
        \midrule
        \multicolumn{4}{c}\emph{baseline}                                                                                           \\
        %         LNSAI &  84.64 & 68.51 & 75.73\\
        SEM (CRF)                                                      & 87.18              & 80.48             & 83.70             \\
        \midrule
        LSTM-seq2seq                                                   & 85.10              & 81.87             & 83.45             \\
        + FastText                                                     & 86.98              & 83.07             & 84.98             \\
        + FastText + FrELMo                                            & 89.49              & 87.48             & 88.47             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-WWM}           & 89.79              & 88.86             & 89.32             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-WWM} + FrELMo  & 90.00              & 88.60             & 89.30             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-WWM}           & 90.31              & 89.29             & 89.80             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-WWM} + FrELMo  & 90.11              & 88.86             & 89.48             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-SWM}           & 90.09              & 89.46             & 89.77             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-SWM} + FrELMo  & 90.11              & 88.95             & 89.53             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-SWM}           & 90.31              & 89.38             & 89.84             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-SWM} + FrELMo  & 90.64              & 89.46             & \underline{90.05} \\
        + FastText + CamemBERT\textsubscript{CCNET-500K-WWM}           & \underline{90.68}  & 89.03             & 89.85             \\
        + FastText + CamemBERT\textsubscript{CCNET-500K-WWM} + FrELMo  & 90.13              & 88.34             & 89.23             \\
        + FastText + CamemBERT\textsubscript{CCNET-LARGE-WWM}          & 90.39              & 88.51             & 89.44             \\
        + FastText + CamemBERT\textsubscript{CCNET-LARGE-WWM} + FrELMo & 89.72              & 88.17             & 88.94             \\
        \midrule
        \multicolumn{4}{c}\emph{LSTM-CRF + embeddings}                                                                              \\
        LSTM-CRF                                                       & 85.87              & 81.35             & 83.55             \\
        + FastText                                                     & 88.53              & 84.63             & 86.53             \\
        + FastText + FrELMo                                            & 88.89              & 88.43             & 88.66             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-WWM}           & 90.47              & 88.51             & 89.48             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-WWM} + FrELMo  & 89.70              & 88.77             & 89.24             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-WWM}           & 90.24              & 89.46             & 89.85             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-WWM} + FrELMo  & 89.38              & 88.69             & 89.03             \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-SWM}           & \textbf{90.96}     & \underline{89.55} & \textbf{90.25}    \\
        + FastText + CamemBERT\textsubscript{OSCAR-BASE-SWM} + FrELMo  & 89.44              & 88.51             & 88.98             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-SWM}           & 90.09              & 88.69             & 89.38             \\
        + FastText + CamemBERT\textsubscript{CCNET-BASE-SWM} + FrELMo  & 88.18              & 87.65             & 87.92             \\
        + FastText + CamemBERT\textsubscript{CCNET-500K-WWM}           & 89.46              & 88.69             & 89.07             \\
        + FastText + CamemBERT\textsubscript{CCNET-500K-WWM} + FrELMo  & 90.11              & 88.86             & 89.48             \\
        + FastText + CamemBERT\textsubscript{CCNET-LARGE-WWM}          & 89.19              & 88.34             & 88.76             \\
        + FastText + CamemBERT\textsubscript{CCNET-LARGE-WWM} + FrELMo & 89.03              & 88.34             & 88.69             \\
        \midrule
        \multicolumn{4}{c}\emph{fine-tuning}                                                                                        \\
        mBERT                                                          & 80.35              & 84.02             & 82.14             \\ %% Qu'est-ce que c'est ?

        CamemBERT\textsubscript{OSCAR-BASE-WWM}                        & 89.36              & 89.18             & 89.27             \\
        CamemBERT\textsubscript{CCNET-500K-WWM}                        & 89.35              & 88.81             & 89.08             \\
        CamemBERT\textsubscript{CCNET-LARGE-WWM}                       & 88.76              & \textbf{89.58}    & 89.39             \\
        \bottomrule
    \end{tabular}
    \caption{Results on the test set for the best development set scores.}
    \label{tab:results_ordered}
\end{table}

\paragraph{Word Embeddings:} Results obtained by SEM and by our neural models are shown in table \ref{tab:results_ordered}. First important result that should be noted is that LSTM+CRF and LSTM+seq2seq models have similar performances to that of the SEM (CRF) baseline when they are not augmented with any kind of embeddings. Just adding classical fastText word embeddings dramatically increases the performance of the model.

\paragraph{ELMo Embeddings:} Adding contextualized ELMo embeddings increases again the performance for both architectures. However, we note that the difference is not as big as in the case of the pair with/without fastText word embeddings for the LSTM-CRF. For the seq2seq model, it is the contrary: adding ELMo gives a good improvement while fastText does not improve the results as much.

\paragraph{\camembert Embeddings:} Adding the \camembert embeddings always increases the performance of the model LSTM based models. However, as opposed to adding ELMo, the difference with/without \camembert is equally considerable for both the LSTM-seq2seq and LSTM-CRF. In fact adding \camembert embeddings increases the original scores far more than ELMo embeddings does, so much so that the state-of-the-art model is the LSTM + CRF + FastText + CamemBERT\textsubscript{OSCAR-BASE-SWM}.

\paragraph{\camembert + FrELMo:} Contrary to the results given in \citet{strakova-etal-2019-neural}, adding ELMo to \camembert did not have a positive impact on the performances of the models. Our hypothesis for these results is that, contrary to \citet{strakova-etal-2019-neural}, we trained ELMo and \camembert on the same corpus. We think that, in our case, ELMo either does not bring any new information or even interfere with \camembert.

\paragraph{Base vs large:} an interesting observation is that using large model negatively impacts the performances of the models. One possible reason could be that, because the models are larger, the information is more sparsely distributed and that training on the FTB-NE, a relatively small corpus, is harder.

\subsection{Impact of shuffling the data}
\label{subsubsec:shuffling}

One important thing about the FTB is that the underlying text is made of articles from the newspaper Le Monde that are chronologically ordered. Moreover, the standard development and test sets are at the end of the corpus, which means that they are made of articles that are more recent than those found in the training set. This means that a lot of entities in the development and test sets may be new and therefore unseen in the training set. To estimate the impact of this distribution, we shuffled the data, created a new training/development/test split of the same lengths as in the standard split, and retrained and reevaluated our models. We repeated this process 3 times to avoid unexpected biases. The raw results of this experiment are given in table \ref{tab:results_shuffled}. We can see that the shuffled splits result in improvements on all metrics, the improvement in F1-score on the test set ranging from 4.04 to 5.75 (or 25\% to 35\% error reduction) for our SEM baseline, and from 1.73 to 3.21 (or 18\% to 30\% error reduction) for our LSTM-CRF architectures, reaching scores comparable to the English state-of-the-art. This highlights a specific difficulty of the FTB-NE corpus where the development and test sets seem to contain non-negligible amounts of unknown entities. This specificity, however, allows to have a quality estimation which is more in line with real use cases, where unknown NEs are frequent. This is especially the case when processing newly produced texts with models trained on FTB-NE, as the text annotated in the FTB is made of articles around 20 years old.

\begin{table}
    \centering\small
    \begin{tabular}{lrrr}
        \toprule
        \textsc{Model}                                         & \textsc{Precision} & \textsc{Recall}   & \textsc{F1-Score} \\
        \midrule
        \multicolumn{4}{c}\emph{shuf 1}                                                                                     \\
        SEM(dev)                                               & 92.96              & 87.84             & 90.33             \\
        LSTM-CRF+CamemBERT\textsubscript{OSCAR-BASE-SWM}(dev)  & \underline{93.77}  & \underline{94.00} & \underline{93.89} \\
        SEM(test)                                              & 91.88              & 87.14             & 89.45             \\
        LSTM-CRF+CamemBERT\textsubscript{OSCAR-BASE-SWM}(test) & \textbf{92.59}     & \textbf{93.96}    & \textbf{93.27}    \\
        \midrule
        \multicolumn{4}{c}\emph{shuf 2}                                                                                     \\
        SEM(dev)                                               & 91.67              & 85.96             & 88.73             \\
        LSTM-CRF+CamemBERT\textsubscript{OSCAR-BASE-SWM}(dev)  & \underline{93.15}  & \underline{94.21} & \underline{93.68} \\
        SEM(test)                                              & 90.57              & 87.76             & 89.14             \\
        LSTM-CRF+CamemBERT\textsubscript{OSCAR-BASE-SWM}(test) & \textbf{92.63}     & \textbf{94.31}    & \textbf{93.46}    \\
        \midrule
        \multicolumn{4}{c}\emph{shuf 3}                                                                                     \\
        SEM(dev)                                               & 92.53              & 88.75             & 90.60             \\
        LSTM-CRF+CamemBERT\textsubscript{OSCAR-BASE-SWM}(dev)  & \underline{94.85}  & \underline{95.82} & \underline{95.34} \\
        SEM(test)                                              & 90.68              & 85.00             & 87.74             \\
        LSTM-CRF+CamemBERT\textsubscript{OSCAR-BASE-SWM}(test) & \textbf{91.30}     & \textbf{92.67}    & \textbf{91.98}    \\
        \bottomrule
    \end{tabular}
    \caption{Results on the test set for the best development set scores.}
    \label{tab:results_shuffled}
\end{table}

\subsection{Conclusions of the Benchmark}
\label{sec:conclusion}

We establish a new state-of-the-art for French NER using state-of-the-art neural techniques and recently produced neural language models for French. Our best neural model reaches an F1-score which is 6.55 points higher (a 40\% error reduction) than the strong baseline provided by the SEM system.

We also highlight how the FTB-NE is a good approximation of a real use case. Its chronological partition increases the number of unseen entities allows to have a better estimation of the generalization capacities of machine learning models than if it were randomized.

One interesting point to investigate is that using Large embeddings overall has a negative impact on the models performances. It could be because larger models store information relevant to NER more sparingly, making it harder for trained models to capitalize them. We would like to investigate this hypothesis in future research.

\section{Pre-training Corpora Evaluation for ELMo models} \label{sect:EvalMethod}

Having completed this Benchmark in NER, we also wanted to better understand the computational impact of the quality, size and linguistic balance in ELMo's \citep{peters-etal-2018-deep} pre-training. We conducted this experiments with ELMo instead of BERT or RoBERTa, as ELMo is a far less demanding model in terms of computing power when it comes to pre-training, and at the moment when we conducted these experiments we didn't have access to the infrastructure required to pre-train Transformer-based models.

\subsection{ELMo Pre-traing \& Fine-tuning Method}\label{MethodTRAIN}

Two protocols were carried out to evaluate the impact of corpora characteristics on the tasks under analysis. \textit{Method 1} implies a full pre-training ELMo-based language models for each of the corpora mentioned in Table \ref{Table_nb_Words}. While \textit{Method 2} is based on pre-training OSCAR + fine-tuning with our French Balanced Reference Corpus \Cabernet, yielding \ELMocoscar. Hence, the pure pre-traing (i.e. Method 1) yields the following four language models which were pre-trained on the four corpora under comparison :  \ELMooscar (FrELMo in the previous section), \ELMowiki, \ELMococa and \ELMocbt.

We conduct the same experiments that we did for CamemBERT in dependency parsing, POS tagging and NER. We also coupled our ELMo models with the same tasks specific architectures as before, namely textbf{UDPipe Future} \citep{straka-2018-udpipe} for POS tagging and dependency parsing and \citep{strakova-etal-2019-neural} for NER. Experiments were run using the Universal Dependencies (UD) paradigm and its corresponding UD POS-tag set \citep{petrov-etal-2012-universal} and UD treebank collection version 2.2 \citep{nivre-etal-2018-universal}, which was used for the CoNLL 2018 shared task.

\subsection{Results \& Discussion} \label{sect:ResultsCorpora}

\subsubsection{Dependency Parsing and POS-tagging}\label{sect:ResultsParsePOS}

\begin{table}[htp!]
    \small\centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{ l  c  c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c  c @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c  c  @{\hspace{0.35cm}}  @{\hspace{0.35cm}} c  c  c }
            \toprule
                                                        & \multicolumn{3}{c @{\hspace{0.5cm}}}{\textsc{GSD}} & \multicolumn{3}{c @{\hspace{0.7cm}}}{\textsc{Sequoia}} & \multicolumn{3}{c @{\hspace{0.7cm}}}{\textsc{Spoken}} & \multicolumn{3}{c @{\hspace{0.35cm}}}{\textsc{ParTUT}}                                                                                                                                                                                                                                                                                                                        \\
            \cmidrule(l{2pt}r{0.4cm}){2-4}\cmidrule(l{-0.2cm}r{0.4cm}){5-7}\cmidrule(l{-0.2cm}r{0.4cm}){8-10}\cmidrule(l{-0.2cm}r{2pt}){11-13}
            \multirow{-2}{*}[1pt]{\textsc{Model}}       & \textsc{UPOS}                                      & \textsc{UAS}                                           & \textsc{LAS}                                          & \textsc{UPOS}                                          & \textsc{UAS}                           & \textsc{LAS}                           & \textsc{UPOS}                              & \textsc{UAS}                           & \textsc{LAS}                           & \textsc{UPOS}     & \textsc{UAS}                           & \textsc{LAS}                           \\
            \midrule
            %\multicolumn{1}{c}{UDPipe Future + ELMo} & \multicolumn{12}{c}{}\\
            %\cmidrule(lr){1-1}

            %\multicolumn{13}{l}{\textit{Baseline}} \\
            \underline{\textit{Baseline} UDPipe Future} & 97.63                                              & 90.65                                                  & 88.06                                                 & 98.79                                                  & 92.37                                  & 90.73                                  & 95.91                                      & 82.90                                  & 77.53                                  & 96.93             & 92.17                                  & 89.63                                  \\

            \:+\ELMocbt                                 & 97.49                                              & 90.21                                                  & 87.37                                                 & 98.40                                                  & 92.18                                  & 90.56                                  & 96.60                                      & 85.05                                  & 79.82                                  & 97.27             & 92.55                                  & 90.44                                  \\

            \:+\ELMowiki                                & \underline{97.92}                                  & 92.13                                                  & 89.77                                                 & 99.22                                                  & 94.28                                  & 92.97                                  & \underline{97.28}                          & 85.61                                  & 80.79                                  & \textbf{97.62}    & 94.01                                  & 91.78                                  \\

            %-FrWak  & \underline{97.89} & 92.04 & 89.70 & 99.25 & 94.53 & 93.36 & 97.20 & \textbf{86.04} & \textbf{81.14} & 97.47 & \textbf{94.78} & 92.40\\ 

            %\midrule 
            %\:+\ELMococa  & 97.76 & 91.91 & 89.49 & \underline{99.27} & \underline{94.65} & \underline{93.40} & \cellcolor[gray]{0.7}\emph{\textbf{97.32}} & 85.63 & 80.61 & \underline{97.58} & 94.24 & 91.90\\ 

            %%%%%%% new results on clean cabernet %%%%%%%%%%%%%%%%
            \:+\ELMocaber                               & 97.87                                              & 92.02                                                  & 89.62                                                 & \underline{99.33}                                      & 94.42                                  & 93.14                                  & \cellcolor[gray]{0.7}\emph{\textbf{97.30}} & 85.39                                  & 80.63                                  & 97.43             & 94.02                                  & 91.86                                  \\
            %\midrule 
            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

            \:+\ELMooscar                               & 97.85                                              & \cellcolor[gray]{0.9}\underline{92.41}                 & \cellcolor[gray]{0.9}\underline{90.05}                & 99.30                                                  & \cellcolor[gray]{0.9}\underline{94.43} & \cellcolor[gray]{0.9}\underline{93.25} & 97.10                                      & \cellcolor[gray]{0.9}\underline{85.83} & \cellcolor[gray]{0.9}\textbf{80.94}    & 97.47             & \cellcolor[gray]{0.9}\textbf{94.74}    & \cellcolor[gray]{0.9}\textbf{92.55}    \\

            \midrule
            %\:+\ELMocoscar & \underline{97.88} & \cellcolor[gray]{0.9}\textbf{92.67} & \cellcolor[gray]{0.9} \textbf{90.34} & 99.26 & \cellcolor[gray]{0.9}\textbf{94.75} & \cellcolor[gray]{0.9}\textbf{93.54} & 97.22 & \cellcolor[gray]{0.9}\underline{85.77} & \cellcolor[gray]{0.9}\underline{80.80} & 97.50 & \cellcolor[gray]{0.9}\underline{94.66} & \cellcolor[gray]{0.9}\underline{92.43} \\ 

            %%%%%%%new results on clean cabernet oscar %%%%%%%%%%%%%%%%

            \:+\ELMocabercar                            & \textbf{97.98}                                     & \cellcolor[gray]{0.9}\textbf{92.57}                    & \cellcolor[gray]{0.9} \textbf{90.22}                  & \textbf{99.34}                                         & \cellcolor[gray]{0.9}\textbf{94.51}    & \cellcolor[gray]{0.9}\textbf{93.38}    & 97.24                                      & \cellcolor[gray]{0.9}\textbf{85.91}    & \cellcolor[gray]{0.9}\underline{80.93} & \underline{97.58} & \cellcolor[gray]{0.9}\underline{94.47} & \cellcolor[gray]{0.9}\underline{92.05} \\

            \midrule %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
            \multicolumn{13}{l}{\textit{State-of-the-art}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\

            \underline{UDify}                           & 97.83                                              & 93.60                                                  & 91.45                                                 & 97.89                                                  & 92.53                                  & 90.05                                  & 96.23                                      & 85.24                                  & 80.01                                  & 96.12             & 90.55                                  & 88.06                                  \\

            UDPipe Future + mBERT                       & 97.98                                              & 92.55                                                  & 90.31                                                 & \emph{99.32}                                           & 94.88                                  & 93.81                                  & 97.23                                      & \emph{86.27}                           & \emph{81.40}                           & \emph{97.64}      & 94.51                                  & 92.47                                  \\

            \camembert                                  & \emph{98.19}                                       & \emph{94.82}                                           & \emph{92.47}                                          & 99.21                                                  & \emph{95.56}                           & \emph{94.39}                           & 96.68                                      & 86.05                                  & 80.07                                  & 97.63             & 95.21                                  & \emph{92.90}                           \\

            \bottomrule
        \end{tabular}
    }
    \caption{Final POS and dependency parsing scores on 4 French treebanks (French GSD, Spoken, Sequoia and ParTUT), reported on test sets (4 averaged runs) assuming gold tokenisation. Best scores in bold, second to best underlined, state-of-the-art results in italics.}
    \label{tab:fine-tuning_results}
\end{table}

\paragraph{\ELMococa : A Test for Balance}
The representations offered by \ELMococa are not only competitive but sometimes better than Wikipedia ones. One should keep in mind that almost all the four treebanks we use in this section include Wikipedia data. \ELMococa is reaching state-of-the-are results in POS-tagging on Spoken. Notably, it performs better than \camembert, the previous state of the art on this oral specialized tree-bank (cf. dark gray highlight on Table \ref{tab:fine-tuning_results}). We understand this results as a clear effect of balance when testing upon a purely spoken test-set. Importantly, this effect is difficultly explainable by the size of oral-style data in \Cabernet. The oral sub-part is only one fifth of the total, and in this one fifth, only an even smaller amount of data comes from purely oral transcripts comparable the ones in the Spoken tree-bank, namely 67,444 words from Rhapsodie corpus, and 575,894 words form \textsc{ORFEO}. Hence, \Cabernet's  balanced oral language use shows to pay off in POS-tagging. These results are surprising, especially given the fact that our evaluation method was aiming at comparing the quality of word-embedding representations and not beating the state-of-the-art.

\paragraph{\ELMococa : A Test for Coverage}
From Table \ref{tab:fine-tuning_results}, we discover that not only balance, but also the broad and diverse genre converge of \Cabernet may play a role in its POS-tagging success is we compare its results with \ELMocbt that also features oral dialogues in youth literature. The fact that \ELMocbt does not show a comparable performance in POS-tagging, can be interpreted as linked to its size, but possibly also to its lack of variety in genres, thus, suggesting the advantage of a comprehensive coverage of language use. This suggests that a balanced sample may enhance the convergence of generalization about oral-style from distinct genre that still implies oral-like dialogues like in fiction. In sum, broad coverage may contribute to enhancing representations about oral language.

\paragraph{The effect of balance on Fine-tuning}
For POS-tagging in GSD the results of \ELMooscar are in second place position compared to \ELMocoscar that is extremely close to \ELMowiki. While in POS-tagging in ParTUT, \ELMowiki exhibits better results than \ELMooscar, and \ELMocoscar is in second position.

Comparing GSD and Sequoia scores from \ELMooscar and \ELMocoscar, we observe that fine-tuning with \Cabernet the embeddings that were pre-trained on OSCAR, yields better representations for the three tasks compared to both the original \ELMooscar and \ELMococa. However, fine-tuning does not always yield better findings than \ELMooscar on Spoken and ParTUT, where \ELMocoscar places in second after \ELMooscar for parsing scores UAS/LAS (cf. Table \ref{tab:fine-tuning_results}).

A closer look on Parsing results reveals an interesting pattern of results across treebanks (see light gray highlights on Table \ref{tab:fine-tuning_results}). We see that for GSD and Sequoia the \Cabernet fine-tuned version \ELMocoscar compared to the pure OSCAR pre-trained \ELMooscar is achieving higher scores. While a reverse and less clear-cut pattern is observable for the other two treebanks, namely Spoken and ParTUT. This configuration can be explained if we understand this pattern as due to the reinforcement and unlearning of \ELMooscar representations during the process of fine-tuning. Specifically, we can observe that parsing scores are better on treebanks that share the kind of language use represented in \Cabernet, while they are worse on corpora that are closer in language sample to OSCAR corpus, like Spoken and ParTuT. This calls for further developments of \Cabernet (ยง\ref{sec:Concl}).

\paragraph{\ELMocbt: small but relevant}
\ELMocbt shows an intriguing pattern of results. Even if its scores are under the baseline on GSD and Sequoia, it yields over the baseline results for Spoken and ParTUT. Given its reduced size, one would expect it to overfit, this would explain the under baseline performance. However, this was not the case on Spoken and ParTUT treebanks, thus showing \ELMocbt contribution in generating representations that are useful to UDPipe model to achieve better results in POS-tagging and parsing tasks on the ParTUT and Spoken tree-banks. The presence of oral dialogues is certainly playing a role in this results' pattern. This unexpected result calls for further investigation on the impact of pre-training with reduced-size, noiseless, domain-specific corpora.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{NER} \label{sect:ResultsNER}

\begin{table}[htp!]
    \centering\small
    \begin{tabular}{lccc}
        \toprule
        %\multicolumn{4}{c}{\textsc{NER - Results}}  \\\midrule
        \textsc{NER - Results} on FTB                 & Precision                            & Recall                              & F1                                  \\
        \midrule
        \multicolumn{4}{l}{\textit{Baselines Models}}                                                                                                                    \\
        SEM (CRF) \citep{dupont-2017-exploration}     & 87.89                                & 82.34                               & 85.02                               \\ %baseline 
        LSTM-CRF \citep{dupont-2017-exploration}      & 87.23                                & 83.96                               & 85.57                               \\ \midrule %baseline 2
        LSTM-CRF  test models                         & 85.87                                & 81.35                               & 83.55                               \\
        \:+FastText                                   & 88.53                                & 84.63                               & 86.53                               \\
        \:+FastText+\ELMocbt                          & 79.77                                & 77.63                               & 78.69                               \\
        \:+FastText+\ELMowiki                         & 88.87                                & 87.56                               & 88.21                               \\
        % \:+FastText+\ELMococa                  & 88.82                 & 87.82                 & 88.32                 \\
        \:+FastText+\ELMocaber                        & 88.91                                & 87.22                               & 88.06                               \\
        \:+FastText+\ELMooscar                        & 88.89                                & 88.43                               & 88.66                               \\\midrule %
        %\:+FastText+\ELMocoscar                & \cellcolor[gray]{0.8} \emph{\textbf{88.93}} & \underline{88.08}     & \underline{88.50}     \\
        \:+FastText+\ELMocabercar                     & \cellcolor[gray]{0.8} \textbf{90.70} & \cellcolor[gray]{0.8}\textbf{89.12} & \cellcolor[gray]{0.8}\textbf{89.93} \\
        \midrule

        \multicolumn{4}{l}{\textit{State-of-the-art Models}}                                                                                                             \\
        \camembert \citep{martin-etal-2020-camembert} & \underline{89.35}                    & \underline{88.81}                   & \underline{89.08}                   \\ %baseline state of the art 
        \bottomrule
    \end{tabular}
    \caption{NER Results on French Treebank (FTB): \textbf{best scores}, \underline{second to best}.}
\end{table}

For NER, LSTM-CRF+FastText+\ELMocabercar achieves a better precision, recall and F1 than the traditional CRF-based SEM architectures and even \camembert. Importantly, LSTM-CRF+FastText+\ELMocaber reaches better results in finding entity mentions, than Wikipedia which is a highly specialized corpus in terms of vocabulary variety and size, as can be seen in the overwhelming total number of unique forms it contains (see Table \ref{Table_MorphoRich}). We can conclude that both pre-training and fine-tuning with \Cabernet on \ELMooscar generates better word-embedding representations than Wikipedia in this downstream task.

CBT-fr NER results are under the LSTM-CRF baseline. This can possibly be explained by the distance in terms of topics and domain from FTB treebank (i.e. newspaper articles), or by the reduced-size of the corpus to yield good-enough representation to perform named entity recognition.

All in all, our evaluations confirm the effectiveness of large ELMo-based language models fine-tuned or pre-trained with a balanced and linguistically representative corpus, like \Cabernet as opposed to domain-specific ones.

\subsection{Conclusion} \label{sec:Concl}

We investigated the relevance of different types of corpora on ELMo's pre-training and fine-tuning. It confirms the effectiveness and quality of word-embeddings obtained through balanced and linguistically representative corpora.

The proposed evaluation methods are showing that \Cabernet and CBT-fr are not only relevant for neural NLP and language modeling in French, but that corpus balance shows to be a significant predictor of ELMo's accuracy on Spoken test data-set and for NER tasks.

The results obtained for the parsing tasks on ParTUT open a new perspective for the development of the French Balanced Reference Corpus, involving the enhancement of the terminological coverage of \Cabernet. A sixth sub-part could be included to cover technical domains like legal and medical ones, and thereby enlarge the specialized lexical coverage of \Cabernet.

Further developments of this resource would involve an extension to cover user-generated content, ranging from well written blogs, tweets to more variable written productions like newspaper's comment or forums, as present in the CoMeRe corpus \citep{chanier-etal-2014-the}. The computational experiments conducted here also show that pre-training language models like ELMo on a very small sample like the French Children Book Test corpus or \Cabernet yields unexpected results. This opens a perspective for languages that have smaller training corpora. ELMo could be a better suited language model for those languages than it is for others having larger size resources.

To conclude, our current evaluations show that linguistic quality in terms of \emph{representativeness} and balance yields better performing contextualized word-embeddings.
